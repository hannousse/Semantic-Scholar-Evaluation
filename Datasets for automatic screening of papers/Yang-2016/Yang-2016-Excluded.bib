%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Hannousse Abdelhakim at 2020-10-21 17:54:33 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{Florio:2008:SLS:1348246.1348249,
	Abstract = {Structures for the expression of fault-tolerance provisions in application software comprise the central topic of this article. Structuring techniques answer questions as to how to incorporate fault tolerance in the application layer of a computer program and how to manage the fault-tolerant code. As such, they provide the means to control complexity, the latter being a relevant factor for the introduction of design faults. This fact and the ever-increasing complexity of today's distributed software justify the need for simple, coherent, and effective structures for the expression of fault-tolerance in the application software. In this text we first define a ``base'' of structural attributes with which application-level fault-tolerance structures can be qualitatively assessed and compared with each other and with respect to the aforementioned needs. This result is then used to provide an elaborated survey of the state-of-the-art of application-level fault-tolerance structures.},
	Acmid = {1348249},
	Address = {New York, NY, USA},
	Articleno = {6},
	Author = {Florio, Vincenzo De and Blondia, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:36:19 +0100},
	Doi = {10.1145/1348246.1348249},
	Issn = {0360-0300},
	Issue_Date = {April 2008},
	Journal = {ACM Comput. Surv.},
	Keywords = {Language support for software-implemented fault tolerance, reconfiguration and error recovery, separation of design concerns, software fault tolerance},
	Number = {2},
	Numpages = {37},
	Pages = {6:1--6:37},
	Publisher = {ACM},
	Title = {A Survey of Linguistic Structures for Application-level Fault Tolerance},
	Url = {http://doi.acm.org/10.1145/1348246.1348249},
	Volume = {40},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1348246.1348249},
	Bdsk-Url-2 = {https://doi.org/10.1145/1348246.1348249}}

@article{Ramsin:2008:PRO:1322432.1322435,
	Abstract = {We provide a detailed review of existing object-oriented software development methodologies, focusing on their development processes. The review aims at laying bare their core philosophies, processes, and internal activities. This is done by using a process-centered template for summarizing the methodologies, highlighting the activities prescribed in the methodology while describing the modeling languages used (mainly diagrams and tables) as secondary to the activities. The descriptions produced using this template aim not to offer a critique on the methodologies and processes, but instead provide an abstract and structured description in a way that facilitates their elaborate analysis for the purposes of improving understanding, and making it easier to tailor, select, and evaluate the processes.},
	Acmid = {1322435},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Ramsin, Raman and Paige, Richard F.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:48:06 +0100},
	Doi = {10.1145/1322432.1322435},
	Issn = {0360-0300},
	Issue_Date = {February 2008},
	Journal = {ACM Comput. Surv.},
	Keywords = {Object-oriented methodologies, agile methods, integrated methodologies, methodology engineering, seminal methodologies},
	Number = {1},
	Numpages = {89},
	Pages = {3:1--3:89},
	Publisher = {ACM},
	Title = {Process-centered Review of Object Oriented Software Development Methodologies},
	Url = {http://doi.acm.org/10.1145/1322432.1322435},
	Volume = {40},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1322432.1322435},
	Bdsk-Url-2 = {https://doi.org/10.1145/1322432.1322435}}

@article{Bentley:2006:IAB:1187976.1187982,
	Abstract = {Several programming languages guarantee that array subscripts are checked to ensure they are within the bounds of the array. While this guarantee improves the correctness and security of array-based code, it adds overhead to array references. This has been an obstacle to using higher-level languages, such as Java, for high-performance parallel computing, where the language specification requires that all array accesses must be checked to ensure they are within bounds. This is because, in practice, array-bounds checking in scientific applications may increase execution time by more than a factor of 2. Previous research has explored optimizations to statically eliminate bounds checks, but the dynamic nature of many scientific codes makes this difficult or impossible. Our approach is, instead, to create a compiler and operating system infrastructure that does not generate explicit bounds checks. It instead places arrays inside of Index Confinement Regions (ICRs), which are large, isolated, mostly unmapped virtual memory regions. Any array reference outside of its bounds will cause a protection violation; this provides implicit bounds checking. Our results show that when applying this infrastructure to high-performance computing programs written in Java, the overhead of bounds checking relative to a program with no bounds checks is reduced from an average of 63% to an average of 9%.},
	Acmid = {1187982},
	Address = {New York, NY, USA},
	Author = {Bentley, Chris and Watterson, Scott A. and Lowenthal, David K. and Rountree, Barry},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:42:44 +0100},
	Doi = {10.1145/1187976.1187982},
	Issn = {1544-3566},
	Issue_Date = {December 2006},
	Journal = {ACM Trans. Archit. Code Optim.},
	Keywords = {64-bit architectures, Array-bounds checking, virtual memory},
	Number = {4},
	Numpages = {26},
	Pages = {502--527},
	Publisher = {ACM},
	Title = {Implicit Array Bounds Checking on 64-bit Architectures},
	Url = {http://doi.acm.org/10.1145/1187976.1187982},
	Volume = {3},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1187976.1187982},
	Bdsk-Url-2 = {https://doi.org/10.1145/1187976.1187982}}

@article{Fields:2004:ICS:1022969.1022971,
	Abstract = {We observe that the challenges software optimizers and microarchitects face every day boil down to a single problem: bottleneck analysis. A bottleneck is any event or resource that contributes to execution time, such as a critical cache miss or window stall. Tasks such as tuning processors for energy efficiency and finding the right loads to prefetch all require measuring the performance costs of bottlenecks.In the past, simple event counts were enough to find the important bottlenecks. Today, the parallelism of modern processors makes such analysis much more difficult, rendering traditional performance counters less useful. If two microarchitectural events (such as a fetch stall and a cache miss) occur in the same cycle, which event should we blame for the cycle? What cost should we assign to each event? In this paper, we introduce a new model for understanding event costs to facilitate processor design and optimization.First, we observe that all instructions, hardware structures, and events in a machine can interact in only one of two ways (in parallel or serially). We quantify these interactions by defining interaction cost, which can be zero (independent, no interaction), positive (parallel), or negative (serial).Second, we illustrate the value of using interaction costs in processor design and optimization. In a processor with a long pipeline, we show how to mitigate the negative performance effect of long latency "critical" loops, such as the level-one cache access and issue-wakeup, by optimizing seemingly unrelated resources that interact with them.Finally, we propose shotgun profiling, a class of hardware profiling infrastructures that are parallelism-aware, in contrast to traditional event counters. Our recommended design requires only modest extensions to current hardware counters, while enabling the construction of full-featured dependence graphs of the microexecution. With these dependence graphs, many types of analyses can be performed, including identifying critical instructions, finding slack, as well as computing costs and interaction costs.},
	Acmid = {1022971},
	Address = {New York, NY, USA},
	Author = {Fields, Brian A. and Bodik, Rastislav and Hill, Mark D. and Newburn, Chris J.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:43:30 +0100},
	Doi = {10.1145/1022969.1022971},
	Issn = {1544-3566},
	Issue_Date = {September 2004},
	Journal = {ACM Trans. Archit. Code Optim.},
	Keywords = {Performance analysis, critical path, modeling, profiling},
	Number = {3},
	Numpages = {33},
	Pages = {272--304},
	Publisher = {ACM},
	Title = {Interaction Cost and Shotgun Profiling},
	Url = {http://doi.acm.org/10.1145/1022969.1022971},
	Volume = {1},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1022969.1022971},
	Bdsk-Url-2 = {https://doi.org/10.1145/1022969.1022971}}

@article{Venkataramani:2011:DDS:1970386.1970389,
	Abstract = {While multicore processors promise large performance benefits for parallel applications, writing these applications is notoriously difficult. Tuning a parallel application to achieve good performance, also known as performance debugging, is often more challenging than debugging the application for correctness. Parallel programs have many performance-related issues that are not seen in sequential programs. An increase in cache misses is one of the biggest challenges that programmers face. To minimize these misses, programmers must not only identify the source of the extra misses, but also perform the tricky task of determining if the misses are caused by interthread communication (i.e., coherence misses) and if so, whether they are caused by true or false sharing (since the solutions for these two are quite different).

In this article, we propose a new programmer-centric definition of false sharing misses and describe our novel algorithm to perform coherence miss classification. We contrast our approach with existing data-centric definitions of false sharing. A straightforward implementation of our algorithm is too expensive to be incorporated in real hardware. Therefore, we explore the design space for low-cost hardware support that can classify coherence misses on-the-fly into true and false sharing misses, allowing existing performance counters and profiling tools to expose and attribute them. We find that our approximate schemes achieve good accuracy at only a fraction of the cost of the ideal scheme. Additionally, we demonstrate the usefulness of our work in a case study involving a real application.},
	Acmid = {1970389},
	Address = {New York, NY, USA},
	Articleno = {8},
	Author = {Venkataramani, Guru and Hughes, Christopher J. and Kumar, Sanjeev and Prvulovic, Milos},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:39:40 +0100},
	Doi = {10.1145/1970386.1970389},
	Issn = {1544-3566},
	Issue_Date = {July 2011},
	Journal = {ACM Trans. Archit. Code Optim.},
	Keywords = {Performance debugging, coherence misses, false sharing, multicore processors},
	Number = {2},
	Numpages = {27},
	Pages = {8:1--8:27},
	Publisher = {ACM},
	Title = {DeFT: Design Space Exploration for On-the-fly Detection of Coherence Misses},
	Url = {http://doi.acm.org/10.1145/1970386.1970389},
	Volume = {8},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1970386.1970389},
	Bdsk-Url-2 = {https://doi.org/10.1145/1970386.1970389}}

@article{Zhao:2005:IWA:1113841.1113842,
	Abstract = {Applications in embedded systems often need to meet specified timing constraints. It is advantageous to not only calculate the worst-case execution time (WCET) of an application, but to also perform transformation, which reduce the WCET, since an application with a lower WCET will be less likely to violate its timing constraints. Some processors incur a pipeline delay whenever an instruction transfers control to a target that is not the next sequential instruction. Code-positioning optimizations attempt to reduce these delays by positioning the basic blocks to minimize the number of unconditional jumps and taken conditional branches that occur. Traditional code-positioning algorithms use profile data to find the frequently executed edges between basic blocks, then minimize the transfers of control along these edges to reduce the average case execution time (ACET). This paper introduces a WCET code-positioning optimization, driven by the worst-case (WC) path information from a timing analyzer, to reduce the WCET instead of ACET. This WCET optimization changes the layout of the code in memory to reduce the branch penalties along the WC paths. Unlike the frequency of edges in traditional profile-driven code positioning, the WC path may change after code-positioning decisions are made. Thus, WCET code positioning is inherently more challenging than ACET code positioning. The experimental results show that this optimization typically finds the optimal layout of the basic blocks with the minimal WCET. The results show over a 7% reduction in WCET is achieved after code positioning is performed.},
	Acmid = {1113842},
	Address = {New York, NY, USA},
	Author = {Zhao, Wankang and Whalley, David and Healy, Christopher and Mueller, Frank},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:43:15 +0100},
	Doi = {10.1145/1113841.1113842},
	Issn = {1544-3566},
	Issue_Date = {December 2005},
	Journal = {ACM Trans. Archit. Code Optim.},
	Keywords = {WCET, code positioning, embedded systems},
	Number = {4},
	Numpages = {31},
	Pages = {335--365},
	Publisher = {ACM},
	Title = {Improving WCET by Applying a WC Code-positioning Optimization},
	Url = {http://doi.acm.org/10.1145/1113841.1113842},
	Volume = {2},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1113841.1113842},
	Bdsk-Url-2 = {https://doi.org/10.1145/1113841.1113842}}

@article{Jiang:2013:FAE:2491465.2491470,
	Abstract = {Due to the limited computational and energy resources available on existing wireless sensor platforms, achieving high-precision classification of high-level events in-network is a challenge. In this article, we present in-network implementations of a Bayesian classifier and a condensed kd-tree classifier for identifying events of interest on resource-lean embedded sensors. The first approach uses preprocessed sensor readings to derive a multidimensional Bayesian classifier used to classify sensor data in real time. The second introduces an innovative condensed kd-tree to represent preprocessed sensor data and uses a fast nearest-neighbor search to determine the likelihood of class membership for incoming samples. Both classifiers consume limited resources and provide high-precision classification. To evaluate each approach, two case studies are considered, in the contexts of human movement and vehicle navigation, respectively. The classification accuracy is above 85% for both classifiers across the two case studies.},
	Acmid = {2491470},
	Address = {New York, NY, USA},
	Articleno = {11},
	Author = {Jiang, Hao and Hallstrom, Jason O.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:41:37 +0100},
	Doi = {10.1145/2491465.2491470},
	Issn = {1556-4665},
	Issue_Date = {July 2013},
	Journal = {ACM Trans. Auton. Adapt. Syst.},
	Keywords = {Bayesian classification, Wireless sensor networks, classification, event detection, kd-tree},
	Number = {2},
	Numpages = {22},
	Pages = {11:1--11:22},
	Publisher = {ACM},
	Title = {Fast, Accurate Event Classification on Resource-Lean Embedded Sensors},
	Url = {http://doi.acm.org/10.1145/2491465.2491470},
	Volume = {8},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2491465.2491470},
	Bdsk-Url-2 = {https://doi.org/10.1145/2491465.2491470}}

@article{Urgaonkar:2008:ADP:1342171.1342172,
	Abstract = {Dynamic capacity provisioning is a useful technique for handling the multi-time-scale variations seen in Internet workloads. In this article, we propose a novel dynamic provisioning technique for multi-tier Internet applications that employs (1) a flexible queuing model to determine how much of the resources to allocate to each tier of the application, and (2) a combination of predictive and reactive methods that determine when to provision these resources, both at large and small time scales. We propose a novel data center architecture based on virtual machine monitors to reduce provisioning overheads. Our experiments on a forty-machine Xen/Linux-based hosting platform demonstrate the responsiveness of our technique in handling dynamic workloads. In one scenario where a flash crowd caused the workload of a three-tier application to double, our technique was able to double the application capacity within five minutes, thus maintaining response-time targets. Our technique also reduced the overhead of switching servers across applications from several minutes to less than a second, while meeting the performance targets of residual sessions.},
	Acmid = {1342172},
	Address = {New York, NY, USA},
	Articleno = {1},
	Author = {Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan and Wood, Timothy},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:37:41 +0100},
	Doi = {10.1145/1342171.1342172},
	Issn = {1556-4665},
	Issue_Date = {March 2008},
	Journal = {ACM Trans. Auton. Adapt. Syst.},
	Keywords = {Internet application, dynamic provisioning},
	Number = {1},
	Numpages = {39},
	Pages = {1:1--1:39},
	Publisher = {ACM},
	Title = {Agile Dynamic Provisioning of Multi-tier Internet Applications},
	Url = {http://doi.acm.org/10.1145/1342171.1342172},
	Volume = {3},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1342171.1342172},
	Bdsk-Url-2 = {https://doi.org/10.1145/1342171.1342172}}

@article{Yuan:2014:SSS:2578044.2555611,
	Abstract = {Self-protecting software systems are a class of autonomic systems capable of detecting and mitigating security threats at runtime. They are growing in importance, as the stovepipe static methods of securing software systems have been shown to be inadequate for the challenges posed by modern software systems. Self-protection, like other self-* properties, allows the system to adapt to the changing environment through autonomic means without much human intervention, and can thereby be responsive, agile, and cost effective. While existing research has made significant progress towards autonomic and adaptive security, gaps and challenges remain. This article presents a significant extension of our preliminary study in this area. In particular, unlike our preliminary study, here we have followed a systematic literature review process, which has broadened the scope of our study and strengthened the validity of our conclusions. By proposing and applying a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area, we have identified key patterns, trends and challenges in the existing approaches, which reveals a number of opportunities that will shape the focus of future research efforts.},
	Acmid = {2555611},
	Address = {New York, NY, USA},
	Articleno = {17},
	Author = {Yuan, Eric and Esfahani, Naeem and Malek, Sam},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:36:38 +0100},
	Doi = {10.1145/2555611},
	Issn = {1556-4665},
	Issue_Date = {January 2014},
	Journal = {ACM Trans. Auton. Adapt. Syst.},
	Keywords = {Self-protection, adaptive security, autonomic computing, self-* properties, self-adaptive systems},
	Number = {4},
	Numpages = {41},
	Pages = {17:1--17:41},
	Publisher = {ACM},
	Title = {A Systematic Survey of Self-Protecting Software Systems},
	Url = {http://doi.acm.org/10.1145/2555611},
	Volume = {8},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2555611},
	Bdsk-Url-2 = {https://doi.org/10.1145/2555611}}

@article{Swift:2005:IRC:1047915.1047919,
	Abstract = {Despite decades of research in extensible operating system technology, extensions such as device drivers remain a significant cause of system failures. In Windows XP, for example, drivers account for 85% of recently reported failures.This article describes Nooks, a reliability subsystem that seeks to greatly enhance operating system (OS) reliability by isolating the OS from driver failures. The Nooks approach is practical: rather than guaranteeing complete fault tolerance through a new (and incompatible) OS or driver architecture, our goal is to prevent the vast majority of driver-caused crashes with little or no change to the existing driver and system code. Nooks isolates drivers within lightweight protection domains inside the kernel address space, where hardware and software prevent them from corrupting the kernel. Nooks also tracks a driver's use of kernel resources to facilitate automatic cleanup during recovery.To prove the viability of our approach, we implemented Nooks in the Linux operating system and used it to fault-isolate several device drivers. Our results show that Nooks offers a substantial increase in the reliability of operating systems, catching and quickly recovering from many faults that would otherwise crash the system. Under a wide range and number of fault conditions, we show that Nooks recovers automatically from 99% of the faults that otherwise cause Linux to crash.While Nooks was designed for drivers, our techniques generalize to other kernel extensions. We demonstrate this by isolating a kernel-mode file system and an in-kernel Internet service. Overall, because Nooks supports existing C-language extensions, runs on a commodity operating system and hardware, and enables automated recovery, it represents a substantial step beyond the specialized architectures and type-safe languages required by previous efforts directed at safe extensibility.},
	Acmid = {1047919},
	Address = {New York, NY, USA},
	Author = {Swift, Michael M. and Bershad, Brian N. and Levy, Henry M.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:43:01 +0100},
	Doi = {10.1145/1047915.1047919},
	Issn = {0734-2071},
	Issue_Date = {February 2005},
	Journal = {ACM Trans. Comput. Syst.},
	Keywords = {I/O, Recovery, device drivers, protection, virtual memory},
	Number = {1},
	Numpages = {34},
	Pages = {77--110},
	Publisher = {ACM},
	Title = {Improving the Reliability of Commodity Operating Systems},
	Url = {http://doi.acm.org/10.1145/1047915.1047919},
	Volume = {23},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1047915.1047919},
	Bdsk-Url-2 = {https://doi.org/10.1145/1047915.1047919}}

@article{Hundhausen:2012:ESL:2395131.2395133,
	Abstract = {For over a century, studio-based instruction has served as an effective pedagogical model in architecture and fine arts education. Because of its design orientation, human-computer interaction (HCI) education is an excellent venue for studio-based instruction. In an HCI course, we have been exploring a studio-based learning activity called the prototype walkthrough, in which a student project team simulates its evolving user interface prototype while a student audience member acts as a test user. The audience is encouraged to ask questions and provide feedback. We have observed that prototype walkthroughs create excellent conditions for learning about user interface design. In order to better understand the educational value of the activity, we performed a content analysis of a video corpus of 16 prototype walkthroughs held in two HCI courses. We found that the prototype walkthrough discussions were dominated by relevant design issues. Moreover, mirroring the justification behavior of the expert instructor, students justified over 80 percent of their design statements and critiques, with nearly one-quarter of those justifications having a theoretical or empirical basis. Our findings suggest that PWs provide valuable opportunities for students to actively learn HCI design by participating in authentic practice, and provide insight into how such opportunities can be best promoted.},
	Acmid = {2395133},
	Address = {New York, NY, USA},
	Articleno = {26},
	Author = {Hundhausen, C. D. and Fairbrother, D. and Petre, M.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:38:28 +0100},
	Doi = {10.1145/2395131.2395133},
	Issn = {1073-0516},
	Issue_Date = {December 2012},
	Journal = {ACM Trans. Comput.-Hum. Interact.},
	Keywords = {HCI, Studio-based learning and instruction, design crit, prototype walkthrough, user interface design, video analysis},
	Number = {4},
	Numpages = {36},
	Pages = {26:1--26:36},
	Publisher = {ACM},
	Title = {An Empirical Study of the \&Ldquo;Prototype Walkthrough\&Rdquo;: A Studio-Based Activity for HCI Education},
	Url = {http://doi.acm.org/10.1145/2395131.2395133},
	Volume = {19},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2395131.2395133},
	Bdsk-Url-2 = {https://doi.org/10.1145/2395131.2395133}}

@article{Lysecky:2004:WP:1142980.1142986,
	Abstract = {We describe a new processing architecture, known as a warp processor, that utilizes a field-programmable gate array (FPGA) to improve the speed and energy consumption of a software binary executing on a microprocessor. Unlike previous approaches that also improve software using an FPGA but do so using a special compiler, a warp processor achieves these improvements completely transparently and operates from a standard binary. A warp processor dynamically detects the binary's critical regions, reimplements those regions as a custom hardware circuit in the FPGA, and replaces the software region by a call to the new hardware implementation of that region. While not all benchmarks can be improved using warp processing, many can, and the improvements are dramatically better than those achievable by more traditional architecture improvements. The hardest part of warp processing is that of dynamically reimplementing code regions on an FPGA, requiring partitioning, decompilation, synthesis, placement, and routing tools, all having to execute with minimal computation time and data memory so as to coexist on chip with the main processor. We describe the results of developing our warp processor. We developed a custom FPGA fabric specifically designed to enable lean place and route tools, and we developed extremely fast and efficient versions of partitioning, decompilation, synthesis, technology mapping, placement, and routing. Warp processors achieve overall application speedups of 6.3X with energy savings of 66% across a set of embedded benchmark applications. We further show that our tools utilize acceptably small amounts of computation and memory which are far less than traditional tools. Our work illustrates the feasibility and potential of warp processing, and we can foresee the possibility of warp processing becoming a feature in a variety of computing domains, including desktop, server, and embedded applications.},
	Acmid = {1142986},
	Address = {New York, NY, USA},
	Author = {Lysecky, Roman and Stitt, Greg and Vahid, Frank and Vahid, Frank},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:50:57 +0100},
	Doi = {10.1145/1142980.1142986},
	Issn = {1084-4309},
	Issue_Date = {July 2006},
	Journal = {ACM Trans. Des. Autom. Electron. Syst.},
	Keywords = {FPGA, Warp processors, configurable logic, dynamic optimization, hardware/software codesign, hardware/software partitioning, just-in-time (JIT) compilation},
	Number = {3},
	Numpages = {23},
	Pages = {659--681},
	Publisher = {ACM},
	Title = {Warp Processors},
	Url = {http://doi.acm.org/10.1145/1142980.1142986},
	Volume = {11},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1142980.1142986},
	Bdsk-Url-2 = {https://doi.org/10.1145/1142980.1142986}}

@article{Ostler:2007:IHT:1278349.1278361,
	Abstract = {Network processors incorporate several architectural features, including symmetric multiprocessing (SMP), block multithreading, and multiple memory elements, to support the high-performance requirements of current day applications. This article presents automated system-level design techniques for application development on such architectures. We propose integer linear programming formulations and heuristic techniques for process allocation and data mapping on SMP and block-multithreading-based network processors. The techniques incorporate process transformations and multithreading-aware data mapping to maximize the throughput of the application. The article presents experimental results that evaluate the techniques by implementing network processing applications on the Intel IXP 2400 architecture.},
	Acmid = {1278361},
	Address = {New York, NY, USA},
	Articleno = {48},
	Author = {Ostler, Chris and Chatha, Karam S. and Ramamurthi, Vijay and Srinivasan, Krishnan},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:42:27 +0100},
	Doi = {10.1145/1278349.1278361},
	Issn = {1084-4309},
	Issue_Date = {September 2007},
	Journal = {ACM Trans. Des. Autom. Electron. Syst.},
	Keywords = {block multithreading, multiprocessor},
	Number = {4},
	Pages = {48:1-48:40},
	Publisher = {ACM},
	Title = {ILP and Heuristic Techniques for System-level Design on Network Processor Architectures},
	Url = {http://doi.acm.org/10.1145/1278349.1278361},
	Volume = {12},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1278349.1278361},
	Bdsk-Url-2 = {https://doi.org/10.1145/1278349.1278361}}

@article{Raval:2010:LTT:1754405.1754408,
	Abstract = {In this article we describe a low-power processor platform for use in Wireless Sensor Network (WSN) nodes (motes). WSN motes are small, battery-powered devices comprised of a processor, sensors, and a radio frequency transceiver. It is expected that WSNs consisting of large numbers of motes will offer long-term, distributed monitoring, and control of real-world equipment and phenomena. A key requirement for these applications is long battery life. We investigate a processor platform architecture based on an application-specific programmable processor core, System-On-Chip bus, and a hardware accelerator. The architecture improves on the energy consumption of a conventional microprocessor design by tuning the architecture for a suite of TinyOS-based WSN applications. The tuning method used minimizes changes to the instruction set architecture facilitating rapid software migration to the new platform. The processor platform was implemented and validated in an FPGA-based WSN mote. The benefits of the approach in terms of energy consumption are estimated to be a reduction of 48% for ASIC implementation relative to a conventional programmable processor for a typical TinyOS application suite without use of voltage scaling.},
	Acmid = {1754408},
	Address = {New York, NY, USA},
	Articleno = {23},
	Author = {Raval, R. K. and Fernandez, C. H. and Bleakley, C. J.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:45:14 +0100},
	Doi = {10.1145/1754405.1754408},
	Issn = {1084-4309},
	Issue_Date = {May 2010},
	Journal = {ACM Trans. Des. Autom. Electron. Syst.},
	Keywords = {Embedded system design, Wireless Sensor Network, hardware-software codesign, low power processor},
	Number = {3},
	Numpages = {17},
	Pages = {23:1--23:17},
	Publisher = {ACM},
	Title = {Low-power TinyOS Tuned Processor Platform for Wireless Sensor Network Motes},
	Url = {http://doi.acm.org/10.1145/1754405.1754408},
	Volume = {15},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1754405.1754408},
	Bdsk-Url-2 = {https://doi.org/10.1145/1754405.1754408}}

@article{Bueno:2010:ORA:1698772.1698776,
	Abstract = {In this article, we study optimization of a RapidIO network and FPGA-based computation engines to address the taxing requirements of a set of real-time Ground-Moving Target Indicators (GMTI) and Synthetic Aperture Radar (SAR) kernels for Space-Based Radar (SBR). By employing a RapidIO hardware testbed and validated simulation, we determine key trade-offs in design of reconfigurable systems for GMTI and SAR in terms of processing, memory, and network throughput. In addition, we study considerations for timely delivery of latency-sensitive control traffic present in many satellite applications. Based on our results, we propose architectural modifications to further improve performance of SBR systems.},
	Acmid = {1698776},
	Address = {New York, NY, USA},
	Articleno = {18},
	Author = {Bueno, David and Conger, Chris and George, Alan D.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:46:53 +0100},
	Doi = {10.1145/1698772.1698776},
	Issn = {1539-9087},
	Issue_Date = {February 2010},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {FPGA, GMTI, RapidIO, SAR, radar, space},
	Number = {3},
	Numpages = {30},
	Pages = {18:1--18:30},
	Publisher = {ACM},
	Title = {Optimizing rapidIO Architectures for Onboard Processing},
	Url = {http://doi.acm.org/10.1145/1698772.1698776},
	Volume = {9},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1698772.1698776},
	Bdsk-Url-2 = {https://doi.org/10.1145/1698772.1698776}}

@article{Bueno:2007:RRP:1324969.1324970,
	Abstract = {Space-based radar is a suite of applications that presents many unique system design challenges. In this paper, we investigate use of RapidIO, a new high-performance embedded systems interconnect, in addressing issues associated with the high network bandwidth requirements of real-time ground moving target indicator (GMTI), and synthetic aperture Radar (SAR) applications in satellite systems. Using validated simulation, we study several critical issues related to the RapidIO network and algorithms under study. The results show that RapidIO is a promising platform for space-based radar using emerging technology, providing network bandwidth to enable parallel computation previously unattainable in an embedded satellite system.},
	Acmid = {1324970},
	Address = {New York, NY, USA},
	Articleno = {1},
	Author = {Bueno, David and Conger, Chris and George, Alan D. and Troxel, Ian and Leko, Adam},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:48:34 +0100},
	Doi = {10.1145/1324969.1324970},
	Issn = {1539-9087},
	Issue_Date = {December 2007},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {RapidIO, ground-moving target indicator, space-based radar, synthetic aperture radar},
	Number = {1},
	Numpages = {38},
	Pages = {1:1--1:38},
	Publisher = {ACM},
	Title = {RapidIO for Radar Processing in Advanced Space Systems},
	Url = {http://doi.acm.org/10.1145/1324969.1324970},
	Volume = {7},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1324969.1324970},
	Bdsk-Url-2 = {https://doi.org/10.1145/1324969.1324970}}

@article{Palem:2013:TYB:2465787.2465789,
	Abstract = {Well over a decade ago, many believed that an engine of growth driving the semiconductor and computing industries---captured nicely by Gordon Moore's remarkable prophecy (Moore's law)---was speeding towards a dangerous cliff-edge. Ranging from expressions of concern to doomsday scenarios, the exact time when serious hurdles would beset us varied quite a bit---some of the more optimistic warnings giving Moore's law until. Needless to say, a lot of people have spent time and effort with great success to find ways for substantially extending the time when we would encounter the dreaded cliff-edge, if not avoiding it altogether. Faced with this issue, we started approaching this in a decidedly different manner---one which suggested falling off the metaphorical cliff as a design choice, but in a controlled way. This resulted in devices that could switch and produce bits that are correct, namely of having the intended value, only with a probabilistic guarantee. As a result, the results could in fact be incorrect. Such devices and associated circuits and computing structures are now broadly referred to as inexact designs, circuits, and architectures. In this article, we will crystallize the essence of inexactness dating back to 2002 through two key principles that we developed: (i) that of admitting error in a design in return for resource savings, and subsequently (ii) making resource investments in the elements of a hardware platform proportional to the value of information they compute. We will also give a broad overview of a range of inexact designs and hardware concepts that our group and other groups around the world have been developing since, based on these two principles. Despite not being deterministically precise, inexact designs can be significantly more efficient in the energy they consume, their speed of execution, and their area needs, which makes them attractive in application contexts that are resilient to error. Significantly, our development of inexactness will be contrasted against the rich backdrop of traditional approaches aimed at realizing reliable computing from unreliable elements, starting with von Neumann's influential lectures and further developed by Shannon-Weaver and others.},
	Acmid = {2465789},
	Address = {New York, NY, USA},
	Articleno = {87},
	Author = {Palem, Krishna and Lingamneni, Avinash},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:53:43 +0100},
	Doi = {10.1145/2465787.2465789},
	Issn = {1539-9087},
	Issue_Date = {May 2013},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {Co-design, EDA, Moore's law, VLSI design, energy-accuracy trade-off, inexact circuit design, low power/energy, probabilistic CMOS},
	Number = {2s},
	Numpages = {23},
	Pages = {87:1--87:23},
	Publisher = {ACM},
	Title = {Ten Years of Building Broken Chips: The Physics and Engineering of Inexact Computing},
	Url = {http://doi.acm.org/10.1145/2465787.2465789},
	Volume = {12},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2465787.2465789},
	Bdsk-Url-2 = {https://doi.org/10.1145/2465787.2465789}}

@article{Seiculescu:2013:DBE:2485984.2485996,
	Abstract = {Many classes of applications require Quality of Service (QoS) guarantees from the system interconnect. In Networks-on-Chip (NoC) QoS guarantees usually translate into bandwidth and latency constraints for the traffic flows and require hardware support in the NoC fabric and its interfaces. In this article we present a novel NoC synthesis framework to automatically build networks that meet hard latency constraints of end-to-end traffic streams without requiring specialized hardware for the network components. The hard latency constraints are met by carefully designing the NoC topology and selecting the appropriate routes for flow using lean best-effort network components. We perform experiments on several System on Chip (SoC) benchmarks. We compared against a topology synthesis method with no support for real-time constraints and we show that the proposed method can produce topologies that can meet significantly tighter worst case latency constraints (on average 44%). We also show that the tightest worst case latency can be provided with little overhead on power consumption (on average 8.5%).},
	Acmid = {2485996},
	Address = {New York, NY, USA},
	Articleno = {108},
	Author = {Seiculescu, Ciprian and Rahmati, Dara and Murali, Srinivasan and Sarbazi-Azad, Hamid and Benini, Luca and Micheli, Giovanni De},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:39:59 +0100},
	Doi = {10.1145/2485984.2485996},
	Issn = {1539-9087},
	Issue_Date = {June 2013},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {NoC, topology, topology synthesis, worst case latency},
	Number = {4},
	Numpages = {23},
	Pages = {108:1--108:23},
	Publisher = {ACM},
	Title = {Designing Best Effort Networks-on-chip to Meet Hard Latency Constraints},
	Url = {http://doi.acm.org/10.1145/2485984.2485996},
	Volume = {12},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2485984.2485996},
	Bdsk-Url-2 = {https://doi.org/10.1145/2485984.2485996}}

@article{Vyas:2013:HAS:2514641.2514643,
	Abstract = {The field of modern control theory and the systems used to implement these controls have shown rapid development over the last 50 years. It was often the case that those developing control algorithms could assume the computing medium was solely dedicated to the task of controlling a plant, for example, the control algorithm being implemented in software on a dedicated Digital Signal Processor (DSP), or implemented in hardware using a simple dedicated Programmable Logic Device (PLD). As time progressed, the drive to place more system functionality in a single component (reducing power, cost, and increasing reliability) has made this assumption less often true. Thus, it has been pointed out by some experts in the field of control theory (e.g., Astrom) that those developing control algorithms must take into account the effects of running their algorithms on systems that will be shared with other tasks. One aspect of the work presented in this article is a hardware architecture that allows control developers to maintain this simplifying assumption. We focus specifically on the Proportional-Integral-Derivative (PID) controller. An on-chip coprocessor has been implemented that can scale to support servicing hundreds of plants, while maintaining microsecond-level response times, tight deterministic control loop timing, and allowing the main processor to service noncontrol tasks.

In order to control a plant, the controller needs information about the plant's state. Typically this information is obtained from sensors with which the plant has been instrumented. There are a number of common computations that may be performed on this sensor data before being presented to the controller (e.g., averaging and thresholding). Thus in addition to supporting PID algorithms, we have developed a Sensor Processing Unit (SPU) that off-loads these common sensor processing tasks from the main processor.

We have prototyped our ideas using Field Programmable Gate Array (FPGA) technology. Through our experimental results, we show our PID execution unit gives orders of magnitude improvement in response time when servicing many plants, as compared to a standard general software implementation. We also show that the SPU scales much better than a general software implementation. In addition, these execution units allow the simplifying assumption of dedicated computing medium to hold for control algorithm development.},
	Acmid = {2514643},
	Address = {New York, NY, USA},
	Articleno = {16},
	Author = {Vyas, Sudhanshu and Gupte, Adwait and Gill, Christopher D. and Cytron, Ron K. and Zambreno, Joseph and Jones, Phillip H.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:42:13 +0100},
	Doi = {10.1145/2514641.2514643},
	Issn = {1539-9087},
	Issue_Date = {September 2013},
	Journal = {ACM Trans. Embed. Comput. Syst.},
	Keywords = {Control systems, application-specific processor, real-time systems, reconfigurable hardware, sensor processing},
	Number = {2},
	Numpages = {25},
	Pages = {16:1--16:25},
	Publisher = {ACM},
	Title = {Hardware Architectural Support for Control Systems and Sensor Processing},
	Url = {http://doi.acm.org/10.1145/2514641.2514643},
	Volume = {13},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2514641.2514643},
	Bdsk-Url-2 = {https://doi.org/10.1145/2514641.2514643}}

@article{Johnson:2005:IZH:1095878.1095889,
	Abstract = {The classical Z-buffer visibility algorithm samples a scene at regularly spaced points on an image plane. Previously, we introduced an extension of this algorithm called the irregular Z-buffer that permits sampling of the scene from arbitrary points on the image plane. These sample points are stored in a two-dimensional spatial data structure. Here we present a set of architectural enhancements to the classical Z-buffer acceleration hardware which supports efficient execution of the irregular Z-buffer. These enhancements enable efficient parallel construction and query of certain irregular data structures, including the grid of linked lists used by our algorithm. The enhancements include flexible atomic read-modify-write units located near the memory controller, an internal routing network between these units and the fragment processors, and a MIMD fragment processor design. We simulate the performance of this new architecture and demonstrate that it can be used to render high-quality shadows in geometrically complex scenes at interactive frame rates. We also discuss other uses of the irregular Z-buffer algorithm and the implications of our architectural changes in the design of chip-multiprocessors.},
	Acmid = {1095889},
	Address = {New York, NY, USA},
	Author = {Johnson, Gregory S. and Lee, Juhyun and Burns, Christopher A. and Mark, William R.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:52:55 +0100},
	Doi = {10.1145/1095878.1095889},
	Issn = {0730-0301},
	Issue_Date = {October 2005},
	Journal = {ACM Trans. Graph.},
	Keywords = {Real-time graphics hardware, architecture, computer graphics, shadow algorithms, visible surface algorithms},
	Number = {4},
	Numpages = {21},
	Pages = {1462--1482},
	Publisher = {ACM},
	Title = {The Irregular Z-buffer: Hardware Acceleration for Irregular Data Structures},
	Url = {http://doi.acm.org/10.1145/1095878.1095889},
	Volume = {24},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1095878.1095889},
	Bdsk-Url-2 = {https://doi.org/10.1145/1095878.1095889}}

@article{Lanman:2011:PFD:2070781.2024220,
	Abstract = {We introduce polarization field displays as an optically-efficient design for dynamic light field display using multi-layered LCDs. Such displays consist of a stacked set of liquid crystal panels with a single pair of crossed linear polarizers. Each layer is modeled as a spatially-controllable polarization rotator, as opposed to a conventional spatial light modulator that directly attenuates light. Color display is achieved using field sequential color illumination with monochromatic LCDs, mitigating severe attenuation and moir{\'e} occurring with layered color filter arrays. We demonstrate such displays can be controlled, at interactive refresh rates, by adopting the SART algorithm to tomographically solve for the optimal spatially-varying polarization state rotations applied by each layer. We validate our design by constructing a prototype using modified off-the-shelf panels. We demonstrate interactive display using a GPU-based SART implementation supporting both polarization-based and attenuation-based architectures. Experiments characterize the accuracy of our image formation model, verifying polarization field displays achieve increased brightness, higher resolution, and extended depth of field, as compared to existing automultiscopic display methods for dual-layer and multi-layer LCDs.},
	Acmid = {2024220},
	Address = {New York, NY, USA},
	Articleno = {186},
	Author = {Lanman, Douglas and Wetzstein, Gordon and Hirsch, Matthew and Heidrich, Wolfgang and Raskar, Ramesh},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:47:38 +0100},
	Doi = {10.1145/2070781.2024220},
	Issn = {0730-0301},
	Issue_Date = {December 2011},
	Journal = {ACM Trans. Graph.},
	Keywords = {automultiscopic 3D displays, computational displays, light fields, multi-layer LCDs, tomography},
	Number = {6},
	Numpages = {10},
	Pages = {186:1--186:10},
	Publisher = {ACM},
	Title = {Polarization Fields: Dynamic Light Field Display Using Multi-layer LCDs},
	Url = {http://doi.acm.org/10.1145/2070781.2024220},
	Volume = {30},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2070781.2024220},
	Bdsk-Url-2 = {https://doi.org/10.1145/2070781.2024220}}

@article{Tocci:2011:VHV:2010324.1964936,
	Abstract = {Although High Dynamic Range (HDR) imaging has been the subject of significant research over the past fifteen years, the goal of acquiring cinema-quality HDR images of fast-moving scenes using available components has not yet been achieved. In this work, we present an optical architecture for HDR imaging that allows simultaneous capture of high, medium, and low-exposure images on three sensors at high fidelity with efficient use of the available light. We also present an HDR merging algorithm to complement this architecture, which avoids undesired artifacts when there is a large exposure difference between the images. We implemented a prototype high-definition HDR-video system and we present still frames from the acquired HDR video, tonemapped with various techniques.},
	Acmid = {1964936},
	Address = {New York, NY, USA},
	Articleno = {41},
	Author = {Tocci, Michael D. and Kiser, Chris and Tocci, Nora and Sen, Pradeep},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:37:13 +0100},
	Doi = {10.1145/2010324.1964936},
	Issn = {0730-0301},
	Issue_Date = {July 2011},
	Journal = {ACM Trans. Graph.},
	Keywords = {HDR video, merging HDR images},
	Number = {4},
	Numpages = {10},
	Pages = {41:1--41:10},
	Publisher = {ACM},
	Title = {A Versatile HDR Video Production System},
	Url = {http://doi.acm.org/10.1145/2010324.1964936},
	Volume = {30},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2010324.1964936},
	Bdsk-Url-2 = {https://doi.org/10.1145/2010324.1964936}}

@article{Choi:2013:ISI:2445560.2445563,
	Abstract = {With firms facing intense rivalry, globalization, and time-to-market pressures, the need for organizational agility assumes greater importance. One of the primary vehicles for achieving organizational agility is the use of agile information systems [IS] and the close alignment of information technologies [IT] with business. However, IS is often viewed as an impediment to organization agility. Recently, service-oriented architecture [SOA] has emerged as a prominent IS agility-enhancing technology. The fundamental question of how SOA can enhance organization agility and foster closer alignment between IT and business has not been adequately addressed. The dynamic interaction among external business environmental factors, organizational agility, and IS architecture makes the process of keeping IT and business aligned more complex. This study uses a design science approach to build a system dynamics model to examine the effect of employing alternative SOA implementation strategies in various organizational and external business environments on the IT business alignment and IS cost. The results provide insights into the shaping of IT-business alignment. Additionally, the system dynamics model serves as a tool for supporting managerial decisions related to SOA implementation.},
	Acmid = {2445563},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Choi, Jae and Nazareth, Derek L. and Jain, Hemant K.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:53:14 +0100},
	Doi = {10.1145/2445560.2445563},
	Issn = {2158-656X},
	Issue_Date = {April 2013},
	Journal = {ACM Trans. Manage. Inf. Syst.},
	Keywords = {IT-business alignment, Service-oriented architecture, information systems agility, system dynamics modeling},
	Number = {1},
	Numpages = {22},
	Pages = {3:1--3:22},
	Publisher = {ACM},
	Title = {The Impact of SOA Implementation on IT-Business Alignment: A System Dynamics Approach},
	Url = {http://doi.acm.org/10.1145/2445560.2445563},
	Volume = {4},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2445560.2445563},
	Bdsk-Url-2 = {https://doi.org/10.1145/2445560.2445563}}

@article{Yang:2013:SHW:2555810.2555811,
	Abstract = {Healthcare informatics has drawn substantial attention in recent years. Current work on healthcare informatics is highly interdisciplinary involving methodologies from computing, engineering, information science, behavior science, management science, social science, as well as many different areas in medicine and public health. Three major tracks, (i) systems, (ii) analytics, and (iii) human factors, can be identified. The systems track focuses on healthcare system architecture, framework, design, engineering, and application; the analytics track emphasizes data/information processing, retrieval, mining, analytics, as well as knowledge discovery; the human factors track targets the understanding of users or context, interface design, and user studies of healthcare applications. In this article, we discuss some of the latest development and introduce several articles selected for this special issue. We envision that the development of computing-oriented healthcare informatics research will continue to grow rapidly. The integration of different disciplines to advance the healthcare and wellbeing of our society will also be accelerated.},
	Acmid = {2555811},
	Address = {New York, NY, USA},
	Articleno = {15},
	Author = {Yang, Christopher C. and Leroy, Gondy and Ananiadou, Sophia},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:18:50 +0100},
	Doi = {10.1145/2555810.2555811},
	Issn = {2158-656X},
	Issue_Date = {December 2013},
	Journal = {ACM Trans. Manage. Inf. Syst.},
	Keywords = {Algorithms; Design; Experimentation; Human Factors; Security},
	Number = {4},
	Numpages = {8},
	Pages = {15:1--15:8},
	Publisher = {ACM},
	Title = {Smart Health and Wellbeing},
	Url = {http://doi.acm.org/10.1145/2555810.2555811},
	Volume = {4},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2555810.2555811},
	Bdsk-Url-2 = {https://doi.org/10.1145/2555810.2555811}}

@article{Lu:2008:DCR:1331897.1331901,
	Abstract = {Advancements in reconfigurable technologies, specifically FPGAs, have yielded faster, more power-efficient reconfigurable devices with enormous capacities. In our work, we provide testament to the impressive capacity of recent FPGAs by hosting a complete Pentium{\textregistered} in a single FPGA chip. In addition we demonstrate how FPGAs can be used for microprocessor design space exploration while overcoming the tension between simulation speed, model accuracy, and model completeness found in traditional software simulator environments. Specifically, we perform preliminary experimentation/prototyping with an original Socket 7 based desktop processor system with typical hardware peripherals running modern operating systems such as Fedora Core 4 and Windows XP; however we have inserted a Xilinx Virtex-4 in place of the processor that should sit in the motherboard and have used the Virtex-4 to host a complete version of the Pentium{\textregistered} microprocessor (which consumes less than half its resources). We can therefore apply architectural changes to the processor and evaluate their effects on the complete desktop system. We use this FPGA-based emulation system to conduct preliminary architectural experiments including growing the branch target buffer and the level 1 caches. In addition, we experimented with interfacing hardware accelerators such as DES and AES engines which resulted in a 27x speedup.},
	Acmid = {1331901},
	Address = {New York, NY, USA},
	Articleno = {5},
	Author = {Lu, Shih-Lien L. and Yiannacouras, Peter and Suh, Taeweon and Kassa, Rolf and Konow, Michael},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:35:44 +0100},
	Doi = {10.1145/1331897.1331901},
	Issn = {1936-7406},
	Issue_Date = {March 2008},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {FPGA, Pentium\textregistered, accelerator, architecture, emulator, exploration, model, operating system, processor, reconfigurable, simulator},
	Number = {1},
	Numpages = {15},
	Pages = {5:1--5:15},
	Publisher = {ACM},
	Title = {A Desktop Computer with a Reconfigurable Pentium\&Reg;},
	Url = {http://doi.acm.org/10.1145/1331897.1331901},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1331897.1331901},
	Bdsk-Url-2 = {https://doi.org/10.1145/1331897.1331901}}

@article{Neely:2013:RTH:2457443.2457448,
	Abstract = {The latest FPGA devices provide the headroom to implement large-scale and complex systems. A key requirement is the integration of modules from diverse sources to promote modular design and reuse. A contrary factor is that using dynamic partial reconfiguration typically requires low-level planning of the system implementation. In this article, we introduce ReShape: a high-level approach for designing reconfigurable systems by interconnecting modules, which gives a ``plug and play'' look and feel, is supported by tools that carry out implementation functions, and is carried through to support system reconfiguration during operation. The emphasis is on the inter-module connections and abstracting the communication patterns that are typical between modules: for example, the streaming of data, or the reading and writing of data to and from memory modules. The details of wiring and signaling are hidden from view, via metadata associated with individual modules. This setting allows system reconfiguration at the module level, both by supporting type checking of replacement modules and by managing the overall system implementation, via metadata associated with its FPGA floorplan. The methodology and tools have been implemented in a prototype targeted to a domain-specific setting---high-speed networking---and have been validated on real telecommunications design projects.},
	Acmid = {2457448},
	Address = {New York, NY, USA},
	Articleno = {5},
	Author = {Neely, Christopher E. and Brebner, Gordon and Shang, Weijia},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:49:24 +0100},
	Doi = {10.1145/2457443.2457448},
	Issn = {1936-7406},
	Issue_Date = {May 2013},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {Modular system design, high-level tools for FPGAs, modular system reconfiguration, networking systems},
	Number = {1},
	Numpages = {23},
	Pages = {5:1--5:23},
	Publisher = {ACM},
	Title = {ReShape: Towards a High-Level Approach to Design and Operation of Modular Reconfigurable Systems},
	Url = {http://doi.acm.org/10.1145/2457443.2457448},
	Volume = {6},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2457443.2457448},
	Bdsk-Url-2 = {https://doi.org/10.1145/2457443.2457448}}

@article{Yu:2009:VPS:1534916.1534922,
	Abstract = {Current FPGA soft processor systems use dedicated hardware modules or accelerators to speed up data-parallel applications. This work explores an alternative approach of using a soft vector processor as a general-purpose accelerator. The approach has the benefits of a purely software-oriented development model, a fixed ISA allowing parallel software and hardware development, a single accelerator that can accelerate multiple applications, and scalable performance from the same source code. With no hardware design experience needed, a software programmer can make area-versus-performance trade-offs by scaling the number of functional units and register file bandwidth with a single parameter. A soft vector processor can be further customized by a number of secondary parameters to add or remove features for a specific application to optimize resource utilization. This article introduces VIPERS, a soft vector processor architecture that maps efficiently into an FPGA and provides a scalable amount of performance for a reasonable amount of area. Compared to a Nios II/s processor, instances of VIPERS with 32 processing lanes achieve up to 44 speedup using up to 26 the area.},
	Acmid = {1534922},
	Address = {New York, NY, USA},
	Articleno = {12},
	Author = {Yu, Jason and Eagleston, Christopher and Chou, Christopher Han-Yu and Perreault, Maxime and Lemieux, Guy},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:51:27 +0100},
	Doi = {10.1145/1534916.1534922},
	Issn = {1936-7406},
	Issue_Date = {June 2009},
	Journal = {ACM Trans. Reconfigurable Technol. Syst.},
	Keywords = {Computer architecture, embedded processor, multimedia processing, parallelism, soft processor, vector processor},
	Number = {2},
	Numpages = {34},
	Pages = {12:1--12:34},
	Publisher = {ACM},
	Title = {Vector Processing As a Soft Processor Accelerator},
	Url = {http://doi.acm.org/10.1145/1534916.1534922},
	Volume = {2},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1534916.1534922},
	Bdsk-Url-2 = {https://doi.org/10.1145/1534916.1534922}}

@article{Jurdak:2013:ELG:2422966.2422980,
	Abstract = {GPS is a commonly used and convenient technology for determining absolute position in outdoor environments, but its high power consumption leads to rapid battery depletion in mobile devices. An obvious solution is to duty cycle the GPS module, which prolongs the device lifetime at the cost of increased position uncertainty while the GPS is off. This article addresses the trade-off between energy consumption and localization performance in a mobile sensor network application. The focus is on augmenting GPS location with more energy-efficient location sensors to bound position estimate uncertainty while GPS is off. Empirical GPS and radio contact data from a large-scale animal tracking deployment is used to model node mobility, radio performance, and GPS. Because GPS takes a considerable, and variable, time after powering up before it delivers a good position measurement, we model the GPS behavior through empirical measurements of two GPS modules. These models are then used to explore duty cycling strategies for maintaining position uncertainty within specified bounds. We then explore the benefits of using short-range radio contact logging alongside GPS as an energy-inexpensive means of lowering uncertainty while the GPS is off, and we propose strategies that use RSSI ranging and GPS back-offs to further reduce energy consumption. Results show that our combined strategies can cut node energy consumption by one third while still meeting application-specific positioning criteria.},
	Acmid = {2422980},
	Address = {New York, NY, USA},
	Articleno = {23},
	Author = {Jurdak, Raja and Corke, Peter and Cotillon, Alban and Dharman, Dhinesh and Crossman, Chris and Salagnac, Guillaume},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:41:06 +0100},
	Doi = {10.1145/2422966.2422980},
	Issn = {1550-4859},
	Issue_Date = {March 2013},
	Journal = {ACM Trans. Sen. Netw.},
	Keywords = {GPS, Localization, animal tracking, efficiency, energy, mobile, wireless sensor networks},
	Number = {2},
	Numpages = {33},
	Pages = {23:1--23:33},
	Publisher = {ACM},
	Title = {Energy-efficient Localization: GPS Duty Cycling with Radio Ranging},
	Url = {http://doi.acm.org/10.1145/2422966.2422980},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2422966.2422980},
	Bdsk-Url-2 = {https://doi.org/10.1145/2422966.2422980}}

@article{Miller:2010:RER:1806895.1806901,
	Abstract = {Retasking and remote programming of sensor networks is an essential functionality to make these networks practical and effective. As the availability of more capable sensor nodes increases and new functional implementations continue to be proposed, these large collections of wireless nodes will need the ability to update and upgrade the software packages they are running. In order to do this, the new binary file must be distributed to all nodes in the network. Making a physical connection with each individual node is impractical in large wireless networks. Standard flooding mechanisms are too energy-costly and computationally expensive and they may interfere with the network's current tasks. A reliable method for distributing new code or binary files to every node in a wireless sensor network is needed. We propose a reprogramming/retasking framework for sensor networks that is energy efficient, responsive, and reliable, while maintaining a stable network.},
	Acmid = {1806901},
	Address = {New York, NY, USA},
	Articleno = {6},
	Author = {Miller, Chris and Poellabauer, Christian},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:49:06 +0100},
	Doi = {10.1145/1806895.1806901},
	Issn = {1550-4859},
	Issue_Date = {August 2010},
	Journal = {ACM Trans. Sen. Netw.},
	Keywords = {Broadcast, energy efficiency, minimum energy broadcast, reliable distribution, reprogramming, retasking, sensor networks},
	Number = {1},
	Numpages = {32},
	Pages = {6:1--6:32},
	Publisher = {ACM},
	Title = {Reliable and Efficient Reprogramming in Sensor Networks},
	Url = {http://doi.acm.org/10.1145/1806895.1806901},
	Volume = {7},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1806895.1806901},
	Bdsk-Url-2 = {https://doi.org/10.1145/1806895.1806901}}

@article{Rajasegarar:2010:EAW:1653760.1653767,
	Abstract = {Anomalies in wireless sensor networks can occur due to malicious attacks, faulty sensors, changes in the observed external phenomena, or errors in communication. Defining and detecting these interesting events in energy-constrained situations is an important task in managing these types of networks. A key challenge is how to detect anomalies with few false alarms while preserving the limited energy in the network. In this article, we define different types of anomalies that occur in wireless sensor networks and provide formal models for them. We illustrate the model using statistical parameters on a dataset gathered from a real wireless sensor network deployment at the Intel Berkeley Research Laboratory. Our experiments with a novel distributed anomaly detection algorithm show that it can detect elliptical anomalies with exactly the same accuracy as that of a centralized scheme, while achieving a significant reduction in energy consumption in the network. Finally, we demonstrate that our model compares favorably to four other well-known schemes on four datasets.},
	Acmid = {1653767},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Rajasegarar, Sutharshan and Bezdek, James C. and Leckie, Christopher and Palaniswami, Marimuthu},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:40:48 +0100},
	Doi = {10.1145/1653760.1653767},
	Issn = {1550-4859},
	Issue_Date = {December 2009},
	Journal = {ACM Trans. Sen. Netw.},
	Keywords = {Elliptical anomalies, anomaly detection, multivariate analysis, outlier detection, security, wireless sensor networks},
	Number = {1},
	Numpages = {28},
	Pages = {7:1--7:28},
	Publisher = {ACM},
	Title = {Elliptical Anomalies in Wireless Sensor Networks},
	Url = {http://doi.acm.org/10.1145/1653760.1653767},
	Volume = {6},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1653760.1653767},
	Bdsk-Url-2 = {https://doi.org/10.1145/1653760.1653767}}

@article{Batory:2002:AET:505145.505147,
	Abstract = {This is a case study in the use of product-line architectures (PLAs) and domain-specific languages (DSLs) to design an extensible command-and-control simulator for Army fire support. The reusable components of our PLA are layers or "aspects" whose addition or removal simultaneously impacts the source code of multiple objects in multiple, distributed programs. The complexity of our component specifications is substantially reduced by using a DSL for defining and refining state machines, abstractions that are fundamental to simulators. We present preliminary results that show how our PLA and DSL synergistically produce a more flexible way of implementing state-machine-based simulators than is possible with a pure Java implementation.},
	Acmid = {505147},
	Address = {New York, NY, USA},
	Author = {Batory, Don and Johnson, Clay and MacDonald, Bob and von Heeder, Dale},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:37:28 +0100},
	Doi = {10.1145/505145.505147},
	Issn = {1049-331X},
	Issue_Date = {April 2002},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Keywords = {GenVoca, aspects, domain-specific languages, refinements, simulation},
	Number = {2},
	Numpages = {24},
	Pages = {191--214},
	Publisher = {ACM},
	Title = {Achieving Extensibility Through Product-lines and Domain-specific Languages: A Case Study},
	Url = {http://doi.acm.org/10.1145/505145.505147},
	Volume = {11},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/505145.505147},
	Bdsk-Url-2 = {https://doi.org/10.1145/505145.505147}}

@article{Zambonelli:2003:DMS:958961.958963,
	Abstract = {Systems composed of interacting autonomous agents offer a promising software engineering approach for developing applications in complex domains. However, this multiagent system paradigm introduces a number of new abstractions and design/development issues when compared with more traditional approaches to software development. Accordingly, new analysis and design methodologies, as well as new tools, are needed to effectively engineer such systems. Against this background, the contribution of this article is twofold. First, we synthesize and clarify the key abstractions of agent-based computing as they pertain to agent-oriented software engineering. In particular, we argue that a multiagent system can naturally be viewed and architected as a computational organization, and we identify the appropriate organizational abstractions that are central to the analysis and design of such systems. Second, we detail and extend the Gaia methodology for the analysis and design of multiagent systems. Gaia exploits the aforementioned organizational abstractions to provide clear guidelines for the analysis and design of complex and open software systems. Two representative case studies are introduced to exemplify Gaia's concepts and to show its use and effectiveness in different types of multiagent system.},
	Acmid = {958963},
	Address = {New York, NY, USA},
	Author = {Zambonelli, Franco and Jennings, Nicholas R. and Wooldridge, Michael},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:40:34 +0100},
	Doi = {10.1145/958961.958963},
	Issn = {1049-331X},
	Issue_Date = {July 2003},
	Journal = {ACM Trans. Softw. Eng. Methodol.},
	Keywords = {Multiagent systems, agent-oriented software engineering, analysis and design methodologies, distributed systems, software architectures},
	Number = {3},
	Numpages = {54},
	Pages = {317--370},
	Publisher = {ACM},
	Title = {Developing Multiagent Systems: The Gaia Methodology},
	Url = {http://doi.acm.org/10.1145/958961.958963},
	Volume = {12},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/958961.958963},
	Bdsk-Url-2 = {https://doi.org/10.1145/958961.958963}}

@article{Grier:2011:DIO:1961659.1961665,
	Abstract = {Current web browsers are plagued with vulnerabilities, providing hackers with easy access to computer systems via browser-based attacks. Browser security efforts that retrofit existing browsers have had limited success because the design of modern browsers is fundamentally flawed. To enable more secure web browsing, we design and implement a new browser, called the OP web browser, that attempts to improve the state-of-the-art in browser security. We combine operating system design principles with formal methods to design a more secure web browser by drawing on the expertise of both communities. Our design philosophy is to partition the browser into smaller subsystems and make all communication between subsystems simple and explicit. At the core of our design is a small browser kernel that manages the browser subsystems and interposes on all communications between them to enforce our new browser security features.

To show the utility of our browser architecture, we design and implement three novel security features. First, we develop flexible security policies that allow us to include browser plugins within our security framework. Second, we use formal methods to prove useful security properties including user interface invariants and browser security policy. Third, we design and implement a browser-level information-flow tracking system to enable post-mortem analysis of browser-based attacks.

In addition to presenting the OP browser architecture, we discuss the design and implementation of a second version of OP, OP2, that includes features from other secure web browser designs to improve on the overall security and performance of OP. To evaluate our design, we implemented OP2 and tested both performance, memory, and filesystem impact while browsing popular pages. We show that the additional security features in OP and OP2 introduce minimal overhead.},
	Acmid = {1961665},
	Address = {New York, NY, USA},
	Articleno = {11},
	Author = {Grier, Chris and Tang, Shuo and King, Samuel T.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:40:19 +0100},
	Doi = {10.1145/1961659.1961665},
	Issn = {1559-1131},
	Issue_Date = {May 2011},
	Journal = {ACM Trans. Web},
	Keywords = {OP browser, Web browsing, browser plugin, formal verification, security},
	Number = {2},
	Numpages = {35},
	Pages = {11:1--11:35},
	Publisher = {ACM},
	Title = {Designing and Implementing the OP and OP2 Web Browsers},
	Url = {http://doi.acm.org/10.1145/1961659.1961665},
	Volume = {5},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1961659.1961665},
	Bdsk-Url-2 = {https://doi.org/10.1145/1961659.1961665}}

@article{Chung:2002:DDC:563952.563940,
	Abstract = {VLOS is a project investigating the suitability of a tuple space-based distributed operating system for running computationally intensive distributed applications on clusters of commodity (i.e., Intel{\texttrademark}PC based) hardware. Unlike previous efforts, UNIX backwards compatibility is not a goal of VLOS.In order to provide task-level granularity security for tuple spaces, field types and type signatures, a capability-based system has been developed for VLOS. The distributed capability system uses an arbitration scheme in which the nodes in a cluster of commodity workstations elect a single node to manage namespace allocations, to ensure that there are no collisions in the assignment of new tuple object identities.Initial implementations have been developed to run as Linux applications and to execute "stand-alone" with the support of the Flux OSKit. Trials show that the distributed capability security system for VLOS meets our initial design requirements, and is effective in providing (distributed) system-wide protection of tuple spaces, type signatures and field types},
	Acmid = {563940},
	Address = {Los Alamitos, CA, USA},
	Author = {Chung, V L and McDonald, C S},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:14:15 +0100},
	Issue_Date = {January-February 2002},
	Journal = {Aust. Comput. Sci. Commun.},
	Number = {3},
	Numpages = {8},
	Pages = {57--64},
	Publisher = {IEEE Computer Society Press},
	Title = {The Development of a Distributed Capability System for VLOS},
	Url = {http://dl.acm.org/citation.cfm?id=563952.563940},
	Volume = {24},
	Year = {2002},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=563952.563940}}

@article{Luo:2002:PMP:563952.563943,
	Abstract = {The micro-threaded microprocessor is a chip multi-processor, which uses a multi-threaded approach, where the threads are obtained from within a single context and exploit both vector and instruction level parallelism (ILP). This approach employs vertical and horizontal transfer in a simple pipeline. The horizontal transfer is referred to as the normal scalar pipeline processing used in most microprocessors. Vertical transfer is a context switch, which allows the code to tolerate any latency from undetermined data and control dependencies. The performance of the single pipeline is very important in the overall performance of the whole processor, which can distribute threads to any of the available processors. We have measured the influence of three crucial parameters - cache delay, cache miss rate, and number of registers - on the performance using our simulator. Even for a long cache delay (1000 processor cycles) we found that the micro-threaded pipeline can still achieves an IPC of 0.8 in the peak performance which is some 6 times better than a conventional scalar pipeline. If we further degrade cache performance by using an artificially small cache line size the performance of conventional scalar pipeline gives an IPC of 0.02, whereas with unlimited registers the micro-threaded pipeline still manages to achieve and IPC of 0.8 (a factor of 40 difference in performance).},
	Acmid = {563943},
	Address = {Los Alamitos, CA, USA},
	Author = {Luo, Bing and Jesshope, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:47:24 +0100},
	Issue_Date = {January-February 2002},
	Journal = {Aust. Comput. Sci. Commun.},
	Keywords = {micro thread, micro-threaded pipeline, performance evaluation, scalar pipeline},
	Number = {3},
	Numpages = {8},
	Pages = {83--90},
	Publisher = {IEEE Computer Society Press},
	Title = {Performance of a Micro-threaded Pipeline},
	Url = {http://dl.acm.org/citation.cfm?id=563952.563943},
	Volume = {24},
	Year = {2002},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=563952.563943}}

@article{Batra:2009:MAP:1562164.1562200,
	Abstract = {Frustration with the bureaucratic nature of the disciplined approach has led to the call for agile development. The new approach is defined by the Agile Manifesto (http://agilemanifesto.org/), which values individuals and interactions over processes and tools, working software over comprehensive documentation, customer collaboration over contract negotiation, and agility in responding to change over following a prescribed plan. Agile development does not focus on process improvement; instead it focuses on customer satisfaction and employee empowerment. This is evident from reading the stated values and principles of the Agile Manifesto, which include fairly extreme positions such as "welcome changing requirements, even late in development" and "the best architectures, requirements, and designs emerge from self-organizing teams."

An interesting issue arising from the call for agile development is its role in distributed development, which usually translates to offshore development. A recent study indicates that agile practices can reduce temporal, geographical, and socio-cultural distances in distributed development projects. The study researched agile development between teams located in the U.S. and Ireland, and while it reported that overall communication was improved, it also noted problems related to geographical, temporal, and even language distances. Although there are other reported successes of distributed agile development, the projects are generally small, the team members are likely familiar with each other, and the participants are largely experts or high caliber developers.

This raises a research, as well as a practical, question: can we extend the use of agile practices from small projects to medium and large projects that involve a significant outsourcing component? To address this question, we must drop constraints such as small size projects, and expert developers belonging to the same company, and examine problems arising from geographical, temporal, and cultural distances. Accordingly, agile practices may need to be modified.

In this article, the key issues of software projects with an outsourced component are first identified. These issues are then used as a background to evaluate how standard agile practices stand up when applied to larger projects. This evaluation is followed by recommendations for modified agile practices for outsourced software projects.},
	Acmid = {1562200},
	Address = {New York, NY, USA},
	Author = {Batra, Dinesh},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:24:29 +0100},
	Doi = {10.1145/1562164.1562200},
	Issn = {0001-0782},
	Issue_Date = {September 2009},
	Journal = {Commun. ACM},
	Number = {9},
	Numpages = {6},
	Pages = {143--148},
	Publisher = {ACM},
	Title = {Modified Agile Practices for Outsourced Software Projects},
	Url = {http://doi.acm.org/10.1145/1562164.1562200},
	Volume = {52},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1562164.1562200},
	Bdsk-Url-2 = {https://doi.org/10.1145/1562164.1562200}}

@article{Dean:2008:MSD:1327452.1327492,
	Abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
	Acmid = {1327492},
	Address = {New York, NY, USA},
	Author = {Dean, Jeffrey and Ghemawat, Sanjay},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:25:50 +0100},
	Doi = {10.1145/1327452.1327492},
	Issn = {0001-0782},
	Issue_Date = {January 2008},
	Journal = {Commun. ACM},
	Number = {1},
	Numpages = {7},
	Pages = {107--113},
	Publisher = {ACM},
	Title = {MapReduce: Simplified Data Processing on Large Clusters},
	Url = {http://doi.acm.org/10.1145/1327452.1327492},
	Volume = {51},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1327452.1327492},
	Bdsk-Url-2 = {https://doi.org/10.1145/1327452.1327492}}

@article{Edmondson-Yurkanan:2007:SAJ:1230819.1230840,
	Abstract = {Documenting the technical history of a SIG is indeed a priceless resource.},
	Acmid = {1230840},
	Address = {New York, NY, USA},
	Author = {Edmondson-Yurkanan, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:19:09 +0100},
	Doi = {10.1145/1230819.1230840},
	Issn = {0001-0782},
	Issue_Date = {May 2007},
	Journal = {Commun. ACM},
	Number = {5},
	Numpages = {6},
	Pages = {63--68},
	Publisher = {ACM},
	Title = {SIGCOMM's Archaeological Journey into Networking's Past},
	Url = {http://doi.acm.org/10.1145/1230819.1230840},
	Volume = {50},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1230819.1230840},
	Bdsk-Url-2 = {https://doi.org/10.1145/1230819.1230840}}

@article{Edwards:2013:LPW:2534706.2534711,
	Abstract = {While significant obstacles remain, researchers are optimistic about using DNA to guide graphene into complex circuit shapes on silicon.},
	Acmid = {2534711},
	Address = {New York, NY, USA},
	Author = {Edwards, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:26:14 +0100},
	Doi = {10.1145/2534706.2534711},
	Issn = {0001-0782},
	Issue_Date = {December 2013},
	Journal = {Commun. ACM},
	Number = {12},
	Numpages = {3},
	Pages = {13--15},
	Publisher = {ACM},
	Title = {Life Points the Way to a New Template for Chipmaking},
	Url = {http://doi.acm.org/10.1145/2534706.2534711},
	Volume = {56},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2534706.2534711},
	Bdsk-Url-2 = {https://doi.org/10.1145/2534706.2534711}}

@article{Haines:2010:SAM:1787234.1787269,
	Abstract = {Software development practices have evolved substantially during the past decade. As so called "agile" approaches have gained more acceptance and applications have become progressively more distributed in terms of their physical execution and the development of components, the service-oriented approach to IT architecture has become an important alternative to traditional software development. Another impetus for the trend to a Service-Oriented Architecture (SOA) is provided by enterprise system vendors as they are incorporating the service-oriented paradigm into their products. Substantial efforts related to open standards (such as Web service standards) and open source products (such as open source enterprise service bus, development tools) are further driving a service-oriented approach for information systems.

A key question is whether SOA adopters are going to be ready for this change and whether they can provide a technical and an organizational environment in which SOA-related technologies can be leveraged to their full potential. There is some indication that currently this may not be the case. In fact, some organizations that have embarked on SOA-related projects early have experienced disappointments. As with other technology waves, the important question is not whether SOA is inherently a good or a bad idea, but rather how it can be done right in a given context. This article tries to answer this question with respect to the software development process.

While much of the literature, both in academia and industry, has focused on business implications of SOA, technological realization, architectural issues, and implementation guidelines, few publications have addressed the impact of SOA on the software development process and its methodology. As with any organizational change, modifications to software development processes or practices entail switching cost. Therefore, individuals as well as organizations are inclined to stay with "proven" methodologies, although adjustments based on task requirements and technology characteristics should be key drivers for the methodology choice and are needed to help adopters leverage the full potential of SOA.

This article examines the differences and discusses which parts of development process and methodology may require adjustments to effectively leverage a SOA. It presents the results of a field study suggesting changes to software development practices that are necessary to accommodate the unique properties of the service-oriented approach.},
	Acmid = {1787269},
	Address = {New York, NY, USA},
	Author = {Haines, Marc N. and Rothenberger, Marcus A.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:28:40 +0100},
	Doi = {10.1145/1787234.1787269},
	Issn = {0001-0782},
	Issue_Date = {August 2010},
	Journal = {Commun. ACM},
	Number = {8},
	Numpages = {6},
	Pages = {135--140},
	Publisher = {ACM},
	Title = {How a Service-oriented Architecture May Change the Software Development Process},
	Url = {http://doi.acm.org/10.1145/1787234.1787269},
	Volume = {53},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1787234.1787269},
	Bdsk-Url-2 = {https://doi.org/10.1145/1787234.1787269}}

@article{Houston:2004:VDA:1012037.1012062,
	Abstract = {How to expose the internal 3D structures of multiplayer games and architectural models by automatically generating interactive exploded views.},
	Acmid = {1012062},
	Address = {New York, NY, USA},
	Author = {Houston, Mike and Niederauer, Chris and Agrawala, Maneesh and Humphreys, Greg},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:12:05 +0100},
	Doi = {10.1145/1012037.1012062},
	Issn = {0001-0782},
	Issue_Date = {August 2004},
	Journal = {Commun. ACM},
	Number = {8},
	Numpages = {6},
	Pages = {54--59},
	Publisher = {ACM},
	Title = {Visualizing Dynamic Architectural Environments},
	Url = {http://doi.acm.org/10.1145/1012037.1012062},
	Volume = {47},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1012037.1012062},
	Bdsk-Url-2 = {https://doi.org/10.1145/1012037.1012062}}

@article{Kobryn:2002:UAA:502269.502306,
	Abstract = {The UML sits at an architectural crossroad. Will UML 2.0 resolve the problems of UML 1.x or will it succumb to the dreaded second-language syndrome?},
	Acmid = {502306},
	Address = {New York, NY, USA},
	Author = {Kobryn, Cris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:11:41 +0100},
	Doi = {10.1145/502269.502306},
	Issn = {0001-0782},
	Issue_Date = {January 2002},
	Journal = {Commun. ACM},
	Number = {1},
	Numpages = {4},
	Pages = {107--110},
	Publisher = {ACM},
	Title = {Will UML 2.0 Be Agile or Awkward?},
	Url = {http://doi.acm.org/10.1145/502269.502306},
	Volume = {45},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/502269.502306},
	Bdsk-Url-2 = {https://doi.org/10.1145/502269.502306}}

@article{Papazoglou:2007:BPD:1290958.1290966,
	Abstract = {An innovative roadmap brings together the worlds of business processes and Web services, harnessing their power to construct industrial-strength business applications.},
	Acmid = {1290966},
	Address = {New York, NY, USA},
	Author = {Papazoglou, Michael P. and van den Heuvel, Willem-Jan},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:30:57 +0100},
	Doi = {10.1145/1290958.1290966},
	Issn = {0001-0782},
	Issue_Date = {October 2007},
	Journal = {Commun. ACM},
	Number = {10},
	Numpages = {7},
	Pages = {79--85},
	Publisher = {ACM},
	Title = {Business Process Development Life Cycle Methodology},
	Url = {http://doi.acm.org/10.1145/1290958.1290966},
	Volume = {50},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1290958.1290966},
	Bdsk-Url-2 = {https://doi.org/10.1145/1290958.1290966}}

@article{Ramnath:2005:ISS:1060710.1060740,
	Abstract = {City governments face difficult challenges in serving their increasingly Net-connected constituencies in an environment of change, uncertain demand, and reduced budgets. These conditions require their IT departments to enable governments to adapt to citizen requests in a sense-and-respond (S-R) manner. In this article, the application of S-R concepts is demonstrated by the approaches used in developing an IT strategic plan for Columbus, Ohio. The fractal-based, request-focused strategy used here creates a unified organizational and IT context for connecting the Department of Technology and city government departments to their customers by utilizing an incremental, lean portfolio-management-based action plan and architecture.},
	Acmid = {1060740},
	Address = {New York, NY, USA},
	Author = {Ramnath, Rajiv and Landsbergen, David},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:27:20 +0100},
	Doi = {10.1145/1060710.1060740},
	Issn = {0001-0782},
	Issue_Date = {May 2005},
	Journal = {Commun. ACM},
	Number = {5},
	Numpages = {7},
	Pages = {58--64},
	Publisher = {ACM},
	Title = {IT-enabled Sense-and-respond Strategies in Complex Public Organizations},
	Url = {http://doi.acm.org/10.1145/1060710.1060740},
	Volume = {48},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1060710.1060740},
	Bdsk-Url-2 = {https://doi.org/10.1145/1060710.1060740}}

@article{Richardson:2009:ODL:1498765.1498783,
	Abstract = {Dynamic languages offer a taste of object-relational mapping that eases application code.},
	Acmid = {1498783},
	Address = {New York, NY, USA},
	Author = {Richardson, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:23:23 +0100},
	Doi = {10.1145/1498765.1498783},
	Issn = {0001-0782},
	Issue_Date = {April 2009},
	Journal = {Commun. ACM},
	Number = {4},
	Numpages = {8},
	Pages = {48--55},
	Publisher = {ACM},
	Title = {ORM in Dynamic Languages},
	Url = {http://doi.acm.org/10.1145/1498765.1498783},
	Volume = {52},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1498765.1498783},
	Bdsk-Url-2 = {https://doi.org/10.1145/1498765.1498783}}

@article{Rossbach:2008:TMT:1378727.1378747,
	Abstract = {TxLinux is the first operating system to use hardware trans-actional memory (HTM) as a synchronization primitive, and the  first  to  manage  HTM  in  the  scheduler.  TxLinux,  which is a modification of Linux, is the first real-scale benchmark for transactional memory (TM). MetaTM is a modification of the x86 architecture that supports HTM in general and TxLi-nux specifically.This paper describes and measures TxLinux and MetaTM, the  HTM  model  that  supports  it.  TxLinux  greatly  benefits from  a  new  primitive,  called  the  cooperative  transactional spinlock  (cxspinlock)  that  allows  locks  and  transactions  to protect  the  same  data  while  maintaining  the  advantages of  both  synchronization  primitives.  Integrating  the  TxLi-nux  scheduler  with  the  MetaTM's  architectural  support  for HTM  eliminates  priority  inversion  for  several  real-world benchmarks},
	Acmid = {1378747},
	Address = {New York, NY, USA},
	Author = {Rossbach, Christopher J. and Ramadan, Hany E. and Hofmann, Owen S. and Porter, Donald E. and Bhandari, Aditya and Witchel, Emmett},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:13:18 +0100},
	Doi = {10.1145/1378727.1378747},
	Issn = {0001-0782},
	Issue_Date = {September 2008},
	Journal = {Commun. ACM},
	Number = {9},
	Numpages = {9},
	Pages = {83--91},
	Publisher = {ACM},
	Title = {TxLinux and MetaTM: Transactional Memory and the Operating System},
	Url = {http://doi.acm.org/10.1145/1378727.1378747},
	Volume = {51},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1378727.1378747},
	Bdsk-Url-2 = {https://doi.org/10.1145/1378727.1378747}}

@article{Stiegler:2006:PVC:1151030.1151033,
	Abstract = {It limits the damage a virus can do by using the operating system's own security mechanisms to enforce the Principle of Least Authority on individual applications.},
	Acmid = {1151033},
	Address = {New York, NY, USA},
	Author = {Stiegler, Marc and Karp, Alan H. and Yee, Ka-Ping and Close, Tyler and Miller, Mark S.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:23:09 +0100},
	Doi = {10.1145/1151030.1151033},
	Issn = {0001-0782},
	Issue_Date = {September 2006},
	Journal = {Commun. ACM},
	Number = {9},
	Numpages = {6},
	Pages = {83--88},
	Publisher = {ACM},
	Title = {Polaris: Virus-safe Computing for Windows XP},
	Url = {http://doi.acm.org/10.1145/1151030.1151033},
	Volume = {49},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1151030.1151033},
	Bdsk-Url-2 = {https://doi.org/10.1145/1151030.1151033}}

@article{Rhalibi:2010:CHW:1899687.1899690,
	Abstract = {The MPEG-4 standards define a technique for 3D facial and body model animations (FAPS/BAPS respectively), as seen in animation systems such as Greta. The way this technique works is in contrast to the set of animation techniques currently used within modern games technologies and applications, which utilize more advanced, expressive animation systems such as Skeletal, Morph Target, and Inverse Kinematics. This article describes an object-oriented, Java-based framework for the integration and transformation of MPEG4 standards-compliant animation streams known as Charisma. Charisma is designed for use with modern games animation systems; this article illustrates the application of this framework on top of our Java/OpenGL-based games engine framework known as Homura.},
	Acmid = {1899690},
	Address = {New York, NY, USA},
	Articleno = {8},
	Author = {Rhalibi, Abdennour El and Carter, Chris and Cooper, Simon and Merabti, Madjid and Price, Marc},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:39:21 +0100},
	Doi = {10.1145/1899687.1899690},
	Issn = {1544-3574},
	Issue_Date = {December 2010},
	Journal = {Comput. Entertain.},
	Keywords = {Charisma, FAPS, Homura, MPEG-4, Skinning, facial animation},
	Number = {2},
	Numpages = {15},
	Pages = {8:1--8:15},
	Publisher = {ACM},
	Title = {Charisma: High-performance Web-based MPEG-compliant Animation Framework},
	Url = {http://doi.acm.org/10.1145/1899687.1899690},
	Volume = {8},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1899687.1899690},
	Bdsk-Url-2 = {https://doi.org/10.1145/1899687.1899690}}

@article{Speed:2011:ITE:1962438.1962445,
	Abstract = {On Heritage aims to offer and promote a rich discussion at the intersection of art, performance, and culture that expands the boundaries of HCI while broadening our understanding of how things of the past come to matter in the present.},
	Acmid = {1962445},
	Address = {New York, NY, USA},
	Author = {Speed, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:32:15 +0100},
	Doi = {10.1145/1962438.1962445},
	Issn = {1072-5520},
	Issue_Date = {May + June 2011},
	Journal = {interactions},
	Number = {3},
	Numpages = {4},
	Pages = {18--21},
	Publisher = {ACM},
	Title = {An Internet of Things That Do Not Exist},
	Url = {http://doi.acm.org/10.1145/1962438.1962445},
	Volume = {18},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1962438.1962445},
	Bdsk-Url-2 = {https://doi.org/10.1145/1962438.1962445}}

@article{Rupf:2004:THC:1040151.1040176,
	Abstract = {Knowledge about the history of computing can provide students with a broad perspective and valuable insights into the "real world" and how things came to be the way that they are. However, there is precious little time in today's crowded computer science curriculum to devote to this subject matter. This paper investigated a leaning technique in which a significant "history component" was added to a computer architecture course. The students studied the history component entirely on their own and completely outside the classroom. An analysis of student opinions indicates that the students greatly appreciated the opportunity to learn about the history of computing and that the technique was very successful -- and is likely to be equally applicable in other academic areas.},
	Acmid = {1040176},
	Address = {USA},
	Author = {Rupf, John A.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:15:44 +0100},
	Issn = {1937-4771},
	Issue_Date = {December 2004},
	Journal = {J. Comput. Sci. Coll.},
	Number = {2},
	Numpages = {7},
	Pages = {212--218},
	Publisher = {Consortium for Computing Sciences in Colleges},
	Title = {Teaching the History of Computing (Painlessly)},
	Url = {http://dl.acm.org/citation.cfm?id=1040151.1040176},
	Volume = {20},
	Year = {2004},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1040151.1040176}}

@article{Maly:2001:IJD:376697.376703,
	Abstract = {We used our Original Interactive Remote Instruction (IRI) system to teach scores of university classes over the past years at sites up to 300 km apart. While this system is a prototype, its use in real classes allows us to deal with crucial issues in distributed education instruction systems. We describe our motivation and vision for a reimplementation of IRI that supports synchronous and asynchronous distance education. This new version, called IRI-h (h for hetergeneous), is coded in Java and executes on several different platforms. IRI-h extends IRI both to multiple platforms and heterogeneous network experiences with the developing prototype, including preliminary performance evaluation, and also unresolved issues still to be addressed.

},
	Acmid = {376703},
	Address = {New York, NY, USA},
	Articleno = {8},
	Author = {Maly, R. and Abdel-Wahab, H. and Wild, C. and Overstreet, C. M. and Gupta, A. and Abdel-Hamid, A. and Ghanem, S. and Gonzalez, A. and Zhu, X.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:43:46 +0100},
	Doi = {10.1145/376697.376703},
	Issn = {1531-4278},
	Issue_Date = {March 2001},
	Journal = {J. Educ. Resour. Comput.},
	Keywords = {Java, heterogeneity, platform independence},
	Number = {1es},
	Pages = {1-15},
	Publisher = {ACM},
	Title = {IRI-h, a Java-based Distance Education System: Architecture and Performance},
	Url = {http://doi.acm.org/10.1145/376697.376703},
	Volume = {1},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/376697.376703},
	Bdsk-Url-2 = {https://doi.org/10.1145/376697.376703}}

@article{Li:2008:ADP:1389089.1389093,
	Abstract = {This article presents a holistic hybrid design methodology for low-power, low-cost, testable digital designs using low-temperature polycrystalline-silicon thin-film transistors (LTPS TFTs). An alternate scaling rule under low thermal budget (due to flexible substrate) is developed to improve the performance of TFTs in the presence of process variation. We demonstrate that LTPS TFTs can be further optimized for ultralow-power subthreshold operation with performances comparable to contemporary single-crystal silicon-on-insulator (c-Si SOI) devices after process optimization. The optimized LTPS TFTs with high current drivability and less variability can comprise a promising low-cost option to augment Si CMOS technology, opening up a plethora of new hybrid 3D applications. We illustrate one such application: IC testing. Testing of complex VLSI systems is a prime concern due to design cost of DFT circuits, area/delay overheads, and poor test confidence. To harness the benefits of TFT technology, a novel low-power, process-tolerant, generic, and reconfigurable test structure designed using LTPS TFTs is proposed to reduce the test cost, as well as to improve diagnosability and verifiability, of complex VLSI systems. Due to proper optimization of TFT devices, the proposed test structure consumes low power but operates with reasonable performance. Furthermore, the test circuits do not consume any silicon area because they can be integrated on-chip using 3D technology. Since the test architecture is reconfigurable, this eliminates the need to redesign built-in-self-test (BIST) components that may vary from one processor generation to another. We have developed test structures using 200nm TFT devices and evaluated them on designs implemented in 130nm bulk CMOS. For circuit simulations, we have developed a SPICE-compatible model for TFT devices. The BIST components designed using the test structures operate at 0.8--4.3 GHz (compared to 8.2 GHz in bulk CMOS) with low power consumption. The enhanced scan cells partially implemented in TFT (3D hybrid design) consume 24% less power and 15--20% less area of Si die compared to conventional bulk-Si design (2D planar design), with minimal delay overhead.},
	Acmid = {1389093},
	Address = {New York, NY, USA},
	Articleno = {13},
	Author = {Li, Jing and Bansal, Aditya and Ghosh, Swarop and Roy, Kaushik},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:37:58 +0100},
	Doi = {10.1145/1389089.1389093},
	Issn = {1550-4832},
	Issue_Date = {August 2008},
	Journal = {J. Emerg. Technol. Comput. Syst.},
	Keywords = {3D integration, BIST, DFT, Low-temperature polycrystalline silicon (LTPS), generic, grain boundary (GB), hybrid system, inherent variation, reconfigurable, thin-film transistor (TFT)},
	Number = {3},
	Numpages = {19},
	Pages = {13:1--13:19},
	Publisher = {ACM},
	Title = {An Alternate Design Paradigm for Low-power, Low-cost, Testable Hybrid Systems Using Scaled LTPS TFTs},
	Url = {http://doi.acm.org/10.1145/1389089.1389093},
	Volume = {4},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1389089.1389093},
	Bdsk-Url-2 = {https://doi.org/10.1145/1389089.1389093}}

@article{Sasanka:2002:JLG:635506.605413,
	Abstract = {This work concerns algorithms to control energy-driven architecture adaptations for multimedia applications, without and with dynamic voltage scaling (DVS). We identify a broad design space for adaptation control algorithms based on two attributes: (1) when to adapt or temporal granularity and (2) what structures to adapt or spatial granularity. For each attribute, adaptation may be global or local. Our previous work developed a temporally and spatially global algorithm. It invokes adaptation at the granularity of a full frame of a multimedia application (temporally global) and considers the entire hardware configuration at a time (spatially global). It exploits inter-frame execution time variability, slowing computation just enough to eliminate idle time before the real-time deadline.This paper explores temporally and spatially local algorithms and their integration with the previous global algorithm. The local algorithms invoke architectural adaptation within an application frame to exploit intra-frame execution variability, and attempt to save energy without affecting execution time. We consider local algorithms previously studied for non-real-time applications as well as propose new algorithms. We find that, for systems without and with DVS, the local algorithms are effective in saving energy for multimedia applications, but the new integrated global and local algorithm is best for the systems and applications studied.},
	Acmid = {605413},
	Address = {New York, NY, USA},
	Author = {Sasanka, Ruchira and Hughes, Christopher J. and Adve, Sarita V.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:26:32 +0100},
	Doi = {10.1145/635506.605413},
	Issn = {0163-5964},
	Issue_Date = {December 2002},
	Journal = {SIGARCH Comput. Archit. News},
	Number = {5},
	Numpages = {12},
	Pages = {144--155},
	Publisher = {ACM},
	Title = {Joint Local and Global Hardware Adaptations for Energy},
	Url = {http://doi.acm.org/10.1145/635506.605413},
	Volume = {30},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/635506.605413},
	Bdsk-Url-2 = {https://doi.org/10.1145/635506.605413}}

@article{Shye:2008:LLR:1394608.1382158,
	Abstract = {The ultimate goal of computer design is to satisfy the end-user. In particular computing domains, such as interactive applications, there exists a variation in user expectations and user satisfaction relative to the performance of existing computer systems. In this work, we leverage this variation to develop more efficient architectures that are customized to end-users. We first investigate the relationship between microarchitectural parameters and user satisfaction. Specifically, we analyze the relationship between hardware performance counter (HPC) readings and individual satisfaction levels reported by users for representative applications. Our results show that the satisfaction of the user is strongly correlated to the performance of the underlying hardware. More importantly, the results show that user satisfaction is highly user-dependent. To take advantage of these observations, we develop a framework called Individualized Dynamic Voltage and Frequency Scaling (iDVFS). We study a group of users to characterize the relationship between the HPCs and individual user satisfaction levels. Based on this analysis, we use artificial neural networks to model the function from HPCs to user satisfaction for individual users. This model is then used online to predict user satisfaction and set the frequency level accordingly. A second set of user studies demonstrates that iDVFS reduces the CPU power consumption by over 25% in representative applications as compared to the Windows XP DVFS algorithm.},
	Acmid = {1382158},
	Address = {New York, NY, USA},
	Author = {Shye, Alex and Ozisikyilmaz, Berkin and Mallik, Arindam and Memik, Gokhan and Dinda, Peter A. and Dick, Robert P. and Choudhary, Alok N.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:44:03 +0100},
	Doi = {10.1145/1394608.1382158},
	Issn = {0163-5964},
	Issue_Date = {June 2008},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {Dynamic Power Management, Hardware Performance Counters, Learning User Satisfaction, User-aware Architectures},
	Number = {3},
	Numpages = {12},
	Pages = {427--438},
	Publisher = {ACM},
	Title = {Learning and Leveraging the Relationship Between Architecture-Level Measurements and Individual User Satisfaction},
	Url = {http://doi.acm.org/10.1145/1394608.1382158},
	Volume = {36},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1394608.1382158},
	Bdsk-Url-2 = {https://doi.org/10.1145/1394608.1382158}}

@article{Sun:2011:MME:2024723.2000109,
	Abstract = {In recent years, the increasing number of processor cores and limited increases in main memory bandwidth have led to the problem of the bandwidth wall, where memory bandwidth is becoming a performance bottleneck. This is especially true for emerging latency-insensitive, bandwidth-sensitive applications. Designing the memory hierarchy for a platform with an emphasis on maximizing bandwidth within a fixed power budget becomes one of the key challenges. To facilitate architects to quickly explore the design space of memory hierarchies, we propose an analytical performance model called Moguls. The Moguls model estimates the performance of an application on a system, using the bandwidth demand of the application for a range of cache capacities and the bandwidth provided by the system with those capacities. We show how to extend this model with appropriate approximations to optimize a cache hierarchy under a power constraint. The results show how many levels of cache should be designed, and what the capacity, bandwidth, and technology of each level should be. In addition, we study memory hierarchy design with hybrid memory technologies, which shows the benefits of using multiple technologies for future computing systems.},
	Acmid = {2000109},
	Address = {New York, NY, USA},
	Author = {Sun, Guangyu and Hughes, Christopher J. and Kim, Changkyu and Zhao, Jishen and Xu, Cong and Xie, Yuan and Chen, Yen-Kuang},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:45:45 +0100},
	Doi = {10.1145/2024723.2000109},
	Issn = {0163-5964},
	Issue_Date = {June 2011},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {bandwidth, memory hierarchy, memory model, power consumption, throughput computing},
	Number = {3},
	Numpages = {12},
	Pages = {377--388},
	Publisher = {ACM},
	Title = {Moguls: A Model to Explore the Memory Hierarchy for Bandwidth Improvements},
	Url = {http://doi.acm.org/10.1145/2024723.2000109},
	Volume = {39},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2024723.2000109},
	Bdsk-Url-2 = {https://doi.org/10.1145/2024723.2000109}}

@article{Thomas:2003:IBP:871656.859655,
	Abstract = {Deep pipelines and fast clock rates are necessitating the development of high accuracy, multi-stage branch predictors for future processors. Such a predictor uses a collection of predictors, each of which provides its predictions at a different stage of the pipeline front-end. A simple 1-cycle latency line predictor provides predictions in the first stage, followed in a couple of stages later by predictions from a more accurate global predictor. Finally, one or two stages later, a highly accurate corrector predictor selectively corrects the global predictor's prediction. As the corrector predictor has the final say, its accuracy must be very high. The focus of this paper is to propose and evaluate techniques to build high-accuracy corrector predictors.Our techniques rely on using a long global history, and identifying correlated branches in this history by using runtime dataflow information. In particular, we identify for each dynamic branch a set of branches called "affectors", which control the computation that affect that branch's outcome. We propose efficient hardware structures to track dataflow and to identify the affector branches for each dynamic branch; the hardware overhead for identifying affectors for all dynamic branches from a 64 branch global history is only 312 bytes. We then propose two prediction schemes that put to use the affector branch information. Experimental studies show that adding an 8KB corrector predictor (that uses affector information) to a 16KB perceptron predictor (total size 24.2KB) reduces the average misprediction rate for 12 benchmarks from 6.3% to 5.7%, an improvement achieved only by a 64KB perceptron predictor.},
	Acmid = {859655},
	Address = {New York, NY, USA},
	Author = {Thomas, Renju and Franklin, Manoj and Wilkerson, Chris and Stark, Jared},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:28:11 +0100},
	Doi = {10.1145/871656.859655},
	Issn = {0163-5964},
	Issue_Date = {May 2003},
	Journal = {SIGARCH Comput. Archit. News},
	Number = {2},
	Numpages = {10},
	Pages = {314--323},
	Publisher = {ACM},
	Title = {Improving Branch Prediction by Dynamic Dataflow-based Identification of Correlated Branches from a Large Global History},
	Url = {http://doi.acm.org/10.1145/871656.859655},
	Volume = {31},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/871656.859655},
	Bdsk-Url-2 = {https://doi.org/10.1145/871656.859655}}

@article{Tu:2013:SID:2508148.2485932,
	Abstract = {Virtualization allows flexible mappings between physical resources and virtual entities, and improves allocation efficiency and agility. Unfortunately, most existing virtualization technologies are limited to resources in a single host. This paper presents the design, implementation and evaluation of a multi-host I/O device virtualization system called Ladon, which enables I/O devices to be shared among virtual machines running on multiple hosts in a secure and efficient way. Specifically, Ladon uses a PCIe network to connect multiple servers with PCIe devices and allows VMs running on these servers to directly interact with these PCIe devices without interfering with one another. Through an evaluation of a fully operational Ladon prototype, we show that there is no throughput and latency penalty of the multi-host I/O virtualization enabled by Ladon compared to those of the existing single-host I/O virtualization technology.},
	Acmid = {2485932},
	Address = {New York, NY, USA},
	Author = {Tu, Cheng-Chun and Lee, Chao-tang and Chiueh, Tzi-cker},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:21:28 +0100},
	Doi = {10.1145/2508148.2485932},
	Issn = {0163-5964},
	Issue_Date = {June 2013},
	Journal = {SIGARCH Comput. Archit. News},
	Number = {3},
	Numpages = {12},
	Pages = {108--119},
	Publisher = {ACM},
	Title = {Secure I/O Device Sharing Among Virtual Machines on Multiple Hosts},
	Url = {http://doi.acm.org/10.1145/2508148.2485932},
	Volume = {41},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2508148.2485932},
	Bdsk-Url-2 = {https://doi.org/10.1145/2508148.2485932}}

@article{Weaver:2004:TRS:1028176.1006723,
	Abstract = {Transient faults due to neutron and alpha particle strikes posea significant obstacle to increasing processor transistor counts infuture technologies. Although fault rates of individual transistorsmay not rise significantly, incorporating more transistors into adevice makes that device more likely to encounter a fault. Hence,maintaining processor error rates at acceptable levels will requireincreasing design effort.This paper proposes two simple approaches to reduce errorrates and evaluates their application to a microprocessor instructionqueue. The first technique reduces the time instructions sit invulnerable storage structures by selectively squashing instructionswhen long delays are encountered. A fault is less likely to cause anerror if the structure it affects does not contain valid instructions.We introduce a new metric, MITF (Mean Instructions To Failure),to capture the trade-off between performance and reliability introducedby this approach.The second technique addresses false detected errors. In theabsence of a fault detection mechanism, such errors would nothave affected the final outcome of a program. For example, a faultaffecting the result of a dynamically dead instruction would notchange the final program output, but could still be flagged by thehardware as an error. To avoid signalling such false errors, wemodify a pipeline's error detection logic to mark affected instructionsand data as possibly incorrect rather than immediately signalingan error. Then, we signal an error only if we determine laterthat the possibly incorrect value could have affected the program'soutput.},
	Acmid = {1006723},
	Address = {New York, NY, USA},
	Author = {Weaver, Christopher and Emer, Joel and Mukherjee, Shubhendu S. and Reinhardt, Steven K.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:15:01 +0100},
	Doi = {10.1145/1028176.1006723},
	Issn = {0163-5964},
	Issue_Date = {March 2004},
	Journal = {SIGARCH Comput. Archit. News},
	Number = {2},
	Pages = {1-11},
	Publisher = {ACM},
	Title = {Techniques to Reduce the Soft Error Rate of a High-Performance Microprocessor},
	Url = {http://doi.acm.org/10.1145/1028176.1006723},
	Volume = {32},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1028176.1006723},
	Bdsk-Url-2 = {https://doi.org/10.1145/1028176.1006723}}

@article{Wilkerson:2010:RCP:1816038.1815973,
	Abstract = {Technology advancements have enabled the integration of large on-die embedded DRAM (eDRAM) caches. eDRAM is significantly denser than traditional SRAMs, but must be periodically refreshed to retain data. Like SRAM, eDRAM is susceptible to device variations, which play a role in determining refresh time for eDRAM cells. Refresh power potentially represents a large fraction of overall system power, particularly during low-power states when the CPU is idle. Future designs need to reduce cache power without incurring the high cost of flushing cache data when entering low-power states.

In this paper, we show the significant impact of variations on refresh time and cache power consumption for large eDRAM caches. We propose Hi-ECC, a technique that incorporates multi-bit error-correcting codes to significantly reduce refresh rate. Multi-bit error-correcting codes usually have a complex decoder design and high storage cost. Hi-ECC avoids the decoder complexity by using strong ECC codes to identify and disable sections of the cache with multi-bit failures, while providing efficient single-bit error correction for the common case. Hi-ECC includes additional optimizations that allow us to amortize the storage cost of the code over large data words, providing the benefit of multi-bit correction at same storage cost as a single-bit error-correcting (SECDED) code (2% overhead). Our proposal achieves a 93% reduction in refresh power vs. a baseline eDRAM cache without error correcting capability, and a 66% reduction in refresh power vs. a system using SECDED codes.},
	Acmid = {1815973},
	Address = {New York, NY, USA},
	Author = {Wilkerson, Chris and Alameldeen, Alaa R. and Chishti, Zeshan and Wu, Wei and Somasekhar, Dinesh and Lu, Shih-lien},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:48:54 +0100},
	Doi = {10.1145/1816038.1815973},
	Issn = {0163-5964},
	Issue_Date = {June 2010},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {dram, ecc, edram, idle power, idle states, multi-bit ecc, refresh power, vccmin},
	Number = {3},
	Numpages = {11},
	Pages = {83--93},
	Publisher = {ACM},
	Title = {Reducing Cache Power with Low-cost, Multi-bit Error-correcting Codes},
	Url = {http://doi.acm.org/10.1145/1816038.1815973},
	Volume = {38},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1816038.1815973},
	Bdsk-Url-2 = {https://doi.org/10.1145/1816038.1815973}}

@article{Wilkerson:2008:TOC:1394608.1382139,
	Abstract = {One of the most effective techniques to reduce a processor's power consumption is to reduce supply voltage. However, reducing voltage in the context of manufacturing-induced parameter variations cancause many types of memory circuits to fail. As a result, voltage scaling is limited by a minimum voltage, often called Vccmin, beyond which circuits may not operate reliably. Large memory structures (e.g., caches) typically set Vccmin for the whole processor. In this paper, we propose two architectural techniques that enable microprocessor caches (L1and L2), to operate at low voltages despite very high memory cell failure rates. The Word-disable scheme combines two consecutive cache lines, to form a single cache line where only non-failing words are used. The Bit-fix scheme uses a quarter of the ways in a cache set to store positions and fix bits for failing bits in other ways of the set. During high voltage operation, both schemes allow use of the entire cache. During low voltage operation, they sacrifice cache capacity by 50% and 25%, respectively, to reduce Vccmin below 500mV. Compared to current designs with a Vccmin of 825mV, our schemes enable a 40% voltage reduction, which reduces power by 85% and energy per instruction (EPI) by 53%},
	Acmid = {1382139},
	Address = {New York, NY, USA},
	Author = {Wilkerson, Chris and Gao, Hongliang and Alameldeen, Alaa R. and Chishti, Zeshan and Khellah, Muhammad and Lu, Shih-Lien},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:51:56 +0100},
	Doi = {10.1145/1394608.1382139},
	Issn = {0163-5964},
	Issue_Date = {June 2008},
	Journal = {SIGARCH Comput. Archit. News},
	Keywords = {SRAM, Vccmin, cache, cache design, low power, low voltage, reliability, stability},
	Number = {3},
	Numpages = {12},
	Pages = {203--214},
	Publisher = {ACM},
	Title = {Trading off Cache Capacity for Reliability to Enable Low Voltage Operation},
	Url = {http://doi.acm.org/10.1145/1394608.1382139},
	Volume = {36},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1394608.1382139},
	Bdsk-Url-2 = {https://doi.org/10.1145/1394608.1382139}}

@article{Wu:2001:CFF:384285.379256,
	Abstract = {The growth of the Internet as a vehicle for secure communication and electronic commerce has brought cryptographic processing performance to the forefront of high throughput system design. This trend will be further underscored with the widespread adoption of secure protocols such as secure IP (IPSEC) and virtual private networks (VPNs).

In this paper, we introduce the CryptoManiac processor, a fast and flexible co-processor for cryptographic workloads. Our design is extremely efficient; we present analysis of a 0.25um physical design that runs the standard Rijndael cipher algorithm 2.25 times faster than a 600MHz Alpha 21264 processor. Moreover, our implementation requires 1/100th the area and power in the same technology. We demonstrate that the performance of our design rivals a state-of-the-art dedicated hardware implementation of the 3DES (triple DES) algorithm, while retaining the flexibility to simultaneously support multiple cipher algorithms. Finally, we define a scalable system architecture that combines CryptoManiac processing elements to exploit inter-session and inter-packet parallelism available in many communication protocols. Using I/O traces and detailed timing simulation, we show that chip multiprocessor configurations can effectively service high throughput applications including secure web and disk I/O processing.},
	Acmid = {379256},
	Address = {New York, NY, USA},
	Author = {Wu, Lisa and Weaver, Chris and Austin, Todd},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:30:10 +0100},
	Doi = {10.1145/384285.379256},
	Issn = {0163-5964},
	Issue_Date = {May 2001},
	Journal = {SIGARCH Comput. Archit. News},
	Number = {2},
	Numpages = {10},
	Pages = {110--119},
	Publisher = {ACM},
	Title = {CryptoManiac: A Fast Flexible Architecture for Secure Communication},
	Url = {http://doi.acm.org/10.1145/384285.379256},
	Volume = {29},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/384285.379256},
	Bdsk-Url-2 = {https://doi.org/10.1145/384285.379256}}

@article{Schwarz:2013:FVS:2583687.2583699,
	Abstract = {Many future Driver-Assistance-Systems (DAS) will use components not permanently mounted to the vehicle. Unlike state-of-the-art DAS with static configurations, the system and software architecture changes at runtime. To handle configuration changes, Service Oriented Architecture (SOA) and automatic orchestration is a promising approach. Whenever systems are set up automatically, they have to be validated. This paper presents an approach based on formal methods. Existing component models are annotated with Quality-of-Service parameters and transformed automatically to Hybrid Automata. These automata are then composed to an overall system model and model checking is used to check safety properties. The complete transformation-orchestration-validation process is executed without user interaction and thus can be performed at runtime.},
	Acmid = {2583699},
	Address = {New York, NY, USA},
	Author = {Schwarz, Christian and Z\"{o}bel, Dieter and Wagner, Marco},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:29:15 +0100},
	Doi = {10.1145/2583687.2583699},
	Issn = {1551-3688},
	Issue_Date = {December 2013},
	Journal = {SIGBED Rev.},
	Number = {4},
	Numpages = {4},
	Pages = {49--52},
	Publisher = {ACM},
	Title = {Formal Verification of Service-oriented Adaptive Driver Assistance Systems},
	Url = {http://doi.acm.org/10.1145/2583687.2583699},
	Volume = {10},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2583687.2583699},
	Bdsk-Url-2 = {https://doi.org/10.1145/2583687.2583699}}

@article{Banerjee:2002:SAL:964725.633045,
	Abstract = {We describe a new scalable application-layer multicast protocol, specifically designed for low-bandwidth, data streaming applications with large receiver sets. Our scheme is based upon a hierarchical clustering of the application-layer multicast peers and can support a number of different data delivery trees with desirable properties.We present extensive simulations of both our protocol and the Narada application-layer multicast protocol over Internet-like topologies. Our results show that for groups of size 32 or more, our protocol has lower link stress (by about 25%), improved or similar end-to-end latencies and similar failure recovery properties. More importantly, it is able to achieve these results by using orders of magnitude lower control traffic.Finally, we present results from our wide-area testbed in which we experimented with 32-100 member groups distributed over 8 different sites. In our experiments, average group members established and maintained low-latency paths and incurred a maximum packet loss rate of less than 1% as members randomly joined and left the multicast group. The average control overhead during our experiments was less than 1 Kbps for groups of size 100.},
	Acmid = {633045},
	Address = {New York, NY, USA},
	Author = {Banerjee, Suman and Banerjee, Suman and Bhattacharjee, Bobby and Kommareddy, Christopher},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:54:01 +0100},
	Doi = {10.1145/964725.633045},
	Issn = {0146-4833},
	Issue_Date = {October 2002},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {application layer multicast, hierarchy, overlay networks, peer-to-peer systems, scalability},
	Number = {4},
	Numpages = {13},
	Pages = {205--217},
	Publisher = {ACM},
	Title = {Scalable Application Layer Multicast},
	Url = {http://doi.acm.org/10.1145/964725.633045},
	Volume = {32},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/964725.633045},
	Bdsk-Url-2 = {https://doi.org/10.1145/964725.633045}}

@article{Bishop:2005:RSC:1090191.1080123,
	Abstract = {Network protocols are hard to implement correctly. Despite the existence of RFCs and other standards, implementations often have subtle differences and bugs. One reason for this is that the specifications are typically informal, and hence inevitably contain ambiguities. Conformance testing against such specifications is challenging.In this paper we present a practical technique for rigorous protocol specification that supports specification-based testing. We have applied it to TCP, UDP, and the Sockets API, developing a detailed 'post-hoc' specification that accurately reflects the behaviour of several existing implementations (FreeBSD 4.6, Linux 2.4.20-8, and Windows XP SP1). The development process uncovered a number of differences between and infelicities in these implementations.Our experience shows for the first time that rigorous specification is feasible for protocols as complex as TCP@. We argue that the technique is also applicable 'pre-hoc', in the design phase of new protocols. We discuss how such a design-for-test approach should influence protocol development, leading to protocol specifications that are both unambiguous and clear, and to high-quality implementations that can be tested directly against those specifications.},
	Acmid = {1080123},
	Address = {New York, NY, USA},
	Author = {Bishop, Steve and Fairbairn, Matthew and Norrish, Michael and Sewell, Peter and Smith, Michael and Wansbrough, Keith},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:54:15 +0100},
	Doi = {10.1145/1090191.1080123},
	Issn = {0146-4833},
	Issue_Date = {October 2005},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {API, HOL, TCP/IP, conformance testing, higher-order logic, network protocols, operational semantics, sockets, specification},
	Number = {4},
	Numpages = {12},
	Pages = {265--276},
	Publisher = {ACM},
	Title = {Rigorous Specification and Conformance Testing Techniques for Network Protocols, As Applied to TCP, UDP, and Sockets},
	Url = {http://doi.acm.org/10.1145/1090191.1080123},
	Volume = {35},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1090191.1080123},
	Bdsk-Url-2 = {https://doi.org/10.1145/1090191.1080123}}

@article{Greenberg:2008:CCR:1496091.1496103,
	Abstract = {The data centers used to create cloud services represent a significant investment in capital outlay and ongoing costs. Accordingly, we first examine the costs of cloud service data centers today. The cost breakdown reveals the importance of optimizing work completed per dollar invested. Unfortunately, the resources inside the data centers often operate at low utilization due to resource stranding and fragmentation. To attack this first problem, we propose (1) increasing network agility, and (2) providing appropriate incentives to shape resource consumption. Second, we note that cloud service providers are building out geo-distributed networks of data centers. Geo-diversity lowers latency to users and increases reliability in the presence of an outage taking out an entire site. However, without appropriate design and management, these geo-diverse data center networks can raise the cost of providing service. Moreover, leveraging geo-diversity requires services be designed to benefit from it. To attack this problem, we propose (1) joint optimization of network and data center resources, and (2) new systems and mechanisms for geo-distributing state.},
	Acmid = {1496103},
	Address = {New York, NY, USA},
	Author = {Greenberg, Albert and Hamilton, James and Maltz, David A. and Patel, Parveen},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:53:28 +0100},
	Doi = {10.1145/1496091.1496103},
	Issn = {0146-4833},
	Issue_Date = {January 2009},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {cloud-service data centers, costs, network challenges},
	Number = {1},
	Numpages = {6},
	Pages = {68--73},
	Publisher = {ACM},
	Title = {The Cost of a Cloud: Research Problems in Data Center Networks},
	Url = {http://doi.acm.org/10.1145/1496091.1496103},
	Volume = {39},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1496091.1496103},
	Bdsk-Url-2 = {https://doi.org/10.1145/1496091.1496103}}

@article{Greenberg:2009:VSF:1594977.1592576,
	Abstract = {To be agile and cost effective, data centers should allow dynamic resource allocation across large server pools. In particular, the data center network should enable any server to be assigned to any service. To meet these goals, we present VL2, a practical network architecture that scales to support huge data centers with uniform high capacity between servers, performance isolation between services, and Ethernet layer-2 semantics. VL2 uses (1) flat addressing to allow service instances to be placed anywhere in the network, (2) Valiant Load Balancing to spread traffic uniformly across network paths, and (3) end-system based address resolution to scale to large server pools, without introducing complexity to the network control plane. VL2's design is driven by detailed measurements of traffic and fault data from a large operational cloud service provider. VL2's implementation leverages proven network technologies, already available at low cost in high-speed hardware implementations, to build a scalable and reliable network architecture. As a result, VL2 networks can be deployed today, and we have built a working prototype. We evaluate the merits of the VL2 design using measurement, analysis, and experiments. Our VL2 prototype shuffles 2.7 TB of data among 75 servers in 395 seconds - sustaining a rate that is 94% of the maximum possible.},
	Acmid = {1592576},
	Address = {New York, NY, USA},
	Author = {Greenberg, Albert and Hamilton, James R. and Jain, Navendu and Kandula, Srikanth and Kim, Changhoon and Lahiri, Parantap and Maltz, David A. and Patel, Parveen and Sengupta, Sudipta},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:51:11 +0100},
	Doi = {10.1145/1594977.1592576},
	Issn = {0146-4833},
	Issue_Date = {October 2009},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {commoditization, data center network},
	Number = {4},
	Numpages = {12},
	Pages = {51--62},
	Publisher = {ACM},
	Title = {VL2: A Scalable and Flexible Data Center Network},
	Url = {http://doi.acm.org/10.1145/1594977.1592576},
	Volume = {39},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1594977.1592576},
	Bdsk-Url-2 = {https://doi.org/10.1145/1594977.1592576}}

@article{Lockwood:2003:ATI:956993.957006,
	Abstract = {Tools have been developed to automatically integrate and test networking systems in reconfigurable hardware. These tools dynamically generate circuits for Field Programmable Gate Arrays (FPGAs). A library of hardware-accelerated modules has been developed that processes Internet Protocol (IP) packets, performs header rule matching, scans pocket payloads, and implements per-flow queueing. Other functions can be added to the library as extensible modules.An integration tool was developed to enable a network administrator to specify how a customized system should examine, drop, buffer, and/or modify packets. This tool joins together modules from the library to create a composite circuit that performs multiple functions. The tool allows additional modules to be quickly added to the library and integrated into systems. The integration tool has been used to create circuits that perform Internet firewall, network intrusion detection, network intrusion prevention, and Denial of Service (DoS) attack protection functions.A test tool was developed to automatically verify that circuits created by the integration tool run properly in reconfigurable hardware. Circuits created by the integration tool are deployed into a Field-programmable Port Extender (FPX) platform. As new modules were added to the library, the test tool reconfigured the logic on the FPX, injected traffic, and monitored the resulting packets.By using hardware, not software, networking system can process millions of packets per second. Together, the integration and test tools simplify the otherwise difficult task of developing reconfigurable hardware for networking systems and testing them at Gigabit per second rates.},
	Acmid = {957006},
	Address = {New York, NY, USA},
	Author = {Lockwood, John W. and Neely, Chris and Zuver, Chris and Lim, Dave},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:38:46 +0100},
	Doi = {10.1145/956993.957006},
	Issn = {0146-4833},
	Issue_Date = {July 2003},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {Field Programmable Gate Array (FPGA), Internet, firewall, network intrusion detection and prevention, networks, reconfigurable hardware, tools},
	Number = {3},
	Numpages = {8},
	Pages = {103--110},
	Publisher = {ACM},
	Title = {Automated Tools to Implement and Test Internet Systems in Reconfigurable Hardware},
	Url = {http://doi.acm.org/10.1145/956993.957006},
	Volume = {33},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/956993.957006},
	Bdsk-Url-2 = {https://doi.org/10.1145/956993.957006}}

@article{Nychis:2012:ONN:2377677.2377757,
	Abstract = {In this paper, we present network-on-chip (NoC) design and contrast it to traditional network design, highlighting similarities and differences between the two. As an initial case study, we examine network congestion in bufferless NoCs. We show that congestion manifests itself differently in a NoC than in traditional networks. Network congestion reduces system throughput in congested workloads for smaller NoCs (16 and 64 nodes), and limits the scalability of larger bufferless NoCs (256 to 4096 nodes) even when traffic has locality (e.g., when an application's required data is mapped nearby to its core in the network). We propose a new source throttling-based congestion control mechanism with application-level awareness that reduces network congestion to improve system performance. Our mechanism improves system performance by up to 28% (15% on average in congested workloads) in smaller NoCs, achieves linear throughput scaling in NoCs up to 4096 cores (attaining similar performance scalability to a NoC with large buffers), and reduces power consumption by up to 20%. Thus, we show an effective application of a network-level concept, congestion control, to a class of networks -- bufferless on-chip networks -- that has not been studied before by the networking community.},
	Acmid = {2377757},
	Address = {New York, NY, USA},
	Author = {Nychis, George P. and Fallin, Chris and Moscibroda, Thomas and Mutlu, Onur and Seshan, Srinivasan},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:46:39 +0100},
	Doi = {10.1145/2377677.2377757},
	Issn = {0146-4833},
	Issue_Date = {October 2012},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {congestion control, multi-core, on-chip networks},
	Number = {4},
	Numpages = {12},
	Pages = {407--418},
	Publisher = {ACM},
	Title = {On-chip Networks from a Networking Perspective: Congestion and Scalability in Many-core Interconnects},
	Url = {http://doi.acm.org/10.1145/2377677.2377757},
	Volume = {42},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2377677.2377757},
	Bdsk-Url-2 = {https://doi.org/10.1145/2377677.2377757}}

@article{Yang:2011:PSP:1925861.1925866,
	Abstract = {Proliferation and innovation of wireless technologies require significant amounts of radio spectrum. Recent policy reforms by the FCC are paving the way by freeing up spectrum for a new generation of frequency-agile wireless devices based on software defined radios (SDRs). But despite recent advances in SDR hardware, research on SDR MAC protocols or applications requires an experimental platform for managing physical access. We introduce Papyrus, a software platform for wireless researchers to develop and experiment dynamic spectrum systems using currently available SDR hardware. Papyrus provides two fundamental building blocks at the physical layer: flexible non-contiguous frequency access and simple and robust frequency detection. Papyrus allows researchers to deploy and experiment new MAC protocols and applications on USRP GNU Radio, and can also be ported to other SDR platforms. We demonstrate the use of Papyrus using Jello, a distributed MAC overlay for high-bandwidth media streaming applications and Ganache, a SDR layer for adaptable guardband configuration. Full implementations of Papyrus and Jello are publicly available.},
	Acmid = {1925866},
	Address = {New York, NY, USA},
	Author = {Yang, Lei and Zhang, Zengbin and Hou, Wei and Zhao, Ben Y. and Zheng, Haitao},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:47:10 +0100},
	Doi = {10.1145/1925861.1925866},
	Issn = {0146-4833},
	Issue_Date = {January 2011},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {cognitive radio, dynamic spectrum access, testbed},
	Number = {1},
	Numpages = {7},
	Pages = {31--37},
	Publisher = {ACM},
	Title = {Papyrus: A Software Platform for Distributed Dynamic Spectrum Sharing Using SDRs},
	Url = {http://doi.acm.org/10.1145/1925861.1925866},
	Volume = {41},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1925861.1925866},
	Bdsk-Url-2 = {https://doi.org/10.1145/1925861.1925866}}

@article{Christensen:2009:SAS:1595496.1562901,
	Abstract = {Advanced programming and software engineering techniques are challenging to learn due to their inherent complexity. However, to the average student they are even more challenging because they have never experienced the context in which the techniques are appropriate. For instance, why learn design patterns to increase maintainability when student exercises are never maintained? In this paper, we outline the contextual problems that software engineering teaching has to deal with and present a story telling approach for course design as a remedy. We outline the stories that over the last five years have structured lecturing and mandatory exercises for our advanced programming/software engineering course, and present benefits, liabilities, and experiences with the approach comparing it to the normal, topic structured, course design.},
	Acmid = {1562901},
	Address = {New York, NY, USA},
	Author = {Christensen, Henrik B{\ae}rbak},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:36:03 +0100},
	Doi = {10.1145/1595496.1562901},
	Issn = {0097-8418},
	Issue_Date = {September 2009},
	Journal = {SIGCSE Bull.},
	Keywords = {agile methods, curriculum design, design patterns, frameworks, software architecture, test-driven development.},
	Number = {3},
	Numpages = {5},
	Pages = {60--64},
	Publisher = {ACM},
	Title = {A Story-telling Approach for a Software Engineering Course Design},
	Url = {http://doi.acm.org/10.1145/1595496.1562901},
	Volume = {41},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1595496.1562901},
	Bdsk-Url-2 = {https://doi.org/10.1145/1595496.1562901}}

@article{Hundhausen:2008:ESI:1352322.1352271,
	Abstract = {While the demand for college graduates with computing skills continues to rise, such skills no longer equate to mere programming skills. Modern day computing jobs demand design, communication, and collaborative work skills as well. Since traditional instructional methods in computing education tend to focus on programming skills, we believe that a fundamental rethinking of computing education is in order. We are exploring a new "studio-based" pedagogy that actively engages undergraduate students in collaborative, design-oriented learning. Adapted from architectural education, the studio-based instructional model emphasizes learning activities in which students (a) construct personalized solutions to assigned computing problems, and (b) present solutions to their instructors and peers for feedback and discussion within the context of "design crits." We describe and motivate the studio-based approach, review previous efforts to apply it to computer science education, and propose an agenda for multi-institutional research into the design and impact of studio-based instructional models. We invite educators to participate in a community of research and practice to advance studio-based learning in computing education.

},
	Acmid = {1352271},
	Address = {New York, NY, USA},
	Author = {Hundhausen, Christopher D. and Narayanan, N Hari and Crosby, Martha E.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:41:21 +0100},
	Doi = {10.1145/1352322.1352271},
	Issn = {0097-8418},
	Issue_Date = {March 2008},
	Journal = {SIGCSE Bull.},
	Keywords = {cs ed. research, cs1, cs2, cs3, design crits, pre-cs1, student-constructed artifacts, studio-based learning and instruction},
	Number = {1},
	Numpages = {5},
	Pages = {392--396},
	Publisher = {ACM},
	Title = {Exploring Studio-based Instructional Models for Computing Education},
	Url = {http://doi.acm.org/10.1145/1352322.1352271},
	Volume = {40},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1352322.1352271},
	Bdsk-Url-2 = {https://doi.org/10.1145/1352322.1352271}}

@article{Lass:2003:TTL:961290.961558,
	Abstract = {Courseware/Course Management Systems (CMS) such as WebCT or Blackboard are an increasingly popular way to provide a web presence for a course. However, their current web-browser reliance makes it difficult for them to provide functionality that could be useful to computer science instructors. This paper describes our augmentation of a CMS in a large introductory computer science class. It further describes our enhancement of the CMS by clientside software (i.e. residing on the graders computer), written for use by the instructors and graders. Finally, it indicates how conventional CMS architecture can be extended to provide additional functionality that would be desirable for computer science instruction.},
	Acmid = {961558},
	Address = {New York, NY, USA},
	Author = {Lass, Robert N. and Cera, Christopher D. and Bomberger, Nathaniel T. and Char, Bruce and Popyack, Jeffrey L. and Herrmann, Nira and Zoski, Paul},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:52:19 +0100},
	Doi = {10.1145/961290.961558},
	Issn = {0097-8418},
	Issue_Date = {September 2003},
	Journal = {SIGCSE Bull.},
	Keywords = {Course Management Systems, Courseware, WebCT, electronic pen-based markup, introductory programming, plagiarism detection},
	Number = {3},
	Numpages = {5},
	Pages = {168--172},
	Publisher = {ACM},
	Title = {Tools and Techniques for Large Scale Grading Using Web-based Commercial Off-the-shelf Software},
	Url = {http://doi.acm.org/10.1145/961290.961558},
	Volume = {35},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/961290.961558},
	Bdsk-Url-2 = {https://doi.org/10.1145/961290.961558}}

@article{Miller:2007:TAI:1269900.1268797,
	Abstract = {Learning embedded programming is a highly demanding exercise. The beginner is bombarded with complexity from the start: embedded production based around a myriad of C++ constructs with low-level elements integrated onto ever more complicated processor architectures. The picture is further compounded by tool support having unfamiliar roles and appearances from previous student experiences. This demanding situation often has the student bewildered; seeking for "a crutch" or the simplest way forward regardless of the overall consequences. To control this potentially chaotic picture, the instructor needs to introduce devices to combat this complexity. We argue that test driven development (TDD) should become the instructor's principal weapon in this fight. Reasons for this belief combined with our, and the students', experiences with this novel approach are discussed.},
	Acmid = {1268797},
	Address = {New York, NY, USA},
	Author = {Miller, James and Smith, Michael},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:36:56 +0100},
	Doi = {10.1145/1269900.1268797},
	Issn = {0097-8418},
	Issue_Date = {September 2007},
	Journal = {SIGCSE Bull.},
	Keywords = {TDD, education, embedded programming},
	Number = {3},
	Numpages = {5},
	Pages = {33--37},
	Publisher = {ACM},
	Title = {A TDD Approach to Introducing Students to Embedded Programming},
	Url = {http://doi.acm.org/10.1145/1269900.1268797},
	Volume = {39},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1269900.1268797},
	Bdsk-Url-2 = {https://doi.org/10.1145/1269900.1268797}}

@article{Pita:2009:SAL:1980522.1980527,
	Abstract = {Game theory has played an important role in security decisions. Recent work using Stackelberg games [Fudenberg and Tirole 1991] to model security domains has been particularly influential [Basilico et al. 2009; Kiekintveld et al. 2009; Paruchuri et al. 2008; Pita et al. 2008; Pita et al. 2009]. In a Stackelberg game, a leader (in this case the defender) acts first and commits to a randomized security policy. The follower (attacker) optimizes its reward considering the strategy chosen by the leader. These games are well-suited to representing the problem security forces face in allocating limited resources, such as officers, canine units, and checkpoints. In particular, the fact that the attacker is able to observe the policy reflects the way real terrorist organizations plan attacks using extensive surveillance and long planning cycles.},
	Acmid = {1980527},
	Address = {New York, NY, USA},
	Articleno = {5},
	Author = {Pita, James and Bellamane, Harish and Jain, Manish and Kiekintveld, Chris and Tsai, Jason and Ord\'{o}\~{n}ez, Fernando and Tambe, Milind},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:20:46 +0100},
	Doi = {10.1145/1980522.1980527},
	Issn = {1551-9031},
	Issue_Date = {December 2009},
	Journal = {SIGecom Exch.},
	Number = {2},
	Numpages = {4},
	Pages = {5:1--5:4},
	Publisher = {ACM},
	Title = {Security Applications: Lessons of Real-world Deployment},
	Url = {http://doi.acm.org/10.1145/1980522.1980527},
	Volume = {8},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1980522.1980527},
	Bdsk-Url-2 = {https://doi.org/10.1145/1980522.1980527}}

@article{Michel:2009:WLD:1670598.1670606,
	Abstract = {Due to the dramatically increasing amount of available data, effective and scalable solutions for data organization and search are essential. Distributed solutions naturally provide promising alternatives to standard centralized approaches. With the computational power of thousands or millions of computers in clusters or peer-to-peer systems, the challenges that arise are manifold, ranging from efficient resource discovery to issues in load balancing and distributed query processing.

The 2008 edition of the Workshop on Large-Scale Distributed Systems for Information Retrieval (LSDS-IR'08) provided a forum for researchers to discuss these problems and to define new directions for the work on Distributed Information Retrieval. The Workshop program featured research contributions in the areas of similarity search, resource selection, network organization schemes, issues of data quality, result ranking techniques and query routing algorithms.},
	Acmid = {1670606},
	Address = {New York, NY, USA},
	Author = {Michel, Sebastian and Skobeltsyn, Gleb and Yee, Wai Gen},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:09:28 +0100},
	Doi = {10.1145/1670598.1670606},
	Issn = {0163-5840},
	Issue_Date = {June 2009},
	Journal = {SIGIR Forum},
	Number = {1},
	Numpages = {7},
	Pages = {42--48},
	Publisher = {ACM},
	Title = {Workshop on Large-scale Distributed Systems for Information Retrieval},
	Url = {http://doi.acm.org/10.1145/1670598.1670606},
	Volume = {43},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1670598.1670606},
	Bdsk-Url-2 = {https://doi.org/10.1145/1670598.1670606}}

@article{Purves:2006:WGI:1189702.1189708,
	Abstract = {August 2006 saw the third in a series of workshops held to discuss the state of the art in Geographic Information Retrieval held in conjunction with SIGIR'06. The original call for papers for the workshop suggested the workshop should discuss further progress within the field and potential future research strands. Topics considered relevant for the workshop included the following:* architectures for geographic search engines;* spatial indexing of documents and other media resources;* extraction of geographical context from documents and geo-datasets;* geographical annotation techniques for geo-referenced media;* design, construction, maintenance and access methods for geographical ontologies, gazetteers and geographical thesauri;* geographical query interfaces for the web and geo-spatial libraries;* visualising the results of geographic searches; and* relevance ranking for geographical search.},
	Acmid = {1189708},
	Address = {New York, NY, USA},
	Author = {Purves, Ross and Jones, Chris},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:10:51 +0100},
	Doi = {10.1145/1189702.1189708},
	Issn = {0163-5840},
	Issue_Date = {December 2006},
	Journal = {SIGIR Forum},
	Number = {2},
	Numpages = {2},
	Pages = {40--41},
	Publisher = {ACM},
	Title = {Workshop on Geographic Information Retrieval Held at SIGIR'06},
	Url = {http://doi.acm.org/10.1145/1189702.1189708},
	Volume = {40},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1189702.1189708},
	Bdsk-Url-2 = {https://doi.org/10.1145/1189702.1189708}}

@article{Blackburn:2004:MRP:1012888.1005693,
	Abstract = {This paper explores and quantifies garbage collection behavior for three whole heap collectors and generational counterparts: copying semi-space, mark-sweep, and reference counting, the canonical algorithms from which essentially all other collection algorithms are derived. Efficient implementations in MMTk, a Java memory management toolkit, in IBM's Jikes RVM share all common mechanisms to provide a clean experimental platform. Instrumentation separates collector and program behavior, and performance counters measure timing and memory behavior on three architectures.Our experimental design reveals key algorithmic features and how they match program characteristics to explain the direct and indirect costs of garbage collection as a function of heap size on the SPEC JVM benchmarks. For example, we find that the contiguous allocation of copying collectors attains significant locality benefits over free-list allocators. The reduced collection costs of the generational algorithms together with the locality benefit of contiguous allocation motivates a copying nursery for newly allocated objects. These benefits dominate the overheads of generational collectors compared with non-generational and no collection, disputing the myth that "no garbage collection is good garbage collection." Performance is less sensitive to the mature space collection algorithm in our benchmarks. However the locality and pointer mutation characteristics for a given program occasionally prefer copying or mark-sweep. This study is unique in its breadth of garbage collection algorithms and its depth of analysis.},
	Acmid = {1005693},
	Address = {New York, NY, USA},
	Author = {Blackburn, Stephen M. and Cheng, Perry and McKinley, Kathryn S.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:46:08 +0100},
	Doi = {10.1145/1012888.1005693},
	Issn = {0163-5999},
	Issue_Date = {June 2004},
	Journal = {SIGMETRICS Perform. Eval. Rev.},
	Keywords = {generational, java, mark-sweep, reference counting, semi-space},
	Number = {1},
	Numpages = {12},
	Pages = {25--36},
	Publisher = {ACM},
	Title = {Myths and Realities: The Performance Impact of Garbage Collection},
	Url = {http://doi.acm.org/10.1145/1012888.1005693},
	Volume = {32},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1012888.1005693},
	Bdsk-Url-2 = {https://doi.org/10.1145/1012888.1005693}}

@article{Butt:2005:PIK:1071690.1064231,
	Abstract = {A fundamental challenge in improving the file system performance is to design effective block replacement algorithms to minimize buffer cache misses. Despite the well-known interactions between prefetching and caching, almost all buffer cache replacement algorithms have been proposed and studied comparatively without taking into account file system prefetching which exists in all modern operating systems. This paper shows that such kernel prefetching can have a significant impact on the relative performance in terms of the number of actual disk I/Os of many well-known replacement algorithms; it can not only narrow the performance gap but also change the relative performance benefits of different algorithms. These results demonstrate the importance for buffer caching research to take file system prefetching into consideration.},
	Acmid = {1064231},
	Address = {New York, NY, USA},
	Author = {Butt, Ali R. and Gniady, Chris and Hu, Y. Charlie},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:52:39 +0100},
	Doi = {10.1145/1071690.1064231},
	Issn = {0163-5999},
	Issue_Date = {June 2005},
	Journal = {SIGMETRICS Perform. Eval. Rev.},
	Keywords = {buffer caching, prefetching, replacement algorithms},
	Number = {1},
	Numpages = {12},
	Pages = {157--168},
	Publisher = {ACM},
	Title = {The Performance Impact of Kernel Prefetching on Buffer Cache Replacement Algorithms},
	Url = {http://doi.acm.org/10.1145/1071690.1064231},
	Volume = {33},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1071690.1064231},
	Bdsk-Url-2 = {https://doi.org/10.1145/1071690.1064231}}

@article{Tang:2005:LTO:1071690.1064216,
	Abstract = {The routing tables of Distributed Hash Tables (DHTs) can vary from size O(1) to O(n). Currently, what is lacking is an analytic framework to suggest the optimal routing table size for a given workload. This paper (1) compares DHTs with O(1) to O(n) routing tables and identifies some good design points; and (2) proposes protocols to realize the potential of those good design points.We use total traffic as the uniform metric to compare heterogeneous DHTs and emphasize the balance between maintenance cost and lookup cost. Assuming a node on average processes 1,000 or more lookups during its entire lifetime, our analysis shows that large routing tables actually lead to both low traffic and low lookup hops. These good design points translate into one-hop routing for systems of medium size and two-hop routing for large systems.Existing one-hop or two-hop protocols are based on a hierarchy. We instead demonstrate that it is possible to achieve completely decentralized one-hop or two-hop routing, i.e., without giving up being peer-to-peer. We propose 1h-Calot for one-hop routing and 2h-Calot for two-hop routing. Assuming a moderate lookup rate, compared with DHTs that use O(log n) routing tables, 1h-Calot and 2h-Calot save traffic by up to 70% while resolving lookups in one or two hops as opposed to O(log n) hops.},
	Acmid = {1064216},
	Address = {New York, NY, USA},
	Author = {Tang, Chunqiang and Buco, Melissa J. and Chang, Rong N. and Dwarkadas, Sandhya and Luan, Laura Z. and So, Edward and Ward, Christopher},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:45:00 +0100},
	Doi = {10.1145/1071690.1064216},
	Issn = {0163-5999},
	Issue_Date = {June 2005},
	Journal = {SIGMETRICS Perform. Eval. Rev.},
	Keywords = {distributed hash table, overlay network, peer-to-peer system},
	Number = {1},
	Numpages = {12},
	Pages = {14--25},
	Publisher = {ACM},
	Title = {Low Traffic Overlay Networks with Large Routing Tables},
	Url = {http://doi.acm.org/10.1145/1071690.1064216},
	Volume = {33},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1071690.1064216},
	Bdsk-Url-2 = {https://doi.org/10.1145/1071690.1064216}}

@article{Arye:2012:INR:2412096.2412099,
	Abstract = {The primary focus of the NEBULA Future Internet Architecture is to provide resilient networking for the emerging cloud computing model. One of the attractions of cloud computing is its support for online services and data storage by thin clients such as mobile devices. This paper describes two components of NEBULA's edge network technology, Serval and CRYSTAL. Serval provides a new layer 3.5 service abstraction that naturally supports mobility, multi-homing, and multi-path transport, while CRYSTAL is a new virtualization scheme for software radios that makes it easier to expose greater network diversity at the network edge.},
	Acmid = {2412099},
	Address = {New York, NY, USA},
	Author = {Arye, Matvey and Kiefer, Robert and Super, Kyle and Nordstr\"{o}m, Erik and Freedman, Michael J. and Keller, Eric and Rondeau, Tom and Smith, Jonathan M.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:27:46 +0100},
	Doi = {10.1145/2412096.2412099},
	Issn = {1559-1662},
	Issue_Date = {July 2012},
	Journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
	Number = {3},
	Numpages = {7},
	Pages = {14--20},
	Publisher = {ACM},
	Title = {Increasing Network Resilience Through Edge Diversity in NEBULA},
	Url = {http://doi.acm.org/10.1145/2412096.2412099},
	Volume = {16},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2412096.2412099},
	Bdsk-Url-2 = {https://doi.org/10.1145/2412096.2412099}}

@article{Bortnikov:2008:QWM:1374512.1374527,
	Abstract = {We present QMesh, a software package that allows utilizing multiple geographically scattered Windows desktops as a wireless mesh network infrastructure with seamless user mobility support. QMesh supports its users through standard protocols, and does not require any client software installation. We optimize the solution's quality of service (QoS) by providing a centralized management infrastructure, which allows an assignment of users to Internet gateways that balances between distance and load considerations. QMesh is implemented as a Windows XP driver, on top of the Mesh Connectivity Layer (MCL) toolkit from Microsoft Research that provides basic routing capabilities. To the best of our knowledge, this is the first mobile mesh solution implemented within the Win32 kernel space.},
	Acmid = {1374527},
	Address = {New York, NY, USA},
	Author = {Bortnikov, Edward and Cidon, Israel and Keidar, Idit and Kol, Tal and Vaisman, Arkady},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:34:32 +0100},
	Doi = {10.1145/1374512.1374527},
	Issn = {1559-1662},
	Issue_Date = {January 2008},
	Journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
	Number = {1},
	Numpages = {3},
	Pages = {46--48},
	Publisher = {ACM},
	Title = {A QoS WMN with Mobility Support},
	Url = {http://doi.acm.org/10.1145/1374512.1374527},
	Volume = {12},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1374512.1374527},
	Bdsk-Url-2 = {https://doi.org/10.1145/1374512.1374527}}

@article{Mitchell:2006:MNL:1148094.1148098,
	Abstract = {As 3rd Generation (3G) networks emerge they provide not only higher data transmission rates, but also the ability to transmit both voice and low latency data simultaneously. This capability can be leveraged to provide a multimodal user interface. We describe the end-to-end architecture of our implementation of a multimodal application (voice and graphical user interface) that uses Natural Language Understanding in the speech interface combined with a WAP browser to perform mobile office functions on a cellular phone. A novel aspect of the multimodal platform is that no software is required to be installed on the mobile device. The feasibility of our approach is demonstrated by a successful trial with 50 users over a 3G mobile network. We outline our framework, present the results and observations made during the trial.},
	Acmid = {1148098},
	Address = {New York, NY, USA},
	Author = {Mitchell, Stella and Pavlovski, Christopher J. and Smith, Braam and Stavropoulos, Harry and Wood, David},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:24:01 +0100},
	Doi = {10.1145/1148094.1148098},
	Issn = {1559-1662},
	Issue_Date = {July 2006},
	Journal = {SIGMOBILE Mob. Comput. Commun. Rev.},
	Number = {3},
	Numpages = {12},
	Pages = {34--45},
	Publisher = {ACM},
	Title = {Multimodal Natural Language Platform Supporting Cellular Phones},
	Url = {http://doi.acm.org/10.1145/1148094.1148098},
	Volume = {10},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1148094.1148098},
	Bdsk-Url-2 = {https://doi.org/10.1145/1148094.1148098}}

@article{Bussler:2002:RBE:507338.507351,
	Abstract = {Semantic B2B Integration architectures must enable enterprises to communicate standards-based B2B events like purchase orders with any potential trading partner. This requires not only back end application integration capabilities to integrate with e.g. enterprise resource planning (ERP) systems as the company-internal source and destination of B2B events, but also a capability to implement every necessary B2B protocol like Electronic Data Interchange (EDI), RosettaNet as well as more generic capabilities like web services (WS). This paper shows the placement and functionality of B2B engines in semantic B2B integration architectures that implement a generic framework for modeling and executing any B2B protocol. A detailed discussion shows how a B2B engine can provide the necessary abstractions to implement any standard-based B2B protocol or any trading partner specific specialization.},
	Acmid = {507351},
	Address = {New York, NY, USA},
	Author = {Bussler, Christoph},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:13:36 +0100},
	Doi = {10.1145/507338.507351},
	Issn = {0163-5808},
	Issue_Date = {March 2002},
	Journal = {SIGMOD Rec.},
	Number = {1},
	Numpages = {6},
	Pages = {67--72},
	Publisher = {ACM},
	Title = {The Role of B2B Engines in B2B Integration Architectures},
	Url = {http://doi.acm.org/10.1145/507338.507351},
	Volume = {31},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/507338.507351},
	Bdsk-Url-2 = {https://doi.org/10.1145/507338.507351}}

@article{Bussler:2002:CAS:637411.637415,
	Abstract = {Semantic Web Enabled Web Services (SWWS) will transform the web from a static collection of information into a distributed device of computation on the basis of Semantic technology making content within the World Wide Web machine-processable and machine-interpretable. Semantic Web Enabled Web Services will allow the automatic discovery, selection and execution of inter-organization business logic making areas like dynamic supply chain composition a reality. In this paper we introduce the vision of Semantic Web Enabled Web Services, describe requirements for building semantics-driven web services and sketch a first draft of conceptual architecture for implementing semantic web enabled web services.},
	Acmid = {637415},
	Address = {New York, NY, USA},
	Author = {Bussler, Christoph and Fensel, Dieter and Maedche, Alexander},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:34:58 +0100},
	Doi = {10.1145/637411.637415},
	Issn = {0163-5808},
	Issue_Date = {December 2002},
	Journal = {SIGMOD Rec.},
	Number = {4},
	Numpages = {6},
	Pages = {24--29},
	Publisher = {ACM},
	Title = {A Conceptual Architecture for Semantic Web Enabled Web Services},
	Url = {http://doi.acm.org/10.1145/637411.637415},
	Volume = {31},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/637411.637415},
	Bdsk-Url-2 = {https://doi.org/10.1145/637411.637415}}

@article{Bussler:2001:SBI:376284.375810,
	Abstract = {The tutorial ``Semantic B2B Integration'' will give an introduction to the field of business-to-business (B2B) integration from a technical viewpoint with the focus on semantic integration aspects. The set of B2B integration concepts is introduced as well as their implementation in form of a technical semantic B2B integration architecture. A mix of examples is taken illustrating the problems that need to be solved in semantic B2B integration projects. The tutorial enables the audience to identify semantic B2B integration problems as well as to determine the benefits and deficiencies of various technical integration architecture approaches or B2B integration technologies.},
	Acmid = {375810},
	Address = {New York, NY, USA},
	Author = {Bussler, Christoph},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:19:56 +0100},
	Doi = {10.1145/376284.375810},
	Issn = {0163-5808},
	Issue_Date = {June 2001},
	Journal = {SIGMOD Rec.},
	Number = {2},
	Pages = {625-625},
	Publisher = {ACM},
	Title = {Semantic B2B Integration},
	Url = {http://doi.acm.org/10.1145/376284.375810},
	Volume = {30},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/376284.375810},
	Bdsk-Url-2 = {https://doi.org/10.1145/376284.375810}}

@article{Doan:2001:RSD:376284.375731,
	Abstract = {A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.},
	Acmid = {375731},
	Address = {New York, NY, USA},
	Author = {Doan, AnHai and Domingos, Pedro and Halevy, Alon Y.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:22:18 +0100},
	Doi = {10.1145/376284.375731},
	Issn = {0163-5808},
	Issue_Date = {June 2001},
	Journal = {SIGMOD Rec.},
	Number = {2},
	Numpages = {12},
	Pages = {509--520},
	Publisher = {ACM},
	Title = {Reconciling Schemas of Disparate Data Sources: A Machine-learning Approach},
	Url = {http://doi.acm.org/10.1145/376284.375731},
	Volume = {30},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/376284.375731},
	Bdsk-Url-2 = {https://doi.org/10.1145/376284.375731}}

@article{Lang:2001:MHI:376284.375716,
	Abstract = {A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.},
	Acmid = {375716},
	Address = {New York, NY, USA},
	Author = {Lang, Christian A. and Singh, Ambuj K.},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:25:14 +0100},
	Doi = {10.1145/376284.375716},
	Issn = {0163-5808},
	Issue_Date = {June 2001},
	Journal = {SIGMOD Rec.},
	Number = {2},
	Numpages = {12},
	Pages = {389--400},
	Publisher = {ACM},
	Title = {Modeling High-dimensional Index Structures Using Sampling},
	Url = {http://doi.acm.org/10.1145/376284.375716},
	Volume = {30},
	Year = {2001},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/376284.375716},
	Bdsk-Url-2 = {https://doi.org/10.1145/376284.375716}}

@article{Awan:2007:MHS:1272998.1273014,
	Abstract = {In this paper, we present COSMOS, a novel architecture for macroprogramming heterogeneous sensor network systems. Macroprogramming specifies aggregate system behavior, as opposed to device-specific programs that code distributed behavior using explicit messaging. COSMOS is comprised of a macroprogramming language, mPL, and an operating system, mOS. mPL macroprograms are statically verifiable compositions of reusable user-specified, or system supported functional components. The mOS node/network operating system provides component management and a lean execution environment for mPL programs in heterogeneous resource-constrained sensor networks. It provides runtime application instantiation, with over-the-air reprogramming of the network. COSMOS facilitates composition of complex real-world applications that are robust, scalable and adaptive in dynamic data-driven sensor network environments. An important and novel aspect of COSMOS is the ability to easily extend its component basis library to add rich macroprogramming abstractions to mPL, tailored to domain and resource constraints, without modifications to the OS. Applications built on COSMOS are currently in use at the Bowen Labs for Structural Engineering, in Purdue University, for high-fidelity structural monitoring. We present a detailed description of the COSMOS architecture, its various components, and a comprehensive experimental evaluation using macro- and micro- benchmarks to demonstrate performance characteristics of COSMOS.},
	Acmid = {1273014},
	Address = {New York, NY, USA},
	Author = {Awan, Asad and Jagannathan, Suresh and Grama, Ananth},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:45:31 +0100},
	Doi = {10.1145/1272998.1273014},
	Issn = {0163-5980},
	Issue_Date = {June 2007},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Keywords = {dataflow, heterogeneous networks, macroprogramming, wireless sensor networks},
	Number = {3},
	Numpages = {14},
	Pages = {159--172},
	Publisher = {ACM},
	Title = {Macroprogramming Heterogeneous Sensor Networks Using Cosmos},
	Url = {http://doi.acm.org/10.1145/1272998.1273014},
	Volume = {41},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1272998.1273014},
	Bdsk-Url-2 = {https://doi.org/10.1145/1272998.1273014}}

@article{Baldwin:2009:PSS:1496909.1496919,
	Abstract = {Virtualization brings exibility to the data center and enables separations allowing for better security properties. For these security properties to be fully utilized, virtual machines need to be able to connect to secure services such as networking and storage. This paper addresses the problems associated with managing the cryptographic keys upon which such services rely by ensuring that keys remain within the trusted computing base. Here we describe a general architecture for managing keys tied to the underlying virtualized systems, with a specific example given for secure storage.},
	Acmid = {1496919},
	Address = {New York, NY, USA},
	Author = {Baldwin, Adrian and Dalton, Chris and Shiu, Simon and Kostienko, Krzysztof and Rajpoot, Qasim},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:48:20 +0100},
	Doi = {10.1145/1496909.1496919},
	Issn = {0163-5980},
	Issue_Date = {January 2009},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Keywords = {TCG, key management, storage, virtualization},
	Number = {1},
	Numpages = {8},
	Pages = {44--51},
	Publisher = {ACM},
	Title = {Providing Secure Services for a Virtual Infrastructure},
	Url = {http://doi.acm.org/10.1145/1496909.1496919},
	Volume = {43},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1496909.1496919},
	Bdsk-Url-2 = {https://doi.org/10.1145/1496909.1496919}}

@article{Chinya:2011:BDE:1945023.1945027,
	Abstract = {In this paper, we introduce Bothnia, an extension to the Intel production graphics driver to support a shared virtual memory heterogeneous multithreading programming model. With Bothnia, the Intel graphics device driver can support both the traditional 3D graphics rendering software stack and a new class of heterogeneous multithreaded applications, which can use both IA (Intel Architecture) CPU cores and Intel integrated Graphics and Media Accelerator (GMA) cores in the same virtual address space. We describe the necessary architectural supports in both IA CPU and the GMA cores and present a reference Bothnia implementation. For a set of GPU accelerated media applications on a PC platform with Intel Core 2 Duo CPU and the Intel integrated GMA X3000 running under the Windows XP operating system, Bothnia achieves an average speedup of 3.6x compared to using the GPU as a device, primarily due to Bothnia's support for creation of shared virtual address space between heterogeneous threads of the same application spread on both IA CPU and GMA cores.},
	Acmid = {1945027},
	Address = {New York, NY, USA},
	Author = {Chinya, Gautham N. and Collins, Jamison D. and Wang, Perry H. and Jiang, Hong and Lueh, Guei-Yuan and Piazza, Thomas A. and Wang, Hong},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:39:02 +0100},
	Doi = {10.1145/1945023.1945027},
	Issn = {0163-5980},
	Issue_Date = {January 2011},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Keywords = {GPGPU, heterogeneous multicore systems, media},
	Number = {1},
	Numpages = {10},
	Pages = {11--20},
	Publisher = {ACM},
	Title = {Bothnia: A Dual-personality Extension to the Intel Integrated Graphics Driver},
	Url = {http://doi.acm.org/10.1145/1945023.1945027},
	Volume = {45},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1945023.1945027},
	Bdsk-Url-2 = {https://doi.org/10.1145/1945023.1945027}}

@article{Dalton:2009:TVP:1496909.1496918,
	Abstract = {This paper introduces our work around combining machine virtualization technology with Trusted Computing Group technology. We first describe our architecture for reducing and containing the privileged code of the Xen Hypervisor. Secondly we describe our Trusted Virtual Platform architecture. This is aimed at supporting the strong enforcement of integrity and security policy controls over a virtual entity where a virtual entity can be either a full guest operating system or virtual appliance running on a virtualized platform. The architecture includes a virtualization-specific integrity measurement and reporting framework. This is designed to reflect all the dependencies of the virtual environment of a guest operating system. The work is a core enabling component of our research around converged devices -- client platforms such as notebooks or desktop PCs that can safely host multiple virtual operating systems and virtual appliances concurrently and report accurately on the trustworthiness of the individually executing entities.},
	Acmid = {1496918},
	Address = {New York, NY, USA},
	Author = {Dalton, Chris I. and Plaquin, David and Weidner, Wolfgang and Kuhlmann, Dirk and Balacheff, Boris and Brown, Richard},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:51:41 +0100},
	Doi = {10.1145/1496909.1496918},
	Issn = {0163-5980},
	Issue_Date = {January 2009},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Keywords = {TCG, TPM, open trusted computing, trusted virtualization},
	Number = {1},
	Numpages = {8},
	Pages = {36--43},
	Publisher = {ACM},
	Title = {Trusted Virtual Platforms: A Key Enabler for Converged Client Devices},
	Url = {http://doi.acm.org/10.1145/1496909.1496918},
	Volume = {43},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1496909.1496918},
	Bdsk-Url-2 = {https://doi.org/10.1145/1496909.1496918}}

@article{Gordon:2002:SCC:635508.605428,
	Abstract = {With the increasing miniaturization of transistors, wire delays are becoming a dominant factor in microprocessor performance. To address this issue, a number of emerging architectures contain replicated processing units with software-exposed communication between one unit and another (e.g., Raw, SmartMemories, TRIPS). However, for their use to be widespread, it will be necessary to develop compiler technology that enables a portable, high-level language to execute efficiently across a range of wire-exposed architectures.In this paper, we describe our compiler for StreamIt: a high-level, architecture-independent language for streaming applications. We focus on our backend for the Raw processor. Though StreamIt exposes the parallelism and communication patterns of stream programs, some analysis is needed to adapt a stream program to a software-exposed processor. We describe a partitioning algorithm that employs fission and fusion transformations to adjust the granularity of a stream graph, a layout algorithm that maps a stream graph to a given network topology, and a scheduling strategy that generates a fine-grained static communication pattern for each computational element.We have implemented a fully functional compiler that parallelizes StreamIt applications for Raw, including several load-balancing transformations. Using the cycle-accurate Raw simulator, we demonstrate that the StreamIt compiler can automatically map a high-level stream abstraction to Raw without losing performance. We consider this work to be a first step towards a portable programming model for communication-exposed architectures.},
	Acmid = {605428},
	Address = {New York, NY, USA},
	Author = {Gordon, Michael I. and Thies, William and Karczmarek, Michal and Lin, Jasper and Meli, Ali S. and Lamb, Andrew A. and Leger, Chris and Wong, Jeremy and Hoffmann, Henry and Maze, David and Amarasinghe, Saman},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:33:25 +0100},
	Doi = {10.1145/635508.605428},
	Issn = {0163-5980},
	Issue_Date = {December 2002},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Number = {5},
	Numpages = {13},
	Pages = {291--303},
	Publisher = {ACM},
	Title = {A Stream Compiler for Communication-exposed Architectures},
	Url = {http://doi.acm.org/10.1145/635508.605428},
	Volume = {36},
	Year = {2002},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/635508.605428},
	Bdsk-Url-2 = {https://doi.org/10.1145/635508.605428}}

@article{Ho:2006:PTP:1218063.1217939,
	Abstract = {Many software attacks are based on injecting malicious code into a target host. This paper demonstrates the use of a well-known technique, data tainting, to track data received from the network as it propagates through a system and to prevent its execution. Unlike past approaches to taint tracking, which track tainted data by running the system completely in an emulator or simulator, resulting in considerable execution overhead, our work demonstrates the ability to dynamically switch a running system between virtualized and emulated execution. Using this technique, we are able to explore hardware support for taint-based protection that is deployable in real-world situations, as emulation is only used when tainted data is being processed by the CPU. By modifying the CPU, memory, and I/O devices to support taint tracking and protection, we guarantee that data received from the network may not be executed, even if it is written to, and later read from disk. We demonstrate near native speeds for workloads where little taint data is present.},
	Acmid = {1217939},
	Address = {New York, NY, USA},
	Author = {Ho, Alex and Fetterman, Michael and Clark, Christopher and Warfield, Andrew and Hand, Steven},
	Date-Added = {2019-11-01 12:22:59 +0100},
	Date-Modified = {2020-10-21 17:47:52 +0100},
	Doi = {10.1145/1218063.1217939},
	Issn = {0163-5980},
	Issue_Date = {October 2006},
	Journal = {SIGOPS Oper. Syst. Rev.},
	Keywords = {QEMU, Xen, demand emulation, emulation, false tainting, tainting, virtual machine, virtualization},
	Number = {4},
	Numpages = {13},
	Pages = {29--41},
	Publisher = {ACM},
	Title = {Practical Taint-based Protection Using Demand Emulation},
	Url = {http://doi.acm.org/10.1145/1218063.1217939},
	Volume = {40},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1218063.1217939},
	Bdsk-Url-2 = {https://doi.org/10.1145/1218063.1217939}}

@inproceedings{6344557,
	Abstract = {Model-based testing is a well known technique to generate automatically highly qualitative tests for a given system based on a simplified testing model. Test-driven development is an established development practice in the agile development projects, which implies firstly the partial specification of a system by using tests, and after this, the development of the system. In test driven development the system implementation is continuously checked against the tests in order to assess its correctness with respect to the specification. In this paper we investigate how can these two methods be combined such that the advantages of these two approaches can be leveraged: highly qualitative test-cases used as specification of requirements and support of a continuous checking of architecture. We propose to formalize sub-sets of requirements into models that are amenable to generate tests by using automatic techniques well-known from model based testing. These tests can then be used to check the system architecture specification against the requirements in a continuous manner.},
	Author = {D. {Mou} and D. {Ratiu}},
	Booktitle = {2012 First IEEE International Workshop on the Twin Peaks of Requirements and Architecture (TwinPeaks)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:34 +0100},
	Doi = {10.1109/TwinPeaks.2012.6344557},
	Keywords = {automatic programming;formal specification;object-oriented programming;program testing;program verification;software architecture;software prototyping;model-based test-driven development;component architecture;automatic high qualitative test generation;agile development projects;partial system specification;requirements specification;continuous architecture checking;automatic techniques;system architecture specification;Testing;Computer architecture;Load modeling;Computational modeling;Component architectures;Embedded systems;USA Councils;model-based;test-driven},
	Pages = {27-30},
	Title = {Binding requirements and component architecture by using model-based test-driven development},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TwinPeaks.2012.6344557}}

@inproceedings{6062266,
	Abstract = {Software architecture is taking a bad rap with the agilists---proponents of agile and lean software development approaches: "BUFD big up-front design", "YAGNI You Ain't Gonna Need It", "massive documentation", "smells of waterfall", it is pictured as a typical non-agile practice. However, certain classes of system, ignoring architectural issues too long "hit a wall" and collapse by lack of an architectural focus. 'Agile architecture': a paradox, an oxymoron, two totally incompatible approaches? In this tutorial, we examine the real issues at stake, beyond the rhetoric and posturing, and show that the two cultures can coexist and support each other, where appropriate. We define heuristics to scope how much architecture a project really needs, to assign actual value to an otherwise invisible architecture; and we review management and development practices that do work in the circumstances where some significant architectural effort is needed, when you are actually going to need it.},
	Author = {P. {Kruchten}},
	Booktitle = {2010 ACM/IEEE 32nd International Conference on Software Engineering},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:46:41 +0100},
	Doi = {10.1145/1810295.1810448},
	Keywords = {software architecture;software development management;software prototyping;system documentation;agile software architecture;agile software development;lean software development;BUFD big up-front design;architectural issue;project architecture;architectural effort;Computer architecture;Software architecture;Programming;Software;Tutorials;Documentation;agile process;software architecture},
	Pages = {497-498},
	Title = {Software architecture and agile software development: a clash of two cultures?},
	Volume = {2},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1145/1810295.1810448}}

@inproceedings{5190630,
	Abstract = {Test-Driven Development (TDD) is an important software development practice that enables rapid iterations, refactoring, and improved quality. Supporting TDD can be difficult when building Service-Oriented Architecture (SOA) applications, since standard test frameworks often do not have capabilities for performing and validating Web service (WS) calls; invoking Web services depends on running and connecting to a service container; and services and clients often have entirely separate implementations. In this paper we present case studies of two SOA applications we developed, GRIDL and TxFlow. These are distributed, multi-language applications using Web services as the interface between service and client components. They implement Web service and client tests both for verification and validation of the application components and to facilitate the TDD process. Our approach to Web service testing to support TDD is easily reproducible in any SOA application without requiring significant development effort or changes to the software design.},
	Author = {P. {Hamill} and D. {Alexander} and S. {Shasharina}},
	Booktitle = {2009 Congress on Services - I},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:44:54 +0100},
	Doi = {10.1109/SERVICES-I.2009.113},
	Keywords = {program verification;software architecture;Web services;Web service validation;test-driven development;software development practice;service-oriented architecture;GRIDL application;TxFlow application;Web services;Application software;Service oriented architecture;Software testing;Programming;Buildings;Performance evaluation;Joining processes;Containers;Software design;Web services;testing;validation;Test-Driven Development;Service Oriented Architecture},
	Pages = {467-470},
	Title = {Web Service Validation Enabling Test-Driven Development of Service-Oriented Applications},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/SERVICES-I.2009.113}}

@inproceedings{6324602,
	Abstract = {Development process divided into two categories: Traditional models and agile models, traditional models have heavy documentation and delay in time project but agile models being lightweight processes because have light weight documentation and heavy intercommunications therefore reduce the delays of the traditional models. They emphasize customer satisfaction, fast response to changes, and release in less time. In the last decades, to achieve high quality of system, use the architecture as an important matter in the development process. Therefore using software architecture skills in agile methods could improve them toward producing a software system that has an appropriate structure because focuses on achieving quality attributes for a software system. This paper is a review on accomplished proceedings that combine software architecture and agile methods to improve Software developments.},
	Author = {F. {Akbari} and S. M. {Sharafi}},
	Booktitle = {2012 International Symposium on Instrumentation Measurement, Sensor Network and Automation (IMSNA)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:04 +0100},
	Doi = {10.1109/MSNA.2012.6324602},
	Keywords = {customer satisfaction;software architecture;software prototyping;software quality;system documentation;software architecture;agile method;software development process;traditional software model;documentation;delay;customer satisfaction;software quality attribute;Computer architecture;Software architecture;Crystals;Software systems;Conferences;Adaptation models;Agile method;FDD;XP;Crystal;Software Architecture;QAW;ADD;ATAM;ARID;Quality attributes},
	Pages = {389-392},
	Title = {A review to the usage of concepts of software architecture in agile methods},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/MSNA.2012.6324602}}

@inproceedings{6827100,
	Abstract = {This paper is an experience report of a long running Scrum project, conducted in a collaboration between industry and research, in a so called "Joint Research and Development Laboratory". Over time in the collaboration, we experienced a constant decrease in the pace of our development progress. Planning forward only within the limits of single sprints was the main reason for this. It resulted in a degenerating design and therefore a lack of flexibility that affected the agility of our project. Therefore, we introduced the concept of "epic-architectures", an architecture design for a coherent group of user stories. Shifting the planning horizon further, across single sprints, helped us to create more stable and reusable concepts and to construct simpler, more elegant, and more maintainable solutions. We were able to significantly reduce the refactoring effort and increase the development speed, without significant overhead. With reporting on our experiences we hope to provide practically applicable guidance on how to integrate lightweight architecting in agile development processes, to sustain agility while creating high quality products.},
	Author = {B. {Weitzel} and D. {Rost} and M. {Scheffe}},
	Booktitle = {2014 IEEE/IFIP Conference on Software Architecture},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:32 +0100},
	Doi = {10.1109/WICSA.2014.38},
	Keywords = {project management;software architecture;software maintenance;software prototyping;software quality;joint research and development laboratory;Scrum project;project agility;epic-architectures;architecture design;user stories;planning horizon;maintainable solutions;refactoring effort;development speed;lightweight architecting;agile development processes;high quality products;Computer architecture;Planning;Software architecture;Software;Concrete;Documentation;Collaboration;Architecture;agile;Scrum;experience report;industry;sustainable;agility;development},
	Pages = {53-56},
	Title = {Sustaining Agility through Architecture: Experiences from a Joint Research and Development Laboratory},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2014.38}}

@inproceedings{4599475,
	Abstract = {Large enterprise organisations usually have a project management office (PMO) whose responsibility it is to oversee a standardisation of project management practices across a portfolio. This can often be in conflict with the agile principles of self-organisation and inspect and adapt. We describe how one of Yahoo! Europepsilas teams helped implement a process which was compatible both with agile values and principles, and European PMO standards, and also how it has influenced international process framework evolution. As a result, we demonstrate that a PMO process can be compatible with an agile process, and that a PMO can add value to agile teams.},
	Author = {K. {Scotland} and A. {Boutin}},
	Booktitle = {Agile 2008 Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:31 +0100},
	Doi = {10.1109/Agile.2008.22},
	Keywords = {groupware;project management;software development management;team working;project management office;Yahoo teams;agile principles;European PMO standards;international process framework evolution;process groups;Scrum;Logic gates;Europe;Book reviews;Organizations;Computer architecture;Business;Process control;Agile;Scrum;Process;Framework;Europe;PMO;Project Office;Standard;Experience},
	Pages = {191-195},
	Title = {Integrating Scrum with the Process Framework at Yahoo! Europe},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/Agile.2008.22}}

@inproceedings{6827125,
	Abstract = {Requirements evolve over the development lifecycle of a software project. Agile practices are designed specifically to address this challenge while showing early and continuous progress towards project goals. Applying an agile approach allows stakeholders to adapt the scope and capabilities of a development release to changing market needs. More recently, an agile approach has been recommended for developing the architecture of software systems, enabling the design to support current requirements and early releases while evolving to meet future expectations. Our experience defining emergent software systems to build a product line architecture for advanced data analytics demonstrates the benefits that can be gained from prioritizing work activities and delaying architecture decisions. This paper proposes a process and ontology for agile architecture development. Only the necessary aspects for each evolutionary release are designed and prototyped, as determined by expectations of the identified application domain scenarios. Feedback from implementing the scenarios using the architecture extends our understanding of the requirements and provides the backlog for successive design iterations.},
	Author = {K. E. {Harper} and A. {Dagnino}},
	Booktitle = {2014 IEEE/IFIP Conference on Software Architecture},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:20 +0100},
	Doi = {10.1109/WICSA.2014.16},
	Keywords = {ontologies (artificial intelligence);software architecture;software prototyping;agile software architecture;advanced data analytics;software project development lifecycle;agile practices;project goals;software system architecture;architecture decisions;ontology;agile architecture development;application domain scenario;design iterations;Computer architecture;Ontologies;Software architecture;Automation;Software systems;Unified modeling language;software architecture;agile development;product line architectures;industrial experiments;industry best practices},
	Pages = {243-246},
	Title = {Agile Software Architecture in Advanced Data Analytics},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2014.16}}

@inproceedings{6344556,
	Abstract = {Traditionally the flow of authoritative information and control in requirements and software engineering is from requirements to architecture, design, development, implementation and testing. Iterative, spiral and agile methods, among others, have introduced increments and iterations in eliciting and discovering requirements within the project life cycle. Yet the authoritative flow of information across organizational boundaries within the enterprise continues to be from requirements to architecture to design. We argue that two additional implicit sources of information should be included in the requirements engineering process, contextual environment concerns and architectural patterns and heuristics. To account for these two sources of implicit requirements information we introduce the concept of forward and backward inferred macro-architectural requirements. Forward inferred macro-architectural requirements are elicited from contextual environment concerns. Backward inferred macro-architectural requirements are extracted through a reverse requirements elicitation process from architectural heuristics and patterns. We have observed significant improvements in the efficiency of the development processes and the quality of the final software products as a result of making inferred macro-architectural requirements explicit.},
	Author = {P. {Petrov} and U. {Buy} and R. L. {Nord}},
	Booktitle = {2012 First IEEE International Workshop on the Twin Peaks of Requirements and Architecture (TwinPeaks)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:40 +0100},
	Doi = {10.1109/TwinPeaks.2012.6344556},
	Keywords = {formal specification;organisational aspects;software architecture;software quality;software architecture analysis process;software architecture design process;forward inferred macroarchitectural requirements;authoritative information flow;requirements control;software engineering;iterative methods;spiral methods;agile methods;requirement elicitation;requirement discovery;project life cycle;organizational boundaries;requirements engineering process;contextual environment concerns;architectural patterns;architectural heuristics;backward inferred macro-architectural requirements;software product quality;Computer architecture;Software architecture;Software;Organizations;Context;Ecosystems;Enterprise architecture;software architecture;contextual concern;architectural heuristics;architectural requirements;requirements engineering;analysis and design},
	Pages = {20-26},
	Title = {Enhancing the software architecture analysis and design process with inferred macro-architectural requirements},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/TwinPeaks.2012.6344556}}

@inproceedings{4599461,
	Abstract = {The ability of agile practices to scale to "large" software development efforts (more than three teams or 30 team members) has been widely debated in recent years. While a variety of inhibitors to scaling agile are frequently cited, this experience report describes how the primary challenge to scaling agile that we faced was finding the right people to build our agile tribe. The effective adoption of agile requires passionate, motivated individuals who accept change, think like a business owner, operate willingly outside team walls, and identify and drive program issues. Finding such a large number of people with these qualities is much more difficult than establishing methods to support communications, progress tracking, and other agile practices. We conclude by describing the attributes of the type of person that we believe will thrive in a large-scale agile shop.},
	Author = {E. {Moore} and J. {Spens}},
	Booktitle = {Agile 2008 Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:44 +0100},
	Doi = {10.1109/Agile.2008.43},
	Keywords = {software development management;team working;agile practice;software development;agile tribe;large-scale agile shop;Computer architecture;Programming;USA Councils;Software;Testing;Service oriented architecture;Radiation detectors;culture large-scale agile recruiting},
	Pages = {121-124},
	Title = {Scaling Agile: Finding your Agile Tribe},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/Agile.2008.43}}

@inproceedings{4959408,
	Abstract = {Unified process (UP) and extreme programming (XP) have been adopted widely in the software development world. The main concepts and practices of UP and XP are analyzed and the unreasonable or impracticable ones are figured out. A new software development process, the UniX process, is proposed. The new process model and its phases, releases, iterations and workflows are briefly analyzed in comparison with the UP and XP. The conclusion is that the UniX process, adopting the advantages of UP and XP and discarding their disadvantages, is a more adaptable and promising process to faster develop better software.},
	Author = {Y. {Zhou}},
	Booktitle = {2009 First International Workshop on Education Technology and Computer Science},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:44:02 +0100},
	Doi = {10.1109/ETCS.2009.690},
	Keywords = {programming;software architecture;UniX process;UP;unified process;XP;extreme programming;software development practice;architecture-centered;Merging;Testing;Programming profession;Unified modeling language;Educational technology;Computer science;Software quality;Feedback;Computer architecture;Computer science education;software process;Unified Process;Extreme Programming;UniX Process;architecture-centered},
	Pages = {699-702},
	Title = {UniX Process, Merging Unified Process and Extreme Programming to Benefit Software Development Practice},
	Volume = {3},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ETCS.2009.690}}

@inproceedings{4670306,
	Abstract = {This paper suggests a holistic design and development method combining test-driven and model-driven development for SOA architectures. It uses test-driven development on component level and model-based testing on system level. Moreover, monitored performance parameters during test execution serve as input for a model-driven performance analysis of the business application, providing early indication of possible performance issues.},
	Author = {S. {Wieczorek} and A. {Stefanescu} and M. {Fritzsche} and J. {Schnitter}},
	Booktitle = {Testing: Academic Industrial Conference - Practice and Research Techniques (taic part 2008)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:18 +0100},
	Doi = {10.1109/TAIC-PART.2008.17},
	Keywords = {enterprise resource planning;program testing;software architecture;software performance evaluation;test driven development;model based testing;performance analysis;SOA architectures;business application;Performance analysis;Service oriented architecture;System testing;Automatic testing;Enterprise resource planning;Application software;Programming;Software testing;Design methodology;Semiconductor optical amplifiers;model-based testing;agile;SOA;test-driven development;performance;model-driven development},
	Pages = {82-86},
	Title = {Enhancing Test Driven Development with Model Based Testing and Performance Analysis},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/TAIC-PART.2008.17}}

@inproceedings{1611687,
	Abstract = {Despite the fact that nowadays, architectures, development processes, frameworks and technologies are maturing to be service oriented architecture (SOA). Former frameworks for enterprise architecting couldn't be matched with those SOAs. Thus putting to use those frameworks to the business enterprises make IT/ICT consultants and enterprise architects encounter with some complex difficulties in the enterprise architecting projects. ISRUP e-service framework is proposed and developed based on SOA and RUP to converge e-business, e-commerce and e-government concepts through just leveraging e-services (especially Web and grid services) by architecting the information systems in the enterprises. This paper explains in details that what ISRUP is},
	Author = {S. M. {Hashemi} and M. {Razzazi} and A. {Bahrami}},
	Booktitle = {Third International Conference on Information Technology: New Generations (ITNG'06)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:37:37 +0100},
	Doi = {10.1109/ITNG.2006.82},
	Keywords = {electronic commerce;government data processing;information systems;Internet;software architecture;ISRUP e-service framework;agile business enterprise architecture;service oriented architecture;e-business;e-commerce;e-government;Web service;grid service;enterprise information system;Unified modeling language;Service oriented architecture;Computer architecture;Terminology;Logic;Web and internet services;Context modeling;Electronic government;Management information systems;Software engineering;UML;RUP;Zachman;SOA;ISRUP E-Service;framework;E-Service;Agile;Enterprise Pattern;View;Model;AS-IS;TO-BE;Activity;Artifact;Context.},
	Pages = {701-706},
	Title = {ISRUP E-Service Framework for agile Enterprise Architecting},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/ITNG.2006.82}}

@inproceedings{5646224,
	Abstract = {Intelligentized distributed system is advanced mode of network operation in agile manufacturing system. In intelligent agile manufacturing, Architecture of knowledge base directly influences intelligent level of manufacture. Application knowledge and reorganization are two sides of implement intelligent application. When application knowledge don't meet needs of application, starting reorganization knowledge base architecture will get new application knowledge. This is second level knowledge base(shallow knowledge and depth knowledge) architecture. Research of knowledge base in agile manufacturing will provide implemented knowledge architecture for intelligent agile manufacturing system and enable whole system high knowledge.},
	Author = {{Guo-chang Li}},
	Booktitle = {2010 Third International Symposium on Knowledge Acquisition and Modeling},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:48:04 +0100},
	Doi = {10.1109/KAM.2010.5646224},
	Keywords = {agile manufacturing;distributed processing;intelligent manufacturing systems;intelligentized agile manufacturing system;distributed knowledge base architecture;intelligent agile manufacturing;shallow knowledge;depth knowledge;Artificial intelligence;Welding;Computer architecture;Middleware;Educational institutions;Distributed;knowledge base;agile manufacturing;intelligent;roll},
	Pages = {1-4},
	Title = {Research on intelligentized agile manufacturing system in distributed knowledge base architecture},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/KAM.2010.5646224}}

@inproceedings{4340948,
	Abstract = {Agile supply chain comes into being with the development of supply chain and agile manufacturing. It is an effective method to use multiagent technology to support agile supply chain management. Based on the analysis on the characteristic of agile supply chain and multiagent technology, and to achieve the goal of quick response and dynamic reconstruction, this paper presents the architecture of multiagent-based agile supply chain, and analyzes the construction and the operation mechanism of enterprise agents and coordination agent in detail. Furthermore, taking CORBA (Common Object Request Broker Architecture) as the distributed standard, this paper discusses the design and the development of agile supply chain system.},
	Author = {Z. {Zhang}},
	Booktitle = {2007 International Conference on Wireless Communications, Networking and Mobile Computing},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:39:56 +0100},
	Doi = {10.1109/WICOM.2007.1173},
	Keywords = {agile manufacturing;distributed object management;multi-agent systems;production engineering computing;supply chain management;multiagent-based agile supply chain architecture;agile manufacturing;multiagent technology;agile supply chain management;common object request broker architecture;Supply chains;Agile manufacturing;Supply chain management;Multiagent systems;Standards development;Technology management;Control systems;Electronic mail;Virtual enterprises;IP networks},
	Pages = {4782-4785},
	Title = {The Architecture of Multiagent-Based Agile Supply Chain},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICOM.2007.1173}}

@inproceedings{5457762,
	Abstract = {One of the ideas of agile software development is to respond to changes rather than following a plan. Constantly changing businesses result in changing requirements, to be handled in the development process. Therefore, it is essential that the underlying software architecture is capable of managing agile business processes. However, criticism on agile software development states that it lacks paying attention to architectural and design issues and therefore is bound to engender suboptimal design-decisions. In this paper we propose an architectural framework for agile software development, that by explicitly separating computational, coordinational, and communicational models offers a high degree of flexibility regarding architectural and design changes introduced by agile business processes. The framework strength is facilitated by combining the characteristics and properties of state-of-the-art middleware architectural styles captured in a simple API. The benefit of our approach is a clear architectural design with minimized effects of changes the models have on each other, accompanied by an efficient realization of new business requirements.},
	Author = {R. {Mordinyi} and E. {K{\"u}hn} and A. {Schatten}},
	Booktitle = {2010 17th IEEE International Conference and Workshops on Engineering of Computer Based Systems},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:46:29 +0100},
	Doi = {10.1109/ECBS.2010.38},
	Keywords = {application program interfaces;business data processing;middleware;software architecture;software prototyping;agile software development;software architecture;agile business processes;middleware architectural styles;API;Programming;Application software;Software architecture;Middleware;Variable speed drives;Logic;Systems engineering and theory;Space technology;Software systems;Conferences;Architectural Styles;Agile Business Requirements;Agile Software Development;Decoupling;Abstraction},
	Pages = {276-280},
	Title = {Towards an Architectural Framework for Agile Software Development},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ECBS.2010.38}}

@inproceedings{5562813,
	Abstract = {In this paper, factor analysis is applied on a set of data that was collected to study the effectiveness of 58 different agile practices. The analysis extracted 15 factors, each was associated with a list of practices. These factors with the associated practices can be used as a guide for agile process improvement. Correlations between the extracted factors were calculated, and the significant correlation findings suggested that people who applied iterative and incremental development and quality assurance practices had a high success rate, that communication with the customer was not very popular as it had negative correlations with governance and iterative and incremental development. Also, people who applied governance practices also applied quality assurance practices. Interestingly success rate related negatively with traditional analysis methods such as Gantt chart and detailed requirements specification.},
	Author = {N. {Abbas} and A. M. {Gravell} and G. B. {Wills}},
	Booktitle = {2010 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:29 +0100},
	Doi = {10.1109/AGILE.2010.15},
	Keywords = {iterative methods;pattern clustering;software prototyping;factor analysis;agile practices clusters;agile process improvement;incremental development;iterative development;Gantt chart;Correlation;Quality assurance;Programming;Testing;Computer architecture;Databases;Organizations;agile software development;agile process improvement;empirical research;factor analysis;agile practices},
	Pages = {11-20},
	Title = {Using Factor Analysis to Generate Clusters of Agile Practices (A Guide for Agile Process Improvement)},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2010.15}}

@inproceedings{5685916,
	Abstract = {Test-driven development (TDD) is the practice of creating automated unit tests that exercise planned software functionality, before writing the software itself. TDD leads to higher quality software by encouraging developers to analyze individual units of behavior before coding and by allowing easy detection of problems introduced by software change. However, few tools effectively support TDD for graphical user interfaces. Those that do are platform-specific and require developers to express all tests as executable code for the target platform. This paper describes UIT (User Interface Tester), a toolset designed to support TDD for interactive applications. UIT addresses the weaknesses of existing solutions by using human-readable declarative test scripts and by generating skeleton GUIs from test scripts so that tests can be created first. It also supports driving GUIs that target different platforms. We present details of the scripting language, an overview of the UIT architecture with a focus on its multi-platform capabilities, and a discussion of current status and future plans. We believe that as UIT matures, it can assist developers in reaping the benefits of TDD.},
	Author = {S. E. {Goldin} and T. {Luengwitayakorn} and S. {Supadarattanawong}},
	Booktitle = {TENCON 2010 - 2010 IEEE Region 10 Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:48:10 +0100},
	Doi = {10.1109/TENCON.2010.5685916},
	Keywords = {authoring languages;graphical user interfaces;software engineering;test driven development;graphical UI;multiplatform toolset;automated unit test;planned software functionality;TDD;software quality;graphical user interface;UIT;user interface tester;interactive application;human readable declarative test script;scripting language;Graphical user interfaces;Testing;Software;Containers;XML;Engines;Layout},
	Pages = {2429-2433},
	Title = {Test-driven development for graphical UIs: A multi-platform toolset},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/TENCON.2010.5685916}}

@inproceedings{5775264,
	Abstract = {Laboratory experiments were conducted to protect and consolidate historic architectural heritages by bacterially induced carbonate mineralization forming a deposition layer on the surface of marble samples. Then, the effects of crystal phase and growth of the mineralized layer as well as the deposited crystal on the porosity of samples and on the efficiency of bonding and protection were analyzed by means of XRD, SEM, mercury intrusion porosimeter (MIP) and ultrasonic test. Results show that the mineral crystal is composed of calcite and vaterite phases, that bacteria act as nucleation sites in the precipitation of the mineral crystal uniformly depositing on the sample surface, that the precipitation has no significant effect on the pore size distribution of samples rather than results in a porosity decrease by 22.2%, and that the mineral crystal strongly attaches the substratum. It is thus concluded that the bacterially-induced biomineralization is effective in remediating historic stone architectural heritages and can be an ecological and novel alternative to traditional techniques.},
	Author = {{Peihao Li} and {Wenjun Qu}},
	Booktitle = {2011 International Conference on Electric Technology and Civil Engineering (ICETCE)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:48:54 +0100},
	Doi = {10.1109/ICETCE.2011.5775264},
	Keywords = {architecture;biotechnology;building materials;crystal growth;ecology;history;microorganisms;minerals;nucleation;porosity;rocks;scanning electron microscopy;structural engineering;ultrasonic materials testing;X-ray diffraction;bioremediation;Sporosarcina pasteurii;laboratory experiment;carbonate mineralization;deposition layer;marble sample;crystal phase;crystal growth;mineralized layer;deposited crystal;porosity;bonding;XRD;SEM;mercury intrusion porosimeter;ultrasonic test;mineral crystal;calcite phase;vaterite phase;nucleation site;bacterially-induced biomineralization;historic stone architectural heritage;ecology;Microorganisms;Crystals;Mineralization;Scanning electron microscopy;Acoustics;Calcium;marble;calcium carbonate;Historic Architectural Heritages;biomineralization;bioremediation},
	Pages = {1084-1087},
	Title = {Bioremediation of historic architectural heritages by Sporosarcina pasteurii},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICETCE.2011.5775264}}

@inproceedings{6735972,
	Abstract = {This paper highlights the role of Web-Oriented Architecture as an organizational infrastructure to assist the agility and organizational performance of information systems.},
	Author = {K. {Amayreh} and N. A. {Salleh}},
	Booktitle = {2013 IEEE Conference on e-Learning, e-Management and e-Services},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:54:07 +0100},
	Doi = {10.1109/IC3e.2013.6735972},
	Keywords = {organisational aspects;service-oriented architecture;software prototyping;Web services;Web-oriented architecture;organizational infrastructure;organizational performance;information systems;organizational agility;dynamic capability;Organizations;Information systems;Technological innovation;Conferences;Electronic learning;Information technology;Web-Oriented Architecture;WOA;Dynamic capabilities;Core Competencies;Agility;Perceived performance},
	Pages = {89-92},
	Title = {Creating organizational agility through the dynamic capabilities of Web-Oriented Architecture},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/IC3e.2013.6735972}}

@inproceedings{4518995,
	Abstract = {This paper describes AGSOA, an agile governance for service oriented architectures (SOAs) that is intended to address many of the inherent challenges faced by implementing the DoD's 21s' Century agile net-centric warfare systems using SOAs. The complex interdependencies within SOA-based systems, when combined with DoD's desired agile change capabilities, creates a very complex and open-ended system of systems environment that cannot simply modeled, simulated, verified and/or validated. The AGSOA framework is designed to blend elements of proven agile- style project management methodologies with contemporary SOA governance strategies used in other, less complex industries to yield a more appropriate governance strategy for life- and mission-critical DoD SOA projects.},
	Author = {E. {Sloane} and R. {Beck} and S. {Metzger}},
	Booktitle = {2008 2nd Annual IEEE Systems Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:41:21 +0100},
	Doi = {10.1109/SYSTEMS.2008.4518995},
	Keywords = {military computing;military systems;project management;software architecture;AGSOA;agile governance;service oriented architecture systems;military net-centric systems of systems;open-ended system;project management methodologies;Service oriented architecture;Open source software;Software agents;USA Councils;Government;Project management;Mission critical systems;Systems engineering and theory;Military satellites;Biomedical imaging;service oriented architecture;governance;agile methods;system of systems engineering;net-centric warfare},
	Pages = {1-4},
	Title = {AGSOA - Agile Governance for Service Oriented Architecture (SOA) Systems: A Methodology to Deliver 21st Century Military Net-Centric Systems of Systems},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/SYSTEMS.2008.4518995}}

@inproceedings{1317417,
	Abstract = {Developing software using a well-defined, well-understood process improves the likelihood of delivering a product with the required quality. Enhancing that process to meet recognised process standards, such as CMMI and ISO 9000, can further facilitate the development of complex systems in a repeatable and predictable way. There are tradeoffs involved, however. In particular, because projects differ in their scale, scope, and technical challenge, the same process will not suit all circumstances. Agile approaches to development, such as Extreme Programming (XP), SCRUM and Crystal Methodologies, recognise this dilemma and suggest that processes be tailored to each situation. The research problem for postgraduate investigation is to determine in detail how this can be achieved successfully. This will include a consideration of how best to define, maintain and give access to a knowledge base recording details of process concepts, techniques and experience.},
	Author = {F. {Keenan}},
	Booktitle = {Proceedings. 26th International Conference on Software Engineering},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:35:43 +0100},
	Doi = {10.1109/ICSE.2004.1317417},
	Keywords = {software architecture;software process improvement;software quality;software standards;agile process tailoring;problem analysis;software development;process standards;CMMI;ISO 9000;extreme programming;XP;SCRUM;Crystal Methodologies;Quality management;Information analysis;Software quality;ISO standards;Standards development;Best practices;Programming;Process design;Monitoring;SPICE},
	Pages = {45-47},
	Title = {Agile process tailoring and problem analysis (APTLY)},
	Year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSE.2004.1317417}}

@inproceedings{6149493,
	Abstract = {Agile project management continues to gain a widening and enthusiastic following. Agile methods can achieve a high level of satisfaction among all project stakeholders (users, customers, business managers, developers, and project managers) in terms of productivity, product quality, cost containment, time-to-market, and overall morale. Success with agile requires focus on requirements and design as a continuous discovery process, posing challenges for practitioners of more traditional project management both in terms of method adoption and sustained commitment. Thriving Systems Theory clarifies the appeal of agile project structure and processes, helps project teams determine and achieve the optimal portfolio of quality characteristics, and better articulate their value to all stakeholders. Thriving Systems Theory is an emerging framework of systems design quality that translates the research of design pattern patriarch Christopher Alexander on physical architecture design quality into the domain of systems engineering. The satisfaction achieved through agile methods is explained by Thriving Systems Theory's fifteen choice properties of systems design quality. We demonstrate by identifying the manifestation of the choice properties in SCRUM, an exemplar of agile software project management.},
	Author = {L. J. {Waguespack} and W. T. {Schiano}},
	Booktitle = {2012 45th Hawaii International Conference on System Sciences},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:29 +0100},
	Doi = {10.1109/HICSS.2012.513},
	Keywords = {software architecture;software prototyping;SCRUM project architecture;thriving systems theory;agile project management;project stakeholders;agile project structure;agile project process;optimal portfolio;Project management;Computer architecture;Software;Buildings;Planning;Architecture;Encapsulation;agile methods;architecture;software development;project management;design quality},
	Pages = {4943-4951},
	Title = {SCRUM Project Architecture and Thriving Systems Theory},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/HICSS.2012.513}}

@inproceedings{6525528,
	Abstract = {System development using a Service-Oriented Architecture approach encompasses new roles and tasks as opposed to traditional development. It brings new challenges in different aspects, such as: reuse, flexibility, stakeholders' involvement, business understanding. Considering methods for system development, those aspects are handled by agile methods. However, there is no consensus on how to use agile methods in service-oriented system development. Agile methods can be used in different phases of a software development lifecycle, such as: project management, modeling, software construction, software testing. Extreme Programming (XP) is one of the methods more closely related to the construction phase. XP is widely adopted in the industry and offers practices that can be applied to several business contexts. The goal of this work is to propose guidelines and best practices towards service development, focused on the construction phase, in an SOA environment using XP. It goes towards the identification of the shared concerns and the gaps between SOA and XP practices and, additionally, presents open issues and main challenges to be considered when incrementally constructing service solutions with continuous feedback and flexibility to rapidly develop services that meet changing business requirements.},
	Author = {F. {Carvalho} and L. G. {Azevedo}},
	Booktitle = {2013 IEEE Seventh International Symposium on Service-Oriented System Engineering},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:52:26 +0100},
	Doi = {10.1109/SOSE.2013.25},
	Keywords = {program testing;service-oriented architecture;software prototyping;service agile development;XP;service-oriented architecture;service-oriented system development;software development lifecycle;project management;software construction;software testing;Extreme Programming;Service-oriented architecture;Semiconductor optical amplifiers;Contracts;Complexity theory;Programming;SOA;Extreme Programming;Service Development Methods},
	Pages = {254-259},
	Title = {Service Agile Development Using XP},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/SOSE.2013.25}}

@inproceedings{6827114,
	Abstract = {An approach to software architecture creation is described in the context of agile development. It eschews the traditional separation of top-down and bottom-up design. A concrete, cumulative, least-commitment process is demonstrated that establishes an architecture core likely to remain stable as requirements are added.},
	Author = {E. J. {Braude}},
	Booktitle = {2014 IEEE/IFIP Conference on Software Architecture},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:25 +0100},
	Doi = {10.1109/WICSA.2014.26},
	Keywords = {software architecture;software prototyping;software architecture creation;agile development context;least-commitment process;cumulative software architecture development;Computer architecture;Software architecture;Software;User interfaces;Context;Context modeling;Conferences;software architecture;software design;agile development},
	Pages = {163-166},
	Title = {Cumulative Software Architecture Development},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2014.26}}

@inproceedings{5231547,
	Abstract = {Agile Model Driven Architecture (MDA) software development processes apply agile principles in the context of executable models. In this paper we present an agile MDA approach for constructing, running, and testing executable UML service-oriented components. This work is part of a series referring to COMDEVALCO - a framework for Software Component Definition, Validation, and Composition.},
	Author = {I. {Lazar} and B. {Parv} and S. {Motogna} and I. -. {Czibula} and C. -. {Lazar}},
	Booktitle = {2008 First International Conference on Complexity and Intelligence of the Artificial and Natural Complex Systems. Medical Applications of the Complex Systems. Biomedical Computing},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:43:16 +0100},
	Doi = {10.1109/CANS.2008.12},
	Keywords = {program testing;program verification;software architecture;Unified Modeling Language;agile model driven architecture;software development process;software testing;unified modeling language;service-oriented component-based application;software validation;Unified modeling language;Containers;Integrated circuit modeling;Context modeling;Testing;Runtime;Availability;Logic;Java;Filters;agile MDA;executable UML;action language;component model;execution environment;component-based development;test-driven development},
	Pages = {38-44},
	Title = {An Agile MDA Approach for the Development of Service-Oriented Component-Based Applications},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/CANS.2008.12}}

@inproceedings{5231612,
	Abstract = {The application system of business must be fast enough to response the change from customer's request in current complex business environment. Business agility can ensure the enterprise survives in the explosive competition. In this paper, the author analyze the limitation and deficiency of the business agility in the traditional enterprise application integration, and propose a novel enterprise application integration and business agility scheme centered as business process management (BPM), based on service-oriented architecture (SOA). On the further discussion of SOA and BPMpsilas combining principle, the author proposed a new method that BPM agent acts as the connection of SOA and BPM. Through the coordination of undergoing process and service based on BPM agent, it reduced the influence causing by separating change of SOA and BPM in maximum, and finally achieve the business agility.},
	Author = {C. {Ling} and L. {Xin}},
	Booktitle = {2009 International Forum on Information Technology and Applications},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:44:16 +0100},
	Doi = {10.1109/IFITA.2009.54},
	Keywords = {business data processing;software architecture;enterprise application integration;business agility scheme;business process management;service-oriented architecture;Semiconductor optical amplifiers;Service oriented architecture;Application software;Information technology;Disaster management;Delay;Energy management;Computer science;Explosives;Data security;Service-Oriented Architecture;Business Process Management;BPM Agent;Business Agility},
	Pages = {334-337},
	Title = {Achieving Business Agility by Integrating SOA and BPM Technology},
	Volume = {1},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/IFITA.2009.54}}

@inproceedings{4483187,
	Abstract = {It seems that service oriented architecture (SOA) is to be this year's hot buzzword, rather than a well defined, meaningful and valuable part of the Enterprise Architecture landscape. Before the term fades away completely, perhaps we should agree what's valuable about the move to SOA and how to make the leap, and make the leap valuable. The SOA consortium is making great strides in defining SOA to be a valuable business strategy for business agility, in the context of Enterprise Architecture, Business Process Management and other concepts; and the Object Management Group (OMG) is making headway on modeling standards for services (as opposed to yet another set of standards for moving bits around wires). Dr. Soley will introduce the SOA Consortium and give some context for OMG's work in service modeling, with a focus on early successes in implementing the SOA business strategy leveraging modeling technologies like UML, BPMN and MOF.},
	Author = {R. {Soley} and A. {Watson}},
	Booktitle = {19th Australian Conference on Software Engineering (aswec 2008)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:41:15 +0100},
	Doi = {10.1109/ASWEC.2008.4483187},
	Keywords = {business data processing;object-oriented programming;software architecture;service oriented architecture;model driven architecture;enterprise architecture;business process management;object management group;Service oriented architecture;Computer architecture;Business communication;Application software;Software engineering;Conference management;Engineering management;Software standards;Context-aware services;Context modeling},
	Pages = {32-34},
	Title = {Service Oriented Architecture: Making the Leap, Leveraging Model Driven Architecture and Achieving Software Agility with BPM, SOA and MDA{\textregistered}},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASWEC.2008.4483187}}

@inproceedings{6398012,
	Abstract = {Reports about application of the Extreme Programming - XP method, have evidenced scalability problems in both, the team size and the problem complexity. When the problem is more complex and the team size is bigger, organization of team, project and product became to have more relevance. Architecture practices deal with these concerns, so these are key elements for achieving any scalable XP extension. In this paper we propose XA (XP with Architecture), an XP extension including adapted architecture practices from architecture centered approaches. Furthermore, the paper presents a study case where XA is applied in a project in an academic context.},
	Author = {L. F. {Munoz} and J. A. H. {Alegr{\'\i}a}},
	Booktitle = {2012 7th Colombian Computing Congress (CCC)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:51 +0100},
	Doi = {10.1109/ColombianCC.2012.6398012},
	Keywords = {software architecture;software prototyping;XA;XP extension;architecture practices;extreme programming;scalability problems;team size;problem complexity;team organization;project organization;product organization;XP-with-architecture;Software;Computer architecture;Instruments;Programming;Silicon compounds;Scalability;Complexity theory;Software Engineering;Software Process;Agile Methods},
	Pages = {1-6},
	Title = {XA: An XP extension for supporting architecture practices},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ColombianCC.2012.6398012}}

@inproceedings{6549984,
	Abstract = {System test development, automation and execution process are key stages of the overall product development to both the New Product Introduction (NPI) and Production Release processes. For NPI, companies must create test systems to support product validation and verification. For manufacturing companies, ongoing process metrics are used to ensure the product meets quality specifications and can be sold to customers. This entire test process is time consuming and resource intensive and therefore negatively impacts the overall product net revenue, both in terms of time to market and in terms of development resources. Large and successful companies invest hundreds of thousands of dollars in automated test systems to support product development. Such infrastructures provide a competitive advantage by enabling a systematic methodology to generate test plans and then automatically have the test plan flow through the test software and hardware development, test and data collection, and results analysis phases. The Automatic Testing Equipment (ATE) industry has pushed to develop a framework that supports the sharing of test information, data, and analysis results across various enterprise platforms. An IEEE standard know as Automatic Test Markup Language (ATML), comprising of an XML schema, was proposed and developed in order to allow interoperability of test case, data, equipment information, and results. Our methodology provides a Service-Oriented Architecture that provides an interoperable solution. Users can begin with a test plan, deploy a scalable data monitoring and analysis capability, and follow the process from NPI through production. Various additional capabilities such as advanced analysis capability, customer data sharing resources, test software generation and deployment, closed and open source software library access, test station monitoring and equipment tracking, automated reporting schedules, and others are among the possibilities that can be added to the overall process. The proposed architecture is entirely scalable and can be deployed in single-site or global applications and may be installed behind corporate firewalls or in the cloud.},
	Author = {M. {Weir} and R. {Kulak} and A. {Agarwal}},
	Booktitle = {2013 IEEE International Systems Conference (SysCon)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:52:39 +0100},
	Doi = {10.1109/SysCon.2013.6549984},
	Keywords = {agile manufacturing;automatic test equipment;manufacturing industries;open systems;product development;production engineering computing;public domain software;quality management;service-oriented architecture;software libraries;XML;service oriented architecture;agile automated testing environment;system test development;product development;new product introduction;NPI;production release process;product validation;product verification;manufacturing company;process metrics;quality specification;product net revenue;automated test system;hardware development;automatic testing equipment industry;ATE industry;IEEE standard;Automatic Test Markup Language;ATML;XML schema;data monitoring;advanced analysis capability;customer data sharing resource;test software generation;test software deployment;closed source software library access;open source software library access;test station monitoring;equipment tracking;automated reporting schedule;corporate firewall;cloud;Automation;Servers;Manufacturing;Standards;Testing;Companies;Monitoring;Agile Test;Service Oriented Architecture;Data Analysis System Testing},
	Pages = {853-860},
	Title = {Service Oriented Architecture for agile automated testing environment},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/SysCon.2013.6549984}}

@inproceedings{5261088,
	Abstract = {The use of contemporary software development approaches such as agile methods is growing in widespread use throughout the world. Although some universities are starting to teach them, courses on agile methods at the undergraduate and graduate levels are still a new phenomenon. The University of Maryland University College (UMUC) adapted agile methods for its capstone course towards a masterpsilas degree in software engineering in the Fall of 2008. Three distributed teams of five students were asked to use agile methods to build competing electronic commerce Web sites. With little training in agile methods, virtual teams, collaboration tools, or Web design, each of the three teams completed fully functional e-commerce Web sites using agile methods in little more than 13 weeks. Teams who struck an optimum balance of customer collaboration, use of agile methods, and technical programming ability had better productivity and Web site quality.},
	Author = {D. F. {Rico} and H. H. {Sayani}},
	Booktitle = {2009 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:45:18 +0100},
	Doi = {10.1109/AGILE.2009.13},
	Keywords = {computer science education;educational courses;electronic commerce;software engineering;teaching;Web design;agile method;software engineering education;contemporary software development approach;teaching;capstone course;electronic commerce Web site;virtual team;collaboration tool;Web design;Software engineering;Educational institutions;Programming profession;Computer industry;Service oriented architecture;Software standards;Military computing;Dynamic programming;Testing;Electronic commerce;Agile Methods;Software Engineering;Education;Software Development;Computer Programming;Scrum;Extreme Programming},
	Pages = {174-179},
	Title = {Use of Agile Methods in Software Engineering Education},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2009.13}}

@inproceedings{6223022,
	Abstract = {The use of agile methodologies in industry has increased significantly over the past decade, promoting the value of human-centric software development process. This growing use derives the need to adjust agile methodologies to bigger, more complex system development projects, where architecture plays a significant role. However, many believe that an essential conflict exists between the requirement of minimalism in agile methods and the need for well-defined and documented architecture in complex systems. This paper presents an exploratory study aimed at understanding the software architecture related activities as perceived by architects with and without experience in agile methodologies. The findings indicate that while architects practicing only plan-driven methodologies perceive architecture activities as being related only to the first phases of the development process, architects involved in agile projects perceive architecture activities to be related to most or all phases of the development lifecycle. The latter perceptions are consistent with suggestions expressed in the literature regarding architecture in general and in agile methodologies in particular. Based on these findings we suggest that agile methods not only lead architects to adjust their behavior to the agile philosophy, but also improve architects' perceptions and practice of architecture in general.},
	Author = {I. {Hadar} and S. {Sherman}},
	Booktitle = {2012 5th International Workshop on Co-operative and Human Aspects of Software Engineering (CHASE)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:59 +0100},
	Doi = {10.1109/CHASE.2012.6223022},
	Keywords = {software architecture;software prototyping;plan-driven perceptions;agile methodologies;software architecture;human-centric software development process;complex system development projects;agile projects perceive architecture activities;agile philosophy;Computer architecture;Software architecture;Conferences;Software;Testing;Documentation;Programming;software architecture;development methodologies;agile methodology;qualitative research},
	Pages = {50-55},
	Title = {Agile vs. plan-driven perceptions of software architecture},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/CHASE.2012.6223022}}

@inproceedings{6189529,
	Abstract = {This paper identifies the common misconceptions about agile engineering practice and proposes a methodology for applying agility concepts to the core systems engineering competency model as a means to counter the latent delay from initial Concept of Operations (CONOPS) development through operational fielding and maintenance. Modern systems engineering problems (specifically within the defense domain) are faced with an evolutionary operational deployment environment in which the problems the initial CONOPS sets out to address often evolve to areas that the system under design falls short of addressing, or misses altogether. As the operational mission for modern day systems is non-static, the engineering solutions must be sufficiently adaptive and evolutionary to meet these emerging challenges. The purpose of this paper is to stimulate debate and inspire future work to expand the agility body of knowledge within the systems engineering community in order to devise a framework for instilling agile practice throughout the systems engineering lifecycle.},
	Author = {S. B. {Schapiro} and M. H. {Henry}},
	Booktitle = {2012 IEEE International Systems Conference SysCon 2012},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:46 +0100},
	Doi = {10.1109/SysCon.2012.6189529},
	Keywords = {military computing;open systems;software architecture;software prototyping;systems engineering;engineering agile systems;architectural modularity;agility concepts;core systems engineering competency model;latent delay;concept-of-operations development;CONOPS;operational fielding;operational maintenance;evolutionary operational deployment environment;agile practice;systems engineering lifecycle;defense systems engineering community;modular open systems architecture;MOSA;Open systems;Computer architecture;Business;Technological innovation;Conferences;Industries;Adaptability;Agile Systems Engineering;Deferred Commitment;Evolutionary Systems;Modular Open Systems Architecture (MOSA);Red Teaming},
	Pages = {1-6},
	Title = {Engineering agile systems through architectural modularity},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/SysCon.2012.6189529}}

@inproceedings{6824127,
	Abstract = {Agile processes emphasize iterative delivery rather than assuming the definition of all detailed requirements and architecture up front. This "just enough" approach generally considers user stories and acceptance tests as sufficient documentation for successful system development. However, industry practices have shown that this minimalism is appropriate for projects with short duration and small collocated teams. In the development of large systems, the "just enough" documentation goes beyond the traditional set recommended by the Agile evangelists, due to the diversity of elements to be considered, as for instance geographic distribution of the teams, necessity to comply with industry regulations, strict IT governance programs, integration of the system being developed with others, or even the presence of not-so-agile people in the teams. In this context, a more complex set of artifacts is required to ensure the proper development of systems, such as more detailed requirements documents and architectural specification. In this regard, to support the agile development of large systems, we introduce TraceMan - Traceability Manager as a mechanism for ensuring traceability among user stories, traditional requirements documents, test specifications, architecture design, and source code. We also present an experience report on how TraceMan has been used in the daily activities at John Deere Intelligent Solutions Group (ISG) to support traceability among development artifacts.},
	Author = {P. O. {Antonino} and T. {Keuler} and N. {Germann} and B. {Cronauer}},
	Booktitle = {2014 23rd Australian Software Engineering Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:13 +0100},
	Doi = {10.1109/ASWEC.2014.30},
	Keywords = {formal specification;software architecture;software prototyping;architecture design;requirements specification;agile artifacts;architectural specification;large system agile development;TraceMan;traceability manager;Intelligent Solutions Group;ISG;Computer architecture;Documentation;Agricultural machinery;Software;Context;Industries;Testing;Agile Development;Traceability;Architecture;Requirements;Agile Artifacts},
	Pages = {220-229},
	Title = {A Non-invasive Approach to Trace Architecture Design, Requirements Specification and Agile Artifacts},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASWEC.2014.30}}

@inproceedings{6377890,
	Abstract = {Contemporary industrial automation struggle to cope with the continuous demand for high quality customized products at low cost at the same time that production lifecycle history proves that unexpected behavior arises, equipment fails and requirements are constantly adjusted. All this denotes a permanent need to reengineer in an fast and transparent manner in order to truly implement an agile factory. The present work includes a reference architecture and device model that tries to exploit Internet-based business models alongside Service-oriented Architecture principles and technology to assist the systems integrator during lifecycle reengineering interventions. All architecture elements are portrayed according to their roles and specificity being mutually interoperable to allow a customized mould according to actual system requirements, constraints and goals. The proposed device model follows above architecture guidelines and it supports the enhancement of the overall system ability in terms of agility and openness by laying over open web standards. It allows the transparent deployment of new services besides built-in services that support the discovery, identification, configuration and monitoring of a particular device.},
	Author = {G. {C{\^a}ndido} and J. {Barata} and A. W. {Colombo}},
	Booktitle = {2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:46 +0100},
	Doi = {10.1109/ICSMC.2012.6377890},
	Keywords = {agile manufacturing;formal specification;industrial engineering;product customisation;product life cycle management;production engineering computing;service-oriented architecture;device level service-oriented infrastructure;agile factory;industrial automation;continuous demand;high quality customized product;production lifecycle history;Internet-based business model;service-oriented architecture principle;system integration;lifecycle reengineering;architecture elements;customized mould;system requirements;system constraints;system goals;device model;architecture guidelines;system ability;open Web standards;service deployment;built-in service;device discovery;device identification;device configuration;device monitoring;Service oriented architecture;Automation;Computer architecture;Ontologies;Standards;Service-oriented Architecture;industrial automation;device model;reference architecture;reengineering agility},
	Pages = {1171-1176},
	Title = {Service-oriented infrastructure at device level to implement agile factories},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSMC.2012.6377890}}

@inproceedings{5608805,
	Abstract = {Information technology for e-governance system could yield great benefits and modernization of the different government sectors. The experience of e-governance system in a number of developed and developing countries has shown that information technology can provide greater service delivery with great quality. The purpose of this paper is to better understand how agile principles, specifically eXtreme Programming (XP) practices, can be effectively introduced and implemented into e-government system in developing countries that have historically embraced the plan-driven traditional software development environment.},
	Author = {S. {Roy} and M. {Kumar Debnath}},
	Booktitle = {2010 2nd International Conference on Software Technology and Engineering},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 14:03:37 +0100},
	Doi = {10.1109/ICSTE.2010.5608805},
	Keywords = {government data processing;software architecture;software prototyping;SOA;e-governance system;eXtreme Programming;XP;e-government system;software development environment;agile principle;Service oriented architecture;Government;Programming;Computer architecture;Software;eXtreme Programming;Agile principles;Software Engineering;Service-oriented architecture (SOA);E-governance system},
	Pages = {277-282},
	Title = {Designing SOA based e-governance system using eXtreme Programming methodology for developing countries},
	Volume = {2},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSTE.2010.5608805}}

@article{6871369,
	Abstract = {Emerging wireless standards specify dozens of bands spanning several octaves, which need to be supported in form-factor and energy constrained mobile devices targeting ubiquitous connectivity. However, in current multi-band radio implementations, significant redundancy is still the norm in the RF frontend. This work introduces an improved architecture for multi-band, time-division duplexed (TDD) radios, which replaces multiple narrowband frontend components with a frequency-agile solution, tunable over a wide frequency range. A highly digital architecture is adopted, leading to a fully integrated solution wherein both efficiency and achievable frequency range benefit from CMOS scaling. A prototype is integrated in 45 nm SOI CMOS. Peak PA output power is 27.7 $\pm$0.5 dBm from 1.3 to 3.3 GHz, with up to 30% total efficiency at 2 V. For TDD LTE applications, better than -30 dBc ACLR and -30 dB EVM is measured with 64 QAM, 20 MHz signals from 1.44 to 3.41 GHz, with up to 17.2% average efficiency and 23.4 dBm average power. The LNA achieves AV  14 dB, NF = 4.4 $\pm$1.6 dB and IIP 3  -7 dBm from 1.3 to 3.3 GHz while drawing just 6 mA from 1 V. The demonstrated frequency range covers a total of 11 TDD bands .},
	Author = {S. {Goswami} and H. {Kim} and J. L. {Dawson}},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:47 +0100},
	Doi = {10.1109/JSSC.2014.2339318},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {CMOS integrated circuits;Long Term Evolution;mobile handsets;quadrature amplitude modulation;time division multiplexing;size 45 nm;frequency 1.44 GHz to 3.41 GHz;frequency 1.3 GHz to 3.3 GHz;current 6 mA;voltage 11 V;frequency-agile RF frontend architecture;multiband TDD;wireless standards;energy constrained mobile devices;form-factor mobile devices;ubiquitous connectivity;multiband radio implementations;multiband time-division duplexed radios;narrowband frontend components;frequency-agile solution;digital architecture;CMOS scaling;SOI CMOS;TDD LTE;64 QAM;ACLR;EVM;TDD bands;Computer architecture;Radio frequency;CMOS integrated circuits;Impedance;Microprocessors;Power generation;Switches;4G wireless communication;low-noise amplifiers;power amplifiers;silicon on insulator technology;software radio;time division multiplexing;wireless LAN},
	Number = {10},
	Pages = {2127-2140},
	Title = {A Frequency-Agile RF Frontend Architecture for Multi-Band TDD Applications},
	Volume = {49},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2014.2339318}}

@inproceedings{5615620,
	Abstract = {Agile has been used to refer to a software development paradigm that emphasizes rapid and flexible development. In the meanwhile, we have through our practical experiences in scaling up agile methods, noticed that architecture plays an important role. Due to the inter-relationship between agile methods and architecture, as well as divergent perceptions on their correlation stated in numerous sources, we are motivated to find out how these perceptions are supported by findings in the research community in general and in empirical studies in particular. To fully benefit from agile practices and architectural disciplines, we need empirical data on the perceived and experienced impacts of introducing agile methods to existing software development process, as well as correlations between agile and architecture. In this paper, we survey the research literature for statements made regarding the relationship between agile development and software architecture. The main findings are that there is a lack of scientific support for many of the claims that are concerned with agile and architecture, and more empirical studies are needed to fully reveal the benefits and drawbacks implied by an agile software development method.},
	Author = {H. P. {Breivold} and D. {Sundmark} and P. {Wallin} and S. {Larsson}},
	Booktitle = {2010 Fifth International Conference on Software Engineering Advances},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:34 +0100},
	Doi = {10.1109/ICSEA.2010.12},
	Keywords = {software architecture;software prototyping;software development;rapid development;flexible development;agile development;software architecture;Computer architecture;Software architecture;Programming;Software;Data mining;Conferences;Agile;Agile methodology;Architecture},
	Pages = {32-37},
	Title = {What Does Research Say about Agile and Architecture?},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSEA.2010.12}}

@inproceedings{4594253,
	Abstract = {Pressed by the fierce competition in the auto electronic parts market, we have explored a new production management approach based on manufacture execution system (MES) out of actual needs of the auto electronic parts enterprises to meet the rapidly changing market demands. Technically supported by Web service, XML technology, B/S structure and Microsoft .NET framework, the approach adopts a three-tiered Web-based systematic framework, which is connected to automation equipment and to the management platform, thus achieving seamless data connection, information sharing , and data inquiry in no longer than one second. Manufacturing Execution Systems (MES) actualize quality control of work-in-process (WIP), statistics analysis, production scheduling and maintenance of equipment and instruments. It can also reduce costs and increase enterprisepsila ability to meet the ever changing market demands. This proposed systematic approach is expected to provide and effective way to the design of MES. Case studies show that enterprise installed MES systems have greatly improved their production.},
	Author = {{Zhihui Huang} and {Shulin Kan} and {Yufeng Wei} and {Qiaoying Dong} and {Jing Yuan}},
	Booktitle = {2008 7th World Congress on Intelligent Control and Automation},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:41:41 +0100},
	Doi = {10.1109/WCICA.2008.4594253},
	Keywords = {agile manufacturing;automobile industry;automotive components;automotive electronics;client-server systems;network operating systems;production management;quality control;scheduling;statistical analysis;Web services;work in progress;XML;agile manufacturing execution system;auto electronic parts industry;production management approach;Web service;XML technology;B/S structure;Microsoft .NET framework;three-tiered Web-based systematic framework;automation equipment;seamless data connection;information sharing;data inquiry;quality control;work-in-process;statistics analysis;production scheduling;equipment maintenance;instrument maintenance;Consumer electronics;Automation;Manufacturing;Production;XML;Agile manufacturing;Industries;Agile Manufacturing Execution Systems;architectural structure;Auto electronic parts manufacturer;case studies},
	Pages = {8435-8440},
	Title = {Research on agile manufacturing execution systems for the auto electronic parts industry},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/WCICA.2008.4594253}}

@inproceedings{1402330,
	Abstract = {Although previous studies have achieved a great deal on the local decision areas including inventory policies, vendor selection, production scheduling, etc., and the strategic problems, e.g. networks design, simulation and prediction of the supply chain as a whole, few attempts have been focused on the integrated solution for supply chain operational management. As the structure of multi-agent systems (MAS) inherently meets the requirements of autonomy and decentralization for supply chains, we propose a multi-agent-based model of planning, scheduling and execution that is capable of supporting the resource coordination between the self-interested agents through a combinatorial auction mechanism. And a general framework of ASCMS (agile supply chain management system) is presented as the background. To facilitate the functions of this system, the components of various agents and the negotiation process are also discussed in this paper.},
	Author = {{Lun Ta} and {Yueting Chai} and {Yi Liu}},
	Booktitle = {2005 IEEE International Conference on e-Technology, e-Commerce and e-Service},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:36:51 +0100},
	Doi = {10.1109/EEE.2005.92},
	Keywords = {multi-agent systems;supply chain management;strategic planning;scheduling;agile manufacturing;multiagent systems;resource coordination;combinatorial auction mechanism;agile supply chain operational management system;Supply chains;Supply chain management;Computer architecture;Manufacturing automation;Production;Job shop scheduling;Multiagent systems;Agile manufacturing;Engineering management;Computer integrated manufacturing},
	Pages = {400-403},
	Title = {Multi-agent-based architecture and mechanism for coordination and execution in agile supply chain operational management},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/EEE.2005.92}}

@inproceedings{6005510,
	Abstract = {The technique of vertical slicing was introduced as a mechanism to combat our agile software development teams developing tendency toward building software in horizontal layers, building one service at a time, or grouping stories then breaking them down into layers. Those practices resulted in sprint reviews where no functionality could be demonstrated to the users and several iterations before all of the pieces were working together with useful functionality to show to users. Those habits continued when the next wave of projects with user interfaces came along. In this paper, we highlight the stories of four of our agile teams who tried vertical slicing, the challenges they faced, the victories, and the lessons learned.},
	Author = {I. M. {Ratner} and J. {Harvey}},
	Booktitle = {2011 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:14 +0100},
	Doi = {10.1109/AGILE.2011.46},
	Keywords = {program slicing;software prototyping;user interfaces;vertical slicing;agile software development;user interfaces;Business;Buildings;Production;Data models;User interfaces;Media;Service oriented architecture},
	Pages = {240-245},
	Title = {Vertical Slicing: Smaller is Better},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2011.46}}

@inproceedings{6726586,
	Abstract = {In the last decades the complexity of software development projects had a significant increase. This complexity emerges from the higher degree of sophistication in the contexts they aim to serve and from the evolution of the functionalities implemented by the applications. The Service Oriented Architecture (SOA), a document review investigation has taken place with the objective to compare SOA development with two highly used agile software development methods: Extreme Programming (XP) and Relational Unified Process (RUP). The motor behind this investigation is to see if this agile development methods are capable enough to design effectively and efficiently SOA type applications. The investigation concluded that both methodologies are not exactly suited to create SOA applications, but RUP has a greater chance to provide SOA type applications if the proper adjustments are made, specially on the design side of the methodology.},
	Author = {G. S. {Rao} and C. V. P. {Krishna} and K. R. {Rao}},
	Booktitle = {2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:53:46 +0100},
	Doi = {10.1109/ICCCNT.2013.6726586},
	Keywords = {project management;service-oriented architecture;software prototyping;rational unified process;service oriented application;extreme programming;software development project complexity;service oriented architecture;document review investigation;SOA development;agile software development methods;XP;relational unified process;RUP;Service-oriented architecture;Semiconductor optical amplifiers;Business;Documentation;Programming;Computer architecture;Service Oriented Architecture;Software Development Methods;Extreme Programming;Relational Unified Process (RUP)},
	Pages = {1-6},
	Title = {Rational unified process for service oriented application in extreme programming},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCCNT.2013.6726586}}

@inproceedings{5513837,
	Abstract = {Along with the growing software market in recent years, business requirements are changing more rapidly and the complexity of enterprise applications is growing continuously. An ideal goal is to encapsulate these requirements into a high-level abstraction, which can be used to drive large-scale adaptation of the underlying software implementation. Model Driven Engineering (MDE) is one of the enabling techniques that support this objective. One of the important parts in this process is the creation of Platform Independent Model (PIM). There are several approaches on this matter. However there is still the absence of clear documentation on the methods especially when the purpose is aimed for generic PIMs in a specific domain. This article introduces an approach to developing PIMs by using UML, OCL and promotes agility by applying agile modeling for this purpose. A practical example will be demonstrated.},
	Author = {V. C. {Nguyen} and X. {Qafmolla}},
	Booktitle = {2010 Third International Conference on Information and Computing},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:46:36 +0100},
	Doi = {10.1109/ICIC.2010.180},
	Keywords = {software architecture;Unified Modeling Language;agile development;platform independent model;model driven architecture;business requirements;enterprise applications;high-level abstraction;model driven engineering;Unified Modeling Language;Object Constraint Language;Model driven engineering;Unified modeling language;Computer science;Process design;Weaving;Computer architecture;Application software;Large-scale systems;Documentation;User interfaces;Agile modeling;MDD;OCL;PIM;UML},
	Pages = {344-347},
	Title = {Agile Development of Platform Independent Model in Model Driven Architecture},
	Volume = {2},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIC.2010.180}}

@inproceedings{5172615,
	Abstract = {In this work we present a systematic approach for the creation of new variant software components via customization of existing core assets of a software product line. We consider both functional and quality variants and address the issue of a controlled creation of variants which considers the reference architecture and its co-evolution with a number of other artifacts including components and functional and quality test suites. Furthermore we discuss the relationship between the popular agile practice of test-driven development (TDD) and how it can be used to assist the evolution of software components of a software product line.},
	Author = {G. {Kakarontzas} and I. {Stamelos} and P. {Katsaros}},
	Booktitle = {2008 International Conference on Computational Intelligence for Modelling Control Automation},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:43:22 +0100},
	Doi = {10.1109/CIMCA.2008.84},
	Keywords = {object-oriented programming;product development;program testing;software quality;software reusability;product line variability;elastic components;test-driven development;variant software components;software product line;core assets;reference architecture;customization;Informatics;Software testing;Computer architecture;Programming;Telecommunications;Software engineering;System testing;Documentation;Product development;Software Product Lines;Component Variability;Elastic Software Components;Test-Driven Development},
	Pages = {146-151},
	Title = {Product Line Variability with Elastic Components and Test-Driven Development},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/CIMCA.2008.84}}

@inproceedings{6759192,
	Abstract = {Agile software development pursues to deal with continuous change. But software product architectures without enough flexibility can restrict how products cope with change. However, designing for flexibility often entails high costs and risk that comes with the assumption that change will happen. Actually, in software architecture the flexibility investment decision making problem has become challenging. This paper presents a process to assist architects in Making decisions about Flexibility investment in Software Architecture (MAKE Flexi). MAKE Flexi is based on technical debt and real options approaches. Technical debt allows for estimating the additional cost derived from the lack of flexibility in software architectures, whereas the real options valuation allows for estimating the value of the flexibility that a design option could provide. MAKE Flexi has been applied to an industry project for smart grids to assist architects in making decisions about designing for flexibility to vary data storage technologies.},
	Author = {C. {Fern{\'a}ndez-S{\'a}nchez} and J. {D{\'\i}az} and J. {P{\'e}rez} and J. {Garbajosa}},
	Booktitle = {2014 47th Hawaii International Conference on System Sciences},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:01 +0100},
	Doi = {10.1109/HICSS.2014.590},
	Keywords = {decision making;software architecture;software prototyping;storage management;guiding flexibility investment;agile architecting;agile software development;software product architectures;decision making;MAKE Flexi;data storage technologies;Investment;Software;Computer architecture;Software architecture;Decision trees;Cost accounting;Estimation;agile;architecture;technical debt;real options},
	Pages = {4807-4816},
	Title = {Guiding Flexibility Investment in Agile Architecting},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/HICSS.2014.590}}

@inproceedings{4700491,
	Abstract = {Summary form only given. In many decades, many organizations, especially large consulting companies, have been designing, implementing and managing business solutions for every industry around the globe. But due to numerous limitations in process, tooling and skills, most of those solutions were made very specific to individual industry and client needs at its early design stage. Therefore, reuse and more importantly, managing the ever changing business requirements, become almost impossible. Service-orientation and architecture, model-driven business development provides us a new and powerful approach to facilitate asset based industry solution design and development. To further accelerate this, this tutorial discusses about an innovative approach that take advantage of many proven best software engineering practices, from object/component based technology, meta-data driven architecture types (archetypes) that are used to model the common structural and in some cases non-structural business entities such as customer, product, payment, etc. In order to address the consequences introduced by abstracting those common elements out of the specific industry model and be able to enable easy and meta-data based transformation, we properly decompose business components/services into a multi-layered business architecture. Therefore, process/components/services can be decomposed accordingly to facilitate the decomposition and abstraction, while maintaining certain level of necessary traceability across various artifacts. In the realization phase, existing assets/operational systems will be mapped and transformed to the required business components and services to best leverage those existing valuable industry/client investments. To support such a SOA based, model and business driven development process, existing tooling, especially the necessary transformation and integration capability, needs to be significantly enhanced. This tutorial will also present some recommendation based on some recent design and implementation, and they could be used to guide future tooling alignment and integration effort across software modeling, implementation and solution products. In addition, we will present how to leverage existing internal or external assets or product offerings and the open industry reference models and standards (such as ACCORD, ebXML, ARTS/IxRetail). This work is based on authors' collective experience in leading the large end-to-end client engagements across many industries, while promoting various industry leading software engineering best practices.},
	Author = {M. {Luo}},
	Booktitle = {2008 IEEE Congress on Services Part II (services-2 2008)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:56 +0100},
	Doi = {10.1109/SERVICES-2.2008.53},
	Keywords = {agile manufacturing;flexible manufacturing systems;innovation management;object-oriented programming;product design;product development;software architecture;Web services;business component decomposition;agile industrial solution;flexible industrial solution;business solution management;service-oriented architecture;SOA;model-driven business development;innovative approach;software engineering practice;object-component based technology;meta-data driven architecture;software modeling;open industry reference model;product design;product development;multi layered business architecture;Computer architecture;Software engineering;Computer industry;Companies;Acceleration;Investments;Service oriented architecture;Software tools;Subspace constraints;Best practices},
	Pages = {11-12},
	Title = {Common Business Components and Services toward More Agile and Flexible Industry Solutions and Assets},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/SERVICES-2.2008.53}}

@inproceedings{6298089,
	Abstract = {As its second guiding principle, agile software development promotes working software over comprehensive documentation. In this paper we investigate alignment between two different documentation practices and agile development. We report upon an experiment conducted to explore the impact of formalism and media type on various dimensions of documentation practice in agile teams. 28 students in 8 teams were divided into two groups: SAD and UML. Group SAD was to update and deliver their high-level software architecture in form of a textual description defined by RUP templates. Group UML was instructed to update and deliver their low-level software design in form of UML models. Our results show that iterative documentation practices led to more extensive and more detailed textual documentation. We found that writing documentation was perceived as a intrusive task leading to task specialization and allocation of documentation to less qualified team members. Consequently, this hampered collaboration within the team. Based in our findings, we suggest that if documentation is to be delivered with the project, producing documentation should be communicated and accepted by the team as a proper product. Furthermore, we argue that codification of internal development knowledge should be a non-intrusive task.},
	Author = {C. J. {Stettina} and W. {Heijstek} and T. E. {F{\ae}gri}},
	Booktitle = {2012 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:51:22 +0100},
	Doi = {10.1109/Agile.2012.7},
	Keywords = {software architecture;software prototyping;sustainable development;system documentation;Unified Modeling Language;documentation work;agile teams;documentation formalism role;sustainable practice;agile software development;iterative documentation practices;SAD group;UML group;high-level software architecture;textual description;RUP templates;low-level software design;textual documentation;task specialization;documentation allocation;Documentation;Unified modeling language;Encoding;Teamwork;Software architecture;Software design;agile teams;software development;knowledge sharing;project management;organizational management and coordination;process improvement},
	Pages = {31-40},
	Title = {Documentation Work in Agile Teams: The Role of Documentation Formalism in Achieving a Sustainable Practice},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/Agile.2012.7}}

@inproceedings{4293576,
	Abstract = {Design is an inherently multidisciplinary endeavor. This raises the question of how to develop systems in ways that can best leverage the perspectives, practices, and knowledge bases of these different areas. Agile software development and usability engineering both address important aspects of system design, but there are tensions between the methods that make them difficult to integrate. This work presents a development approach that draws from extreme programming (XP), a widely practiced agile software development process, and scenario-based design (SBD), an established usability engineering process. It describes three key questions that need to be addressed for agile software development methods and usability engineering practices to work together effectively, and it introduces interface architectures and design representations that can address these questions.},
	Author = {J. C. {Lee} and D. S. {McCrickard}},
	Booktitle = {Agile 2007 (AGILE 2007)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:39:39 +0100},
	Doi = {10.1109/AGILE.2007.63},
	Keywords = {software engineering;usable software;agile software development;usability engineering;extreme programming;scenario-based design;interface architectures;design representations;Usability;Programming;Design engineering;Process design;Computer architecture;Software systems;Computer science;Best practices;Surges;Probes},
	Pages = {59-71},
	Title = {Towards Extreme(ly) Usable Software: Exploring Tensions Between Usability and Agile Software Development},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2007.63}}

@inproceedings{4783558,
	Abstract = {This paper describes a novel architectural design model for implementing enterprise systems with the aid of service oriented architecture (SOA). It addresses the organizational challenges in enabling cross communication and collaboration without changing the current ICT infrastructure. Organizations in today's market require a dynamic system that synchronizes not only their supply chain, but also their business processes. The proposed novel approach represents an agile and flexible technology framework based on Service oriented architecture (SOA). The authors demonstrate the utility of the approach by implementing a solution to integrate the customer relationship Manager (CRM) and stock control system.},
	Author = {M. R. {Majedi} and K. A. {Osman}},
	Booktitle = {2008 Third International Conference on Pervasive Computing and Applications},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:43:02 +0100},
	Doi = {10.1109/ICPCA.2008.4783558},
	Keywords = {customer relationship management;enterprise resource planning;software architecture;stock control;supply chain management;novel architectural design model;enterprise systems;enterprise resource planning system;enterprise application integration;service oriented architecture;supply chain;customer relationship manager;stock control system;Enterprise resource planning;Service oriented architecture;Object oriented modeling;Application software;Collaboration;Supply chains;Customer relationship management;Globalization;Business communication;Humans;Enterprise Resource Planning;Enterprise Application Integration;Service Oriented Architecture;Enterprise Systems},
	Pages = {116-121},
	Title = {A Novel Architectural Design Model for Enterprise Systems: Evaluating Enterprise Resource Planning System and Enterprise Application Integration Against Service Oriented Architecture},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICPCA.2008.4783558}}

@inproceedings{6043935,
	Abstract = {This paper addresses the challenges of large commercial and government that struggle with requirements understanding, increasing delivery quality and estimating costs. The paper discusses Smarter Architecture & Engineering (SmarterAE) as an approach that applies scrutiny, metrics and analysis to architecture, requirements management and engineering as is historically given to development, testing and operations. SmarterAE facilitates business agility by capturing the essence of the enterprise business into an actionable business architecture. It treats IT architecture, requirements, engineering as key business processes and accelerates acceptance among users, business stakeholders, development, testing and operations. The approach is based on Best Practices and encourages asset reuse and management.},
	Author = {P. {Bahrs} and T. {Nguyen}},
	Booktitle = {2011 Workshop on Requirements Engineering for Systems, Services and Systems-of-Systems},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:49:58 +0100},
	Doi = {10.1109/RESS.2011.6043935},
	Keywords = {business data processing;formal specification;formal verification;game theory;software architecture;software cost estimation;systems analysis;game changer;requirement management;delivery quality;cost estimation;requirement engineering;business agility;enterprise business architecture;IT architecture;business process;business stakeholder;Computer architecture;Semiconductor optical amplifiers;Measurement;Service oriented architecture;Standards organizations;Organizations;Architecture;Systems Engineering;Business Agility;SOA;Asset Management;Requirements Management;Impact Analysis;Traceability},
	Pages = {1-5},
	Title = {Smarter Architecture amp; Engineering: Game changer for requirements management: A position paper},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/RESS.2011.6043935}}

@inproceedings{5630310,
	Abstract = {A methodology for detailed design hybrid architecture of intelligent service mobile robot (ISMR) is presented in this paper. Aiming at ISMR, the conceptual, logical, physical and hierarchical design model are discussed step by step in-depth from system analysis and agile manufacturing (AM) point of view. These analysis methods of down-up for designing ISMR provide new thought for building ISMR efficiently in engineering practice. Moreover, the detailed design processes based on this strategy are provided by a case study. The results have shown its effectiveness and feasibility.},
	Author = {X. {Wei} and M. {Jiachen} and Y. {Mingli} and Z. {Yaowen}},
	Booktitle = {2010 International Conference on Electrical and Control Engineering},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:46:47 +0100},
	Doi = {10.1109/iCECE.2010.189},
	Keywords = {agile manufacturing;intelligent robots;mobile robots;service robots;hybrid architecture;intelligent service mobile robot;ISMR;agile manufacturing;Computer architecture;Service robots;Mobile robots;Architecture;Analytical models;Computers;ISMR;Hybrid architecture;Design model;System analysis;Agile manufacturing},
	Pages = {740-743},
	Title = {Design of Hybrid Architecture for Intelligent Service Mobile Robot},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/iCECE.2010.189}}

@inproceedings{6614717,
	Abstract = {This experience report builds on an earlier study in which we interviewed eight project teams that were using iterative incremental lifecycles. In the study, we captured the practices the teams felt contributed to rapid delivery. We identified a mix of Agile and architecture practices that teams apply to rapidly field software and minimize disruption and delay. In this paper, we elaborate one practice from the study, prototyping with quality attribute focus. We compared two experiences in prototyping focused on quality attribute considerations applied on Scrum projects. We observe through interviews that feature development and prototyping practice spans multiple levels: feature development/sprint, release planning, and portfolio planning. We also observe other factors including rapid trade-off analysis, flexible architecture, and adoption of a set of enabling prototyping guidelines. The analysis of the observations sheds light on several aspects of the practice that enable the team to respond quickly and efficiently when prototype feedback suggests architectural change.},
	Author = {S. {Bellomo} and R. L. {Nord} and I. {Ozkaya}},
	Booktitle = {2013 2nd International Workshop on the Twin Peaks of Requirements and Architecture (TwinPeaks)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:52:52 +0100},
	Doi = {10.1109/TwinPeaks.2013.6614717},
	Keywords = {project management;software architecture;software development management;software prototyping;software quality;prototype feedback;prototyping guidelines;flexible architecture;rapid trade-off analysis;release planning;portfolio planning;prototyping practice spans;feature development;Scrum projects;prototyping with quality attribute focus;architecture practices;agile practices;iterative incremental lifecycles;project teams;requirement practice;integrated architecture;Prototypes;Computer architecture;Planning;Guidelines;Interviews;Software;Delays;agile software development;architecture;quality attribute;prototyping;release planning;requirements;software development practices;architecture trade-off},
	Pages = {8-13},
	Title = {Elaboration on an integrated architecture and requirement practice: Prototyping with quality attribute focus},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/TwinPeaks.2013.6614717}}

@inproceedings{6827119,
	Abstract = {The proponents of Agile software development approaches claim that software architecture emerges from continuous small refactoring, hence, there is not much value in spending upfront effort on architecture related issues. Based on a large-scale empirical study involving 102 practitioners who had worked with agile and architecture approaches, we have found that whether or not architecture emerges through continuous refactoring depends upon several contextual factors. Our study has identified 20 factors that have been categorized into four elements: project, team, practices, and organization. These empirically identified contextual factors are expected to help practitioners to make informed decisions about their architecture practices in agile software development.},
	Author = {L. {Chen} and M. A. {Babar}},
	Booktitle = {2014 IEEE/IFIP Conference on Software Architecture},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:38 +0100},
	Doi = {10.1109/WICSA.2014.45},
	Keywords = {software architecture;software maintenance;software prototyping;evidence-based understanding;agile software development;continuous small refactoring;software architecture practices;contextual factors;Computer architecture;Software;Software architecture;Organizations;Data analysis;Encoding;software architecture;agile software development component;empirical study},
	Pages = {195-204},
	Title = {Towards an Evidence-Based Understanding of Emergence of Architecture through Continuous Refactoring in Agile Software Development},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2014.45}}

@inproceedings{5606420,
	Abstract = {The size and complexity of software systems is constantly and abruptly increasing, as well as the size of the teams who develop them. Although software systems usually start with a clean design and an unitary architecture, preserving the design quality in the final product, especially its modularity and reusability, depends on the programmers' ability to understand, implement and maintain the initial architecture of the system; in other words, it depends on the ability to preserve a common vision about the high-level design i.e., to preserve the architectural integrity. This is an essential problem, and it has led to a number of approaches that help maintaining architectural integrity by allowing for the specification and checking of architectural rules (constraints) in code. Unfortunately, these approaches are rarely used in practice because of their excessive complexity, lack of flexibility and absence of integration with the actual development environment. In this paper we propose a new, agile approach to defining and checking architectural constraints. The proposed solution consists, on one hand, of the inCode.Rules language that offers a highly intuitive, yet flexible, means for defining architectural rules. On the other hand, inCode.Rules is far more than a language specification: in the paper we show how architectural rules can be automatically checked using the inCode.Rules interpreter; and how they can be easily edited using the full-fledged editor that we created. Both the interpreter and the editor are tightly integrated in the Eclipse IDE. Furthermore, inCode.Rules has grown beyond being just a prototype as it has been already successfully applied on large-scale systems of over 1 MLOC. Thus, inCode.Rules provides a more agile approach to architecture verification, as it brings the evolution of code and architecture closer than ever before.},
	Author = {R. {Marinescu} and G. {Ganea}},
	Booktitle = {Proceedings of the 2010 IEEE 6th International Conference on Intelligent Computer Communication and Processing},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:17 +0100},
	Doi = {10.1109/ICCP.2010.5606420},
	Keywords = {formal specification;formal verification;software architecture;software metrics;specification languages;architectural constraints;software systems;unitary architecture;design quality;high-level design;architectural integrity;architectural rules;development environment;inCode.Rules language;language specification;inCode.Rules interpreter;Eclipse IDE;large-scale systems;architecture verification;Computer architecture;Software systems;Java;Complexity theory;Quality assessment;Containers;quality assessment;architectural integrity;Eclipse;software engineering},
	Pages = {305-312},
	Title = {inCode.Rules: An agile approach for defining and checking architectural constraints},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICCP.2010.5606420}}

@inproceedings{6037635,
	Abstract = {Enterprise architecture (EA) management provides an engineering approach for the continuous advancement of the enterprise as a whole. The high number of involved components and their dense web of interdependencies nevertheless form a major challenge for such approach and demand high initial investment into documentations, communications, and analysis. Aforementioned fact has in the past been an impediment for successful EA management in practice. In the field of software engineering recently lightweight and agile methods have become more and more important. These methods aim at quickly creating results, while staying flexible in respect to the design goals to attain. In this article we explore to which extent the de-facto standard for agile methods, namely Scrum, can be applied to EA management. Thereby, we derive challenges for an agile EA management approach and revisit current approaches regarding their agility. Finally, we outline how agile EA management can be implemented based on the method of Scrum.},
	Author = {S. {Buckl} and F. {Matthes} and I. {Monahov} and S. {Roth} and C. {Schulz} and C. M. {Schweda}},
	Booktitle = {2011 IEEE 15th International Enterprise Distributed Object Computing Conference Workshops},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:04 +0100},
	Doi = {10.1109/EDOCW.2011.33},
	Keywords = {business data processing;software engineering;enterprise architecture management function;agile design;software engineering;Scrum method;Computer architecture;Software;Programming;Decision making;Terminology;Iterative methods;enterprise architecture;EA management;agile methods;scrum},
	Pages = {322-329},
	Title = {Towards an Agile Design of the Enterprise Architecture Management Function},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/EDOCW.2011.33}}

@inproceedings{5169543,
	Abstract = {Agile manufacturing (AM) is a typical concurrent and collaborative way between different enterprises and different departments in order to meet market requirement for time, cost and quality as soon as possible. From the point of view, AM is special project management. Agile manufacturing and project management (PM) have many same characteristics. The characteristics and relations of AM and PM are discussed firstly. The resources of the manufacturing enterprise alliances have characteristics such as autonomous, distributed, heterogeneous, dynamic, and so on. Therefore, the PM of AM is different from traditional PM. Secondly, a typical workflow of agile manufacturing is illustrated. Then the architecture of PM of agile manufacturing is proposed and analyzed, which includes some key technologies to support good project management. Combining the automation and standards of the workflow with the PM of high quality is a good solution to agile manufacturing. Finally, the technologies of information exchange and communication mechanism are introduced.},
	Author = {J. {Sui} and L. {He} and H. {Yu}},
	Booktitle = {2009 Second International Conference on Information and Computing Science},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:44:34 +0100},
	Doi = {10.1109/ICIC.2009.26},
	Keywords = {agile manufacturing;project management;workflow management software;project management;workflow analysis;agile manufacturing;manufacturing enterprise alliances;information exchange;communication mechanism;workflow management system;Project management;Agile manufacturing;Manufacturing automation;Manufacturing processes;Optimal scheduling;Computer aided manufacturing;Educational institutions;Workflow management software;Isolation technology;Computer networks;Agile manufacturing;project management;workflow;communication mechanism},
	Pages = {75-78},
	Title = {Project Management and Workflow Analysis of Agile Manufacturing},
	Volume = {1},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICIC.2009.26}}

@inproceedings{4599538,
	Abstract = {SCRUM poses key challenges for usability (Baxter et al., 2008). First, product goals are set without an adequate study of the userpsilas needs and context. The user stories selected may not be good enough from the usability perspective. Second, user stories of usability import may not be prioritized high enough. Third, given the fact that a product owner thinks in terms of the minimal marketable set of features in a just-in-time process, it is difficult for the development team to get a holistic view of the desired product or features. This experience report proposes U-SCRUM as a variant of the SCRUM methodology. Unlike typical SCRUM, where at best a team member is responsible for usability, U-SCRUM is based on our experience with having two product owners, one focused on usability and the other on the more conventional functions. Our preliminary result is that U-SCRUM yields improved usability than SCRUM.},
	Author = {M. {Singh}},
	Booktitle = {Agile 2008 Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:50 +0100},
	Doi = {10.1109/Agile.2008.33},
	Keywords = {software engineering;agile methodology;product goals;user stories;usability perspective;usability import;just-in-time process;Usability;Business;Marketing and sales;Organizations;Software;Lead;Software architecture;usability;user experience;product owner;roles;two;dual;process},
	Pages = {555-560},
	Title = {U-SCRUM: An Agile Methodology for Promoting Usability},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/Agile.2008.33}}

@inproceedings{6643456,
	Abstract = {Grid Manufacturing (GM) is a new production paradigm, based upon the use of standardized and modular Reconfigurable Manufacturing Systems (RMS). In GM all systems have a virtual counterpart that actsautonomously, this includes both complete manufacturing systems and the products. The control system required for this approach is based upon a distributed and hybrid architecture, using agent technology. An important aspect in the paradigm is the product manufacturing description. This paper introduces the concept of an architecture where the control of the manufacturing is abstracted from the product manufacturing blueprint. A product is delineated step by step by specific services in the grid. The proposed system increases flexibility twofold, first by enabling abstraction of product's parts and second by dynamically using manufacturing means.},
	Author = {D. {Telgen} and L. {van Moergestel} and E. {Puik} and A. {van Zanten} and A. {Abdulamir} and J. {Meyer}},
	Booktitle = {2013 IEEE International Symposium on Assembly and Manufacturing (ISAM)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:53:28 +0100},
	Doi = {10.1109/ISAM.2013.6643456},
	Keywords = {agile manufacturing;control systems;flexible manufacturing systems;grid computing;virtual reality;Agile product manufacturing;grid manufacturing;production paradigm;reconfigurable manufacturing systems;RMS;virtual counterpart;control system;hybrid architecture;agent technology;Hardware;Manufacturing processes;Agile manufacturing;Industries;Agile Manufacturing;Distributed Systems;Agent Technology;Reconfigurable Manufacturing Systems;Multi Agent Systems},
	Pages = {282-284},
	Title = {Agile product manufacturing by dynamically generating control instructions},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISAM.2013.6643456}}

@inproceedings{7056955,
	Abstract = {The Rational Unified Process is a complete software-development process framework that comes with several out-of-the-box instances. The process of developing of new software product versions has speed up rapidly. The necessity of flexible and particularly prompt responses to the changes triggered off genesis of new technologies, software architectures and methodologies. The Service Oriented Architecture (SBA) allows defining services operation environment, web services then form one of the available technologies for SBA realization. The software development limits the possibilities of adapting the development process to the changes and up-to-date requests. This problem might be solved by adoption of Extreme Programming (XP).},
	Author = {G. {Sivanageswara Rao} and C. V. {Phani Krishna} and K. {Rajasekhar Rao}},
	Booktitle = {2014 Conference on IT in Business, Industry and Government (CSIBIG)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:07 +0100},
	Doi = {10.1109/CSIBIG.2014.7056955},
	Keywords = {service-oriented architecture;software prototyping;software quality;Web services;extreme programming;service-based application development architecture;rational unified process;software-development process framework;out-of-the-box instances;software product versions;software architectures;service oriented architecture;SBA;Web services;services operation environment;Educational institutions;Servers;Testing},
	Pages = {1-4},
	Title = {Extreme Programming for service-based application development architecture},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/CSIBIG.2014.7056955}}

@inproceedings{5998992,
	Abstract = {In order to improve the efficiency of management and to meet the demand of automation management on dormitory at university, a university dormitory management system based on agile development architecture is built, which adopts Brower/Server mode and the idea of agile development, and makes full use of agile development architecture technologies and related technologies, including agile architecture, automatic code generation technology and asp.net technology.},
	Author = {X. {Zhang} and Y. {Hu} and Y. {Lu} and J. {Gu}},
	Booktitle = {2011 International Conference on Management and Service Science},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:50:09 +0100},
	Doi = {10.1109/ICMSS.2011.5998992},
	Keywords = {educational administrative data processing;program compilers;software architecture;software prototyping;university dormitory management system;automation management;agile development architecture technology;agile architecture;automatic code generation technology;asp.net technology;browser/server mode;Databases;Computer architecture;Business;Software;Servers;XML;Generators},
	Pages = {1-4},
	Title = {University Dormitory Management System Based on Agile Development Architecture},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMSS.2011.5998992}}

@inproceedings{5636563,
	Abstract = {In the past five years, the IT community has witnessed the adoption of two major trends that had significant ramifications on how software solutions are developed and deployed. The first trend is focused on the software development side and is manifested through the popular use and adoption of software services and service oriented architecture (SOA). The second trend provides support to SOA and changes many views on how software is deployed and developed as well. Both trends are here to stay and pose many challenges for both business and IT on how to leverage them to enhance the overall productivity, agility and efficiency of the overall organization. More specifically, how to realize business agility requirements through the potential synergies between SOA and cloud computing.},
	Author = {M. {Hirzalla}},
	Booktitle = {2010 18th IEEE International Requirements Engineering Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:40 +0100},
	Doi = {10.1109/RE.2010.70},
	Keywords = {business data processing;Internet;software architecture;business agility requirements;SOA;cloud computing;IT community;significant ramifications;software solutions;software development;service oriented architecture;Service oriented architecture;Software;Cloud computing;Organizations;Clouds;SOA;Cloud Computing;Utility Computing;IaaS;PaaS;Busiess Agility},
	Pages = {379-380},
	Title = {Realizing Business Agility Requirements through SOA and Cloud Computing},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/RE.2010.70}}

@inproceedings{4493779,
	Abstract = {The object of The MASAM methodology is providing the process for developing the application SW operated on mobile platform. The mobile application SW has closely connected to the communications environment and has the feature in which there is the immediate reaction of a user while being operated. Moreover, it is important to positively accommodate the demand of a user and rapidly launch a product than the complicated development technology. This characteristic requires agile approach on which The MASAM is based.},
	Author = {Y. {Jeong} and J. {Lee} and G. {Shin}},
	Booktitle = {2008 10th International Conference on Advanced Communication Technology},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:40:34 +0100},
	Doi = {10.1109/ICACT.2008.4493779},
	Keywords = {graphical user interfaces;mobile computing;software architecture;software reusability;mobile application SW development process;agile methodology;reuse architecture;GUI-based architecture;Computer architecture;Software prototyping;Prototypes;Assembly;Design engineering;Application software;Mobile communication;Programming;Books;Software systems;Agile;SPEM;Architecture;Prototype},
	Pages = {362-366},
	Title = {Development Process of Mobile Application SW Based on Agile Methodology},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICACT.2008.4493779}}

@inproceedings{4606187,
	Abstract = {In the current e-commerce applications, the existence of a large number of complex issues is mainly due to the complexity of e-commerce applications' own problem domain and the complication caused by building them on Web. In addition, constant changes in business environment have increased the difficulty of software development. Many styles such as heavy SOA style, the workflow-oriented architectural style, and original rest style are created to solve these problems. Unfortunately, the previous studies did not take into account all aspects, and made a number of assumptions excluding the realistic complexity and mutability to facilitate research. This paper attempts to find out the architectural constraints contained in the architectural styles by analyzing the styles mentioned above and to design a set of architectural constraints, which can induce desirable architectural properties for e-commerce applications - e-commerce application architectural style based on the integration of workflow and agile service. In this paper, we evaluate the effectiveness of this architectural style by theoretical derivation and implementation experience.},
	Author = {X. {Wan} and L. {Huang}},
	Booktitle = {2008 International Symposium on Electronic Commerce and Security},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:37 +0100},
	Doi = {10.1109/ISECS.2008.78},
	Keywords = {electronic commerce;Internet;software architecture;e-commerce application architecture;workflow integration;agile service;Web;business environment;software development;heavy SOA style;workflow-oriented architectural style;Business;Service oriented architecture;Computer architecture;Web services;Complexity theory;Software;Information systems;Integration of Workflow and Agile Service;e-Commerce Application Architecture;REST},
	Pages = {843-849},
	Title = {Research on e-Commerce Application Architecture Based on the Integration of Workflow and Agile Service},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISECS.2008.78}}

@inproceedings{1373906,
	Abstract = {The radio architecture of the UMTS is composed of two modes: the FDD mode (WB-CDMA) and the TDD mode (TD-CDMA). Both must be included in the future mobile terminal to provide all services promised by the 3G. A work of reconfiguration will take into account the specificity of these two modes and will improve their switch. This processing simplification of these two modes can be envisaged to facilitating their implementation and cohabitation in the same terminal. For the TDD mode, the receiver called "Ts-structure" (L.Ros et al., July 2001) has been chosen for his good performance and low complexity. For the FDD mode, up to now, the conventional receiver is the Rake. Sometimes, the channel profile will oblige to add an equalizer in order to improve the Rake performance. The objective of this paper is to present a FDD equalizer with architecture similar to the equalizer TDD "Ts-structure".},
	Author = {D. {Cibaud} and G. {Jourdain} and M. {Arnd}},
	Booktitle = {2004 IEEE 15th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No.04TH8754)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:58:18 +0100},
	Doi = {10.1109/PIMRC.2004.1373906},
	Keywords = {3G mobile communication;frequency division multiplexing;equalisers;time division multiplexing;broadband networks;code division multiple access;radio receivers;cellular radio;UMTS;Universal Mobile Telecommunication System;FDD equalizer;frequency division duplex;WB-CDMA;wideband code division multiple access;TDD;time division duplex;TD-CDMA;3G mobile communication;Equalizers;RAKE receivers;Matched filters;3G mobile communication;Interference;Research and development;Switches;Software radio;Fading;Multipath channels},
	Pages = {1292-1296},
	Title = {Functional study of FDD equalizer in the context of TDD reconfiguration},
	Volume = {2},
	Year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/PIMRC.2004.1373906}}

@inproceedings{1667598,
	Abstract = {Funding is often seen an impediment to going agile. If cash is tight, you are not going to be able to pay for a "big-up-front-agile-transformation". However, transitioning to agile can be self-funding. If people take the first bold step themselves, they will find that the funding will sort itself out along the way},
	Author = {D. {Poon}},
	Booktitle = {AGILE 2006 (AGILE'06)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 14:01:01 +0100},
	Doi = {10.1109/AGILE.2006.2},
	Keywords = {project management;software development management;self funding agile transformation;software development;agile practices;Costs;Computer architecture;Companies;Impedance;Authorization;Investments;Counting circuits;Process design;Design methodology;Iterative methods},
	Pages = {9},
	Title = {A self funding agile transformation},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2006.2}}

@inproceedings{4959068,
	Abstract = {Virtual enterprise model affords the valid instruction for rapid establishing and successful running of virtual enterprise. However, authors perceive that low quality and low efficiency are serious restriction factor to the development of virtual enterprise model. In order to overcome above-mentioned embarrassment in virtual enterprise modeling, authors put forward applying software reuse technology and domain engineering theory to establishing the domain specific software architecture of virtual enterprise, then develop application system and establish the reusable component library in terms of domain specific software architecture of virtual enterprise. On the one hand, the quality and efficiency of modeling can be promoted remarkably. On the other hand, the model of virtual enterprise can be reused in the same domain.},
	Author = {R. {Tu} and W. {Liu}},
	Booktitle = {2009 First International Workshop on Education Technology and Computer Science},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:43:54 +0100},
	Doi = {10.1109/ETCS.2009.354},
	Keywords = {agile manufacturing;object-oriented programming;software architecture;software libraries;software reusability;Unified Modeling Language;virtual enterprises;virtual manufacturing;virtual enterprise modeling;domain specific software architecture;software reuse technology;domain engineering theory;component library;agile manufacturing;UML;Virtual enterprises;Software architecture;Software reusability;Virtual manufacturing;Computer integrated manufacturing;Decision support systems;Unified modeling language;Computer science education;Educational technology;Computer science;modeling;Virtual Enterprise;Domain Specific Software Architecture},
	Pages = {415-418},
	Title = {A Study of Virtual Enterprise Modeling Based on Domain Specific Software Architecture},
	Volume = {2},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ETCS.2009.354}}

@inproceedings{5641258,
	Abstract = {Test-driven development (TDD) - an established approach in business IT software development - enables test case generation based on models early in the development process. Applying TDD and models in automation systems engineering (ASE) can increase testing effectiveness and efficiency. A key question is which models are suitable for ASE application. UML models support software and systems engineering development in (a) systematically capturing requirements, (b) describing the static system architecture, and (c) specifying dynamic systems behavior. In this paper we discuss selection criteria for UML model selection in ASE and evaluate strengths and limitations of selected models.},
	Author = {R. {Hametner} and D. {Winkler} and T. {{\"O}streicher} and N. {Surnic} and S. {Biffl}},
	Booktitle = {2010 IEEE 15th Conference on Emerging Technologies Factory Automation (ETFA 2010)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:48 +0100},
	Doi = {10.1109/ETFA.2010.5641258},
	Keywords = {program testing;software engineering;Unified Modeling Language;UML model;test driven development;automation system engineering process;business IT software development approach;test case generation;Unified modeling language;Modeling;Automation;Testing;Business;Programming;Software;Automation Systems Development;UML;Test-Driven Development;Test Case Generation},
	Pages = {1-4},
	Title = {Selecting UML models for test-driven development along the automation systems engineering process},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ETFA.2010.5641258}}

@inproceedings{5501136,
	Abstract = {Agile requires accordingly agile software architecture and programming tools. For web development popular agile architecture is MVC (Model-View-Controller). For transferring data between controller and view XML is very suitable. And to transform it into final form it's better to use XSLT. XSLT is not very popular among developers, because they think this is just yet another ``strange'' language, without any real advantage comparing to usual languages. But advantages are real - XSLT allows for very interesting results, not possible with other templating languages. It definitely worth studying and this is not hard.},
	Author = {A. V. {Mayorov}},
	Booktitle = {2009 5th Central and Eastern European Software Engineering Conference in Russia (CEE-SECR)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:45:48 +0100},
	Doi = {10.1109/CEE-SECR.2009.5501136},
	Keywords = {Internet;software architecture;software prototyping;software tools;XML;XSLT;agile Web development;agile software architecture;agile software programming tools;model view controller;XML;XML;Engines;HTML;Software architecture;Computer architecture;Service oriented architecture;web development;model-view-controller;xslt;template engine},
	Pages = {303-306},
	Title = {XSLT in Agile web development},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEE-SECR.2009.5501136}}

@inproceedings{5369637,
	Abstract = {As the next generation Web technology, semantic Web can improve the automatic information processing capacity. By introducing supply chain ontology and integrating Web service with semantic Web, the semantic Web services realize the description and interaction of Web services on the semantic level. Proposed approach makes the supply chain implement the complex business process dynamic composition and heterogeneous enterprise application systems integration. The coordination and integration system framework of agile supply chain is established based on the proposed semantic Web services and SOA. This developed hierarchical framework has the advantages of loose coupling, cross enterprises, cross multi-platform systems, heterogeneous systems interaction and agile supply chain system reconstruction, which is a feasible and effective solution to implement the characteristics of agile supply chain, including agility, virtualization and reconstruction.},
	Author = {L. {Qu} and Y. {Chen} and M. {Yang}},
	Booktitle = {2009 Third International Symposium on Intelligent Information Technology Application},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:45:56 +0100},
	Doi = {10.1109/IITA.2009.94},
	Keywords = {ontologies (artificial intelligence);semantic Web;software architecture;software prototyping;supply chain management;Web services;agile supply chain;service-oriented technology;next generation Web technology;automatic information processing;supply chain ontology;semantic Web services;business process dynamic composition;heterogeneous enterprise application systems integration;SOA;Supply chains;Semantic Web;Web services;Service oriented architecture;Supply chain management;Ontologies;Collaboration;Companies;Aerodynamics;Information technology;SOA (Service-Oriented Architecture);Semantic Web Services;Agile Supply Chain;Coordination;Integration},
	Pages = {351-354},
	Title = {The Coordination and Integration of Agile Supply Chain Based on Service-oriented Technology},
	Volume = {1},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/IITA.2009.94}}

@inproceedings{1019978,
	Abstract = {Global market environment and integrated region economy greatly challenge global manufacturing. As a typical representative of advanced manufacturing modes, agile manufacturing becomes the reform and reorganization guidance of enterprises in the 21/sup st/ century because of its agile product development and management method. Based on the agility connotation of agile manufacturing, this paper proposes an agile enterprise modeling architecture, which can support the establishment of an agile manufacturing enterprise model fast and efficiently. Furthermore, the object-oriented agile manufacturing enterprise modeling method is also presented. Finally, the framework of modeling supporting tool is elaborated.},
	Author = {{Yiru Dai} and {Juanwei Yan} and {Guangqiang Tang}},
	Booktitle = {Proceedings of the 4th World Congress on Intelligent Control and Automation (Cat. No.02EX527)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:58:06 +0100},
	Doi = {10.1109/WCICA.2002.1019978},
	Keywords = {production control;manufacturing data processing;computer aided production planning;product development;object-oriented methods;agile manufacturing;enterprise modeling architecture;object-oriented technology;modeling supporting tool;advanced manufacturing;product development;production control;Agile manufacturing;Object oriented modeling;Computer integrated manufacturing;Electronic mail;Globalization;Virtual manufacturing;Product development;Intelligent control;Manufacturing automation},
	Pages = {2575-2579},
	Title = {Research on enterprise modeling architecture and supporting tool for agile manufacturing},
	Volume = {4},
	Year = {2002},
	Bdsk-Url-1 = {https://doi.org/10.1109/WCICA.2002.1019978}}

@inproceedings{4077045,
	Abstract = {Offshoring is not as popular as it seems. According to a recent German survey, only 1.5% of all outsourcing activities target offshore locations. This is a remarkably small figure taking into account the widely published purported benefits of offshoring. In this paper we demonstrate that communication problems are at the core of offshoring woes. This does not come as a surprise as they also play a major role in onshore projects. Based on our experience in tackling these challenges with our well established communication-centered agile design and development approach, we present case-study-reinforced advice for successful offshore projects. We show that a common view of the underlying architecture is of paramount importance for these projects.},
	Author = {A. {Kornstadt} and J. {Sauer}},
	Booktitle = {2007 Working IEEE/IFIP Conference on Software Architecture (WICSA'07)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:38:57 +0100},
	Doi = {10.1109/WICSA.2007.39},
	Keywords = {software engineering;offshore communication;agile architecture-centric development;German survey;offshoring;Cultural differences;Costs;Global communication;Software architecture;Outsourcing;Management training;Productivity;Software engineering;Informatics;Programming},
	Pages = {28-28},
	Title = {Tackling Offshore Communication Challenges with Agile Architecture-Centric Development},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2007.39}}

@inproceedings{6968980,
	Abstract = {Spectrum and infrastructure sharing among multiple mobile network operators have recently drawn significant attention from the industry and the academia sectors. Recent advances in the area of Software Defined Networking (SDN) launch the design of innovative SDN-based solutions for resource sharing in multi-tier networks of macrocell and small-sized base stations, a.k.a. HetNet. In this paper, we present a novel SDN-based framework that enables efficient and elastic spectrum utilization among multiple operators in 3GPP LTE-A HetNet scenario. Assuming a multi-operator environment of Frequency Division Duplex (FDD) macrocells complemented by multi-tenant Time Division Duplex (TDD) pico cells, we present a SDN-based architecture that allows efficient resource sharing among the TDD and FDD systems in a dynamic way. A TDD frame re-configuration mechanism is also employed, to optimize the ratio of uplink and downlink slots in the TDD frame of picocells. System-level simulation results demonstrate that the combination of these two functional enhancements in the LTE-A HetNet, significantly reduces the application layer delay for both the FDD macrocell and TDD picocell systems, leading to highly efficient and dynamic spectrum sharing among multiple network operators.},
	Author = {R. {Shrivastava} and S. {Costanzo} and K. {Samdanis} and D. {Xenakis} and D. {Grace} and L. {Merakos}},
	Booktitle = {2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:55:54 +0100},
	Doi = {10.1109/CloudNet.2014.6968980},
	Keywords = {3G mobile communication;frequency division multiplexing;Long Term Evolution;picocellular radio;radio spectrum management;time division multiplexing;SDN-based framework;elastic resource sharing;integrated FDD-TDD LTE-A HetNet;infrastructure sharing;mobile network operators;software defined networking;innovative SDN-based solutions;multitier networks;small-sized base stations;elastic spectrum utilization;3GPP LTE-A HetNet scenario;multioperator environment;frequency division duplex macrocells;FDD macrocells;multitenant time division duplex pico cells;multitenant TDD pico cells;TDD frame re-configuration mechanism;uplink slot-downlink slot ratio;system-level simulation;functional enhancements;TDD picocell systems;dynamic spectrum sharing;Macrocell networks;Bandwidth;Quality of service},
	Pages = {126-131},
	Title = {An SDN-based framework for elastic resource sharing in integrated FDD/TDD LTE-A HetNets},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/CloudNet.2014.6968980}}

@inproceedings{1617340,
	Abstract = {Test-driven development (TDD) is an agile software development strategy that addresses both design and testing. This paper describes a controlled experiment that examines the effects of TDD on internal software design quality. The experiment was conducted with undergraduate students in a software engineering course. Students in three groups completed semester-long programming projects using either an iterative test-first (TDD), iterative test-last, or linear test-last approach. Results from this study indicate that TDD can be an effective software design approach improving both code-centric aspects such as object decomposition, test coverage, and external quality, and developer-centric aspects including productivity and confidence. In addition, iterative development approaches that include automated testing demonstrated benefits over a more traditional linear approach with manual tests. This study demonstrates the viability of teaching TDD with minimal effort in the context of a relatively traditional development approach. Potential dangers with TDD are identified regarding programmer motivation and discipline. Pedagogical implications and instructional techniques which may foster TDD adoption will also be referenced},
	Author = {D. S. {Janzen} and H. {Saiedian}},
	Booktitle = {19th Conference on Software Engineering Education Training (CSEET'06)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:37:44 +0100},
	Doi = {10.1109/CSEET.2006.25},
	Keywords = {computer science education;educational courses;object-oriented programming;program testing;software architecture;software quality;teaching;test-driven development;agile software development;internal software design quality;undergraduate students;software engineering course;programming project;iterative test-first approach;iterative test-last approach;linear test-last approach;object decomposition;iterative development approach;programmer motivation;programmer discipline;Software testing;Software design;Automatic testing;Programming profession;Iterative methods;Productivity;Writing;Software engineering;Linear programming;System testing},
	Pages = {141-148},
	Title = {On the Influence of Test-Driven Development on Software Design},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/CSEET.2006.25}}

@inproceedings{6569596,
	Abstract = {A tunable and highly digital RF frontend for multi-band TDD radios is integrated in 45nm SOI CMOS. The PA absorbs the TX branch of the TX/RX switch with no added loss. Peak PA output power is 27.5$\pm$0.5dBm from 1.6 to 3.4GHz, with up to 30% total efficiency at 2V. For TDD LTE applications, better than -30dBc ACLR and -25dB EVM is measured with 16-QAM, 20MHz signals from 1.65 to 3.5GHz, with up to 16.5% average efficiency and 22.9dBm average power. The broadband LNA achieves AV> 14dB, NF=4.3 $\pm$ 1.6dB and IIP3 > -7dBm from 1.6 to 3.4GHz while drawing just 6mA from 1V.},
	Author = {S. {Goswami} and H. {Kim} and J. L. {Dawson}},
	Booktitle = {2013 IEEE Radio Frequency Integrated Circuits Symposium (RFIC)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:53:16 +0100},
	Doi = {10.1109/RFIC.2013.6569596},
	Keywords = {CMOS analogue integrated circuits;field effect MMIC;Long Term Evolution;low noise amplifiers;MMIC power amplifiers;quadrature amplitude modulation;silicon-on-insulator;time division multiplexing;low-noise amplifiers;LNA;16-QAM;EVM;TDD LTE applications;TX-RX switch;TX branch;power amplifiers;PA;tunable digital RF frontend;highly digital RF frontend;SOI CMOS;multiband TDD radios;frequency-agile RF frontend;size 45 nm;efficiency 30 percent;voltage 2 V;efficiency 16.5 percent;current 6 mA;voltage 1 V;frequency 1.6 GHz to 3.5 GHz;Radio frequency;Switches;CMOS integrated circuits;Resonant frequency;Current measurement;Computer architecture;Power amplifiers;4G wireless communication;low-noise amplifiers;power amplifiers;silicon on insulator technology;software radio;time division multiplexing;wireless LAN},
	Pages = {331-334},
	Title = {A frequency-agile RF frontend for multi-band TDD radios in 45nm SOI CMOS},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/RFIC.2013.6569596}}

@inproceedings{4293630,
	Abstract = {Using Bruce Tuckman 's [1] "Forming - Storming - Norming - Performing" model of team development we review the effects of change on our team through each stage. We go on to show how in certain cases this change is cyclic and techniques abandoned in one phase may well be re-introduced in another. While tools, techniques and processes may change we observed that despite the cultural change there is one thing that remains constant: The agile principles and values.},
	Author = {D. {Rowley} and M. {Lange}},
	Booktitle = {Agile 2007 (AGILE 2007)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:39:26 +0100},
	Doi = {10.1109/AGILE.2007.28},
	Keywords = {software development management;agile team;team development;Testing;Computational Intelligence Society;Companies;Cultural differences;Large-scale systems;Information systems;Aging;Peace technology;Computer architecture;Service oriented architecture},
	Pages = {408-414},
	Title = {Forming to Performing: The Evolution of an Agile Team},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2007.28}}

@inproceedings{1667575,
	Abstract = {The author had a unique opportunity to experience and analyze the adoption of agile practices on two projects at a major car rental company which, although sharing many similarities including staff, professed commitment to agile process, architecture, and programming environment, experienced two drastically different outcomes. The first became bogged down and eventually went to production late, over-budget, with fewer features then expected, and was only reluctantly embraced by its user community. The second was ready for its first production milestone after only 9 weeks, was enthusiastically embraced by its users, and went on to become an unqualified success and a model for other software development projects at this company. In this report the author illustrates the similarities between these two projects, highlight the differences, and draw conclusions about the factors critical to the success of agile projects in large corporate environments},
	Author = {R. {Coffin}},
	Booktitle = {AGILE 2006 (AGILE'06)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 14:01:08 +0100},
	Doi = {10.1109/AGILE.2006.3},
	Keywords = {project management;software development management;agile practices;car rental company;software development projects;agile projects;Production;Java;Programming environments;Object oriented modeling;Object oriented programming;Aging;Hardware;Software quality;Programming profession;Employee welfare},
	Pages = {7},
	Title = {A tale of two projects [agile projects]},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2006.3}}

@inproceedings{6065642,
	Abstract = {Agile development methods are being recognized as popular and efficient approaches to the development of software systems that have features such as a short delivery period and unclear requirements. They emphasize customer satisfaction, fast response to changes, and release in less time. According to a recent survey, SCRUM is one of the most popular methods that are currently being used. Some backlogs, especially high priority backlogs that are functional requirements of customers, are developed repeatedly at each sprint period. Despite the known advantages of SCRUM, however, its backlogs focus only on functional features. Thus, it is difficult to effectively reflect the softwares quality attributes. As known, the failure of a software project is caused by the non-satisfaction not of functional features but of quality attributes, such as performance, usability, and reliability. This paper introduces the ACRUM that is a quality attribute driven agile development method. The main characteristic of the proposed solution is that it is derived from values and practices of SCRUM to be compatible with the SCRUM process and to keep its agility intact. The effect of ACRUM was evaluated through an agile process evaluation checklist and applying it into a commercial project of Samsung Electronics. The results showed that ACRUM is more efficient than the legacy agile development process.},
	Author = {S. {Jeon} and M. {Han} and E. {Lee} and K. {Lee}},
	Booktitle = {2011 Ninth International Conference on Software Engineering Research, Management and Applications},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:49:52 +0100},
	Doi = {10.1109/SERA.2011.24},
	Keywords = {customer satisfaction;project management;software management;software prototyping;software quality;software quality attribute driven agile development;software system development;customer satisfaction;high-priority backlog;customer requirement;software project failure;SCRUM process;ACRUM process;Samsung electronics;legacy agile development process;Computer architecture;Business;Software architecture;Software systems;Random access memory;Quality Attribute;Agile;SCRUM;QAW},
	Pages = {203-210},
	Title = {Quality Attribute Driven Agile Development},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/SERA.2011.24}}

@inproceedings{5261068,
	Abstract = {Imagine yourself with a team that flies in from AU, the UK, and the US in bi-weekly shifts to work with a communications firm. Mix in inexperience, bad behaviours, vague program direction, and a mandated intro to Agile in a silo-ed non-agile environment. Couple this with a capability driven team whose focus is to assist other teams to drive out SOA: and you have a recipe for a Team in Flux.},
	Author = {S. {McKinnon}},
	Booktitle = {2009 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:45:05 +0100},
	Doi = {10.1109/AGILE.2009.39},
	Keywords = {DP industry;process capability analysis;software architecture;software maintenance;team working;communications firm;SOA;software oriented architecture;Service oriented architecture;Semiconductor optical amplifiers;Business communication;Cultural differences;Companies;Gold;Global communication;Springs;Globalization;Mission critical systems;flux;team;iterations;SOA;business capabilities;leadership},
	Pages = {303-308},
	Title = {Iterating a Team in Flux},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2009.39}}

@inproceedings{6621375,
	Abstract = {The development of Service Oriented Architectures (SOA) requires modeling a service development process that covers the entire lifecycle of such architectures and allows an iterative and incremental development. In addition, the change of either environment execution conditions, or requirements, during the development may require the evolution of the process. In this paper, we present a framework named FASOAD that addresses this issue. FASOAD implements the phases of an agile approach for SOA development as a component assembly model. Based on the Service Component Architecture (SCA) model, this framework allows developing SOA projects and enforcing the agile methods principles. In addition, we address the case of the dynamic reconfiguration of service-oriented architectures in presence of an evolution of their development process. A case study is presented to illustrate the approach.},
	Author = {H. {Chehili} and L. {Seinturier} and M. {Boufaida}},
	Booktitle = {2013 24th International Workshop on Database and Expert Systems Applications},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:53:54 +0100},
	Doi = {10.1109/DEXA.2013.28},
	Keywords = {service-oriented architecture;software prototyping;FASOAD;agile service-oriented architectures development;iterative development;incremental development;environment execution conditions;SOA development;component assembly architecture model;SCA model;dynamic reconfiguration;Business;Service-oriented architecture;Assembly;Semiconductor optical amplifiers;Computer architecture;Monitoring;SOA;Agile;Framework;SCA;model;Component},
	Pages = {222-226},
	Title = {FASOAD: A Framework for Agile Service-Oriented Architectures Development},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/DEXA.2013.28}}

@inproceedings{4293605,
	Abstract = {The Department of Defense (DoD) and the Defense Information Systems Agency (DISA) have historically operated on 18-36 months release cycles for major IT projects. DISA is now striving towards delivering smaller components in 30-60-90 day release cycles. This paper discusses Pragmatics' successful implementation of agile development techniques into a non-agile shop, our introduction of agile development techniques to DISA to help them achieve their goal of delivering high quality software on shorter release cycles, and our lessons learned along the way.},
	Author = {S. {Cohan}},
	Booktitle = {Agile 2007 (AGILE 2007)},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:39:32 +0100},
	Doi = {10.1109/AGILE.2007.54},
	Keywords = {computational linguistics;software engineering;agile development techniques;Department of Defense;Defense Information Systems Agency;high quality software;pragmatics;Software testing;Service oriented architecture;Software quality;Application software;Web services;Open source software;Programming profession;Command and control systems;Automatic testing;Keyboards},
	Pages = {255-261},
	Title = {Successful Integration of Agile Development Techniques within DISA},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2007.54}}

@inproceedings{4150009,
	Abstract = {Manufacturing control systems typically include layers of logic and heuristics making them difficult to develop. Furthermore, in agile manufacturing planning and control (MPC) environment, the dynamics of decision-making interactions between agile MPC system components are poorly understood and can be undependable because of the absence of a master controller or optimizer. This paper addresses these challenges by developing a multi-layer architecture for a supervisory decision logic unit (DLU) of an agile MPC system. After presenting a dynamic model for the system, a decision logic unit for that system that links the operational level with the higher enterprise level strategy is described. The architecture of the DLU is composed of three layers where the first layer is responsible for dynamically managing the selection of the different MPC policies that suits the market strategy. The second layer describes an algorithm for optimal parameters settings for each of the MPC policies. Finally the third layer of the architecture is responsible for the automatic on-line control of manufacturing system to maintain required production, work-in- process and inventory levels.},
	Author = {A. M. {Deif} and W. H. {ElMaraghy}},
	Booktitle = {2006 9th International Conference on Control, Automation, Robotics and Vision},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:38:39 +0100},
	Doi = {10.1109/ICARCV.2006.345099},
	Keywords = {agile manufacturing;decision making;optimisation;planning;agile manufacturing planning;manufacturing control systems;heuristics making;decision-making interactions;supervisory decision logic unit;enterprise level strategy;automatic online control;manufacturing system;inventory levels;Logic;Agile manufacturing;Control systems;Manufacturing systems;Automatic control;Control theory;Intelligent manufacturing systems;Electrical equipment industry;Manufacturing industries;Systems engineering and theory;Agile Manufacturing;Architecture;Planning;Control},
	Pages = {1-5},
	Title = {Architecture for Decision Logic Unit in Agile Manufacturing Planning and Control Systems},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICARCV.2006.345099}}

@article{6567990,
	Abstract = {A multistandard SAW-less receiver is designed exploring a current-mode architecture. A class-AB common-gate transformer-based low-noise transconductor amplifier (LNTA) is used to provide high linearity and harmonic filtering. A resonant passive mixer is adopted in order to allow the current-mode operation and improve the harmonic rejection. A low-power divider with intrinsic 25% duty-cycle is introduced to drive the passive mixer. A second-order Rauch biquad with complex poles makes-up the IQ blocker tolerant baseband. The receiver is designed to be suitable for SAW-less TDD and typical FDD applications with 3.8 and 1.9 dB of NF and > 18 and > 16 dBm of IIP3, respectively, using only 32 mW for each receiver.},
	Author = {I. {Fabiano} and M. {Sosio} and A. {Liscidini} and R. {Castello}},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:54:41 +0100},
	Doi = {10.1109/JSSC.2013.2271859},
	Journal = {IEEE Journal of Solid-State Circuits},
	Keywords = {code division multiple access;frequency division multiplexing;low noise amplifiers;mixers (circuits);surface acoustic wave devices;time division multiplexing;FDD applications;SAW-less TDD;IQ blocker tolerant baseband;Rauch biquad;duty cycle;low power divider;harmonic rejection;current mode operation;resonant passive mixer;harmonic filtering;LNTA;low noise transconductor amplifier;class AB common gate transformer;current mode architecture;multistandard SAW-less receiver;SAW-less analog front end receivers;Noise;Receivers;Harmonic analysis;Coils;Mixers;Noise measurement;Impedance;25% duty cycle;baseband;blocker tolerant;current-mode;direct conversion;divider;dynamic range;frequency-division duplexing (FDD);GSM;harmonic mixing;linearity;low-noise transconductor amplifier (LNTA);low power;noise folding;reciprocal mixing;resonant mixer;SAW-less;time-division duplexing (TDD);UMTS;W-CDMA},
	Number = {12},
	Pages = {3067-3079},
	Title = {SAW-Less Analog Front-End Receivers for TDD and FDD},
	Volume = {48},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/JSSC.2013.2271859}}

@inproceedings{5261065,
	Abstract = {In a large agile organization (more than three teams or 30 team members) with self-organized empowered teams, R&D leadership roles are still needed to support teams through topics including resource management and strategic vision. This experience report will highlight these R&D leadership roles, describe flat, hierarchical, and matrix R&D organization structures, and then illustrate the influence of these organizational structures on key leaderships behaviors for an agile environment. These behaviors include leading versus managing, flexing existing team boundaries, driving both team and project success, and balancing team versus individual needs. The report concludes that while some organizations structures are more supportive in an agile environment, the organization structure is less important than identifying leaders who demonstrate those key leadership behaviors.},
	Author = {E. {Moore}},
	Booktitle = {2009 Agile Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:44:59 +0100},
	Doi = {10.1109/AGILE.2009.14},
	Keywords = {behavioural sciences;project management;research and development;software development management;strategic planning;team working;large-scale organization structure;leadership behavior;large agile organization;team member;self-organized empowered team;R&D leadership role;resource management;strategic vision;flat R&D organization structure;hierarchical R&D organization structure;matrix R&D organization structure;team success;project success;agile environment;agile software development;Large-scale systems;Research and development;Testing;Project management;Medical services;Europe;Service oriented architecture;Resource management;Information technology;Financial management;behaviors;large-scale;leadership;roles;teams},
	Pages = {309-313},
	Title = {Influence of Large-Scale Organization Structures on Leadership Behaviors},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2009.14}}

@inproceedings{6045992,
	Abstract = {Despite the fact that lean and agile software development has become mainstream recently, especially for larger-scale organizations building complex products, the methodology leaves many architectural questions unanswered. For instance, agile methods such as Extreme Programming propose late architectural decisions and frequent refactoring, while others suggest an "architectural runway" as infrastructure for a certain set of upcoming customer features. Software "product lines" consist of a set of software products that share a common, managed set of features. These product lines are developed from reusable core assets incorporating variations in order to derive customer-specific product variants. Hence, this research explores interoperability and complementarity of lean and agile approaches in combination with a software product line engineering approach. With this position paper, we discuss both, (i) complementing architectural issues in large scale lean and agile development and (ii) providing methodological guidance to make product line engineering more agile and efficient. In doing so, we come to the conclusion that this combination enables us to build the right products in time and budget. Our findings are based on observations and experience from a large-scale software company with several thousand developers working on various solution combinations of highly interdependent products.},
	Author = {B. {Blau} and T. {Hildenbrand}},
	Booktitle = {2011 Sixth International Conference on Availability, Reliability and Security},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:49:46 +0100},
	Doi = {10.1109/ARES.2011.66},
	Keywords = {lean production;product development;software architecture;software maintenance;software prototyping;software reusability;product line engineering;large-scale lean product development environments;agile software product development environments;decentral control;managed reuse;extreme programming;architectural decisions;frequent refactoring;architectural runway;customer-specific product variants;reusable core assets;software product line engineering approach;large-scale software company;Software;Programming;Computer architecture;Companies;Product development;Software engineering;lean product development;agile software engineering;software product lines},
	Pages = {404-408},
	Title = {Product Line Engineering in Large-Scale Lean and Agile Software Product Development Environments - Towards a Hybrid Approach to Decentral Control and Managed Reuse},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/ARES.2011.66}}

@inproceedings{1438932,
	Abstract = {This paper presents a novel approach for the configuration and runtime usage of user interfaces or human machine interface (HMI) systems based on research being conducted at the MSI Research Institute, Loughborough University. In collaboration with Krause GmbH and Lamb Technicon UK Ltd two full size demonstrator machines have been commissioned to evaluate a component based control system and its associated design environment. The Krause machine consists of a transport system and a tappet assembly station, the Lamb Technicon machine consists of a single station from a transfer line machine for cylinder head machining. The framework for HMI system's described in this paper meets the requirements that automated manufacturing production machines face from the emerging agile manufacturing paradigm. Machines must be able to respond quickly to continuous change by many globally distributed engineering partners. This facilitates visibility of the system common model to all the globally distributed engineering partners involved in a given project. Within the C-B framework, HMI systems are composed from instances of reusable software templates that are targeted at specific user types. User targeted operator interfaces offer a common look and feel that improves usability. The machine's configuration is achieved by populating a series of HMI templates to produce a complete machine HMI system. A thin client architecture is used based on server/client Internet technologies that allow the machine HMI to be executed on any Internet enabled computer using a standard Web browser. It is possible to drive both the real machine and a simulated 3D virtual machine model via the HMI, enabling engineers to be trained on the operation of the HMI prior to the real machine being completed.},
	Author = {E. W. {Mellor} and R. {Harrison} and A. A. {West}},
	Booktitle = {IEEE Conference on Robotics, Automation and Mechatronics, 2004.},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:58:26 +0100},
	Doi = {10.1109/RAMECH.2004.1438932},
	Keywords = {user interfaces;agile manufacturing;production equipment;client-server systems;Internet;agile automated manufacturing system;diagnostic capability;reconfigurable user interface;human machine interface system;component based control system;Krause machine;transport system;tappet assembly station;transfer line machine;cylinder head machining;automated manufacturing production machines;thin client architecture;server/client Internet technology;standard Web browser;3D virtual machine model;Computerized monitoring;Agile manufacturing;Manufacturing automation;Internet;Runtime;User interfaces;Humans;Online Communities/Technical Collaboration;Size control;Control systems},
	Pages = {287-291},
	Title = {Reconfigurable user interface's to support monitoring and diagnostic capabilities within agile automated manufacturing system's},
	Volume = {1},
	Year = {2004},
	Bdsk-Url-1 = {https://doi.org/10.1109/RAMECH.2004.1438932}}

@inproceedings{5262920,
	Abstract = {The dynamic and fast-changing in technology, globalization, and world wide competition, has increased pressure on third party logistics companies (abbreviated 3PL) to be agile and responsive. To support this business need, 3PL companies need their IT infrastructures and business processes to be adaptive to the changes. In this paper, agility of a SOA (service-oriented architecture) and BPM (business process management) based infrastructure has been discussed, and a prototype of SOA-BPM based information system for 3PL management by Tuscany and Osworkflow was introduced.},
	Author = {W. {ZhenHua} and H. {Yousen} and D. {ZiYun} and Z. {Wei}},
	Booktitle = {2009 IEEE International Conference on Automation and Logistics},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:45:13 +0100},
	Doi = {10.1109/ICAL.2009.5262920},
	Keywords = {management information systems;software architecture;SOA-BPM based information system;third party logistics companies;service-oriented architecture;business process management;Semiconductor optical amplifiers;Information systems;Logistics;Service oriented architecture;Companies;Web services;Costs;Automation;Packaging;Application software;Agility;SOA;BPM;3PL;Tuscany;Osworkflow},
	Pages = {248-252},
	Title = {SOA --- BPM based information system for promoting agility of third party logistics},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICAL.2009.5262920}}

@inproceedings{4599496,
	Abstract = {Today, many software projects are being developed by collaborating programmers working across multiple locations. Whatever the reason may be, outsourcing, organizational structure, or external collaboration, these projects often suffer from the physical separation of developing across the city, across the country, or around the world. Such distances intensify challenges such as peer communications, shared understanding between teams, and systems integration. This paper describes how three organizations adapted agile processes to overcome barriers such as multiple time zones, mixed cultures, mismatched schedules, and limited travel budget to frequently deliver successful software releases.},
	Author = {C. {Young} and H. {Terashima}},
	Booktitle = {Agile 2008 Conference},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:42:24 +0100},
	Doi = {10.1109/Agile.2008.7},
	Keywords = {groupware;project management;software architecture;software development management;agile process;distributed software project development;outsourcing;organizational structure;external collaboration;software release;software architecture;Portals;Programming;Collaboration;Java;Schedules;Software;Electronic mail;distributed development;agile processes;collaboration;physical separation},
	Pages = {304-309},
	Title = {How Did We Adapt Agile Processes to Our Distributed Development?},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/Agile.2008.7}}

@inproceedings{5552730,
	Abstract = {Although WS-BPEL is a widely used language for modeling executable business processes in service oriented architectures it is almost impossible to dynamically bind services at runtime taking complex constraints and optimisation goals into account. The approach presented in this paper uses semantically annotated workflow templates and extensions to introduce adaptability which enables agile service oriented architectures. The technological solution will be validated in a critical infrastructure environment where resilience and security play an important role.},
	Author = {W. {Halb} and H. {Zeiner} and B. {Jandl} and H. {Lernbei{\ss}} and C. {Derler}},
	Booktitle = {2010 IEEE International Conference on Web Services},
	Date-Added = {2019-11-01 12:16:14 +0100},
	Date-Modified = {2019-11-01 13:47:05 +0100},
	Doi = {10.1109/ICWS.2010.42},
	Keywords = {software architecture;software prototyping;specification languages;workflow management software;agile service oriented architecture;adaptive processes;semantically annotated workflow templates;WS-BPEL;business processes;Semantics;Business;Service oriented architecture;Concrete;Security;Resilience;Optimization;BPEL;dynamic binding;run time adaptability},
	Pages = {632-633},
	Title = {Agile Service Oriented Architecture with Adaptive Processes Using Semantically Annotated Workflow Templates},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICWS.2010.42}}

@article{MIRANDA20101205,
	Abstract = {There is a need to collect, measure, and present progress information in all projects, and Agile projects are no exception. In this article, the authors show how the line of balance, a relatively obscure indicator, can be used to gain insights into the progress of projects not provided by burn down charts or cumulative flow diagrams, two of the most common indicators used to track and report progress in Agile projects. The authors also propose to replace the original plan-based control point lead-time calculations with dynamic information extracted from a version control system and introduce the concept of the ideal plan to measure progress relative to both, end of iteration milestones and project completion date.},
	Author = {Eduardo Miranda and Pierre Bourque},
	Doi = {https://doi.org/10.1016/j.jss.2010.01.043},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Scrum, Feature Driven Development, Project management, Tracking and control, Line of balance, Release planning, Burn down charts, Cumulative flow diagrams, Agile methodologies, LOB},
	Note = {SPLC 2008},
	Number = {7},
	Pages = {1205 - 1215},
	Title = {Agile monitoring using the line of balance},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210000294},
	Volume = {83},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121210000294},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2010.01.043}}

@article{SCOTT201456,
	Abstract = {Teaching agile practices is in the cutting-edge of Software Engineering education since agile methodologies are widely used in the industry. An effective strategy to teach agile practices is the use of a capstone project, in which students develop requirements following an agile methodology. To improve students' learning experience, professors have to keep track and analyze the information generated by the students during the capstone project development. The problem here arises from the large amount of information generated in the learning process, which hinders professors to meet each student's learning profile. Particularly, to know the students skills and preferences are key aspects on a learner-centered approach of education in order to personalize the teaching. In this work, we aim to discover the relationships between students' performance along a Scrum-based capstone project and their learning style according to the Felder--Silverman model, towards a first step to build the profiles. To address this issue, we mined association rules from the interaction of 33 Software Engineering students with Virtual Scrum, a tool that supports the development of the capstone project in the course. In the present work we describe promising results in experiments with a case-study.},
	Author = {Ezequiel Scott and Guillermo Rodr{\'\i}guez and {\'A}lvaro Soria and Marcelo Campo},
	Doi = {https://doi.org/10.1016/j.chb.2014.03.027},
	Issn = {0747-5632},
	Journal = {Computers in Human Behavior},
	Keywords = {Software Engineering, Agile software development, Software Engineering education, Learning styles},
	Pages = {56 - 64},
	Title = {Are learning styles useful indicators to discover how students use Scrum for the first time?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0747563214001496},
	Volume = {36},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0747563214001496},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.chb.2014.03.027}}

@article{JEYAKUMAR2009363,
	Abstract = {We present geometric criteria for a feasible point that satisfies the Kuhn--Tucker conditions to be a global minimizer of mathematical programming problems with or without bounds on the variables. The criteria apply to multi-extremal programming problems which may have several local minimizers that are not global. We establish such criteria in terms of underestimators of the Lagrangian of the problem. The underestimators are required to satisfy certain geometric property such as the convexity (or a generalized convexity) property. We show that the biconjugate of the Lagrangian can be chosen as a convex underestimator whenever the biconjugate coincides with the Lagrangian at a point. We also show how suitable underestimators can be constructed for the Lagrangian in the case where the problem has bounds on the variables. Examples are given to illustrate our results.},
	Author = {V. Jeyakumar and S. Srisatkunarajah},
	Doi = {https://doi.org/10.1016/j.ejor.2007.12.021},
	Issn = {0377-2217},
	Journal = {European Journal of Operational Research},
	Keywords = {Mathematical programming problems, Sufficient optimality conditions, Multi-extremal problems, Bounds on the variables, Generalized convexity, Underestimators},
	Number = {2},
	Pages = {363 - 367},
	Title = {Geometric conditions for Kuhn--Tucker sufficiency of global optimality in mathematical programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0377221707012088},
	Volume = {194},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0377221707012088},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ejor.2007.12.021}}

@article{SHARP2008506,
	Abstract = {Mature eXtreme programming (XP) teams are highly collaborative and self-organising. In previous studies, we have observed that these teams rely on two apparently simple mechanisms of co-ordination and collaboration: story cards and the Wall. Story cards capture and embody the user stories which form the basis of implementation, while the Wall is a physical space used to organise and display the cards being implemented during the current development cycle (called an iteration). In this paper, we analyse the structure and use of story cards and the Wall in three mature XP teams, using a distributed cognition approach. The teams work in different commercial organisations developing different systems, yet we find significant similarities between their use of these two artefacts. Although simple, teams use the cards and the Wall in sophisticated ways to represent and communicate information that is vital to support their activities. We discuss the significance of the physical medium for the story cards and the Wall in an XP team and discuss the considerations that need to be taken into account for the design of technology to support the teams.},
	Author = {Helen Sharp and Hugh Robinson},
	Doi = {https://doi.org/10.1016/j.ijhcs.2007.10.004},
	Issn = {1071-5819},
	Journal = {International Journal of Human-Computer Studies},
	Keywords = {Distributed cognition, Story card, Information radiator, Agile development},
	Note = {Collaborative and social aspects of software development},
	Number = {7},
	Pages = {506 - 518},
	Title = {Collaboration and co-ordination in mature eXtreme programming teams},
	Url = {http://www.sciencedirect.com/science/article/pii/S1071581907001371},
	Volume = {66},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1071581907001371},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhcs.2007.10.004}}

@article{CHOI2013105,
	Abstract = {Machine to machine (M2M) communications among machines without human intervention have attracted more and more prominence due to high market potential in the near future. In this article, we propose a heuristic algorithm of allocating weighted per-user data rates for half duplex frequency division duplexing (H-FDD), and show how much performance gain can be achieved for the H-FDD M2M communications.},
	Author = {Jihwan P. Choi and Jaesub Shin and Ji-Woong Choi},
	Doi = {https://doi.org/10.3182/20131111-3-KR-2043.00025},
	Issn = {1474-6670},
	Journal = {IFAC Proceedings Volumes},
	Keywords = {Machine-to-Machine (M2M) communication, half duplex FDD (H-FDD), resource allocation},
	Note = {3rd IFAC Symposium on Telematics Applications},
	Number = {29},
	Pages = {105 - 106},
	Title = {A Resource Allocation Scheme in Half Duplex FDD (H-FDD) Systems for M2M Communications},
	Url = {http://www.sciencedirect.com/science/article/pii/S1474667015343731},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1474667015343731},
	Bdsk-Url-2 = {https://doi.org/10.3182/20131111-3-KR-2043.00025}}

@article{KETTUNEN2009408,
	Abstract = {Many industrial new product development (NPD) software projects apply nowadays agile methodologies. These methodologies, such as Scrum, eXtreme Programming (XP), and Feature-Driven Development (FDD) date back to 1990s, and the Agile Manifesto was declared in 2001. However, already before that the concept of agile manufacturing (AM) was discovered to describe a corporate ability for quick adaptation to changing requirements. There is surprising amount in common between these two fields. This raises a question of whether NPD software development companies could take even more overall advantage of those different agile approaches. This interdisciplinary paper explores the commonalities between the key concepts of AM and some of the most popular agile software methods, and consequently suggests potential new areas for software process improvement (SPI) in large-scale NPD organizations. An industrial case example illustrates how agility in embedded software product development can be enhanced by following typical NPD principles. We conclude that there is potential for further improvements in software product development industry in general by seeing agility as a wider, organization-oriented business concept following the AM/NPD learning. Current agile software process models cover only a subset of this space.},
	Author = {Petri Kettunen},
	Doi = {https://doi.org/10.1016/j.technovation.2008.10.003},
	Issn = {0166-4972},
	Journal = {Technovation},
	Keywords = {Agile software process models, Software process improvement, Agile manufacturing, New product development, Agile enterprise},
	Number = {6},
	Pages = {408 - 422},
	Title = {Adopting key lessons from agile manufacturing to agile software product development---A comparative study},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166497208001302},
	Volume = {29},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0166497208001302},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.technovation.2008.10.003}}

@article{ALTARAWNEH2011893,
	Abstract = {Small software development firms represent the majority of all software firms in most countries. These firms are facing the same software engineering challenges that affect large software firms. Software Process Improvement (SPI) traditional models were developed to help large and very large firms, however small software firms could not afford these models. Furthermore, they need to manage and improve their software development processes for several reasons such as dealing with the rapid technology advances, maintaining their products, satisfying the customers' needs and sustaining their operations. This paper presents the methodology's stages of developing a suitable software development process improvement framework by using Capability Maturity Model Integration (CMMI-DEV V1.2) as the basic model for improvement and Extreme Programming (XP) method as the basic software development method.},
	Author = {Mejhem Yousef al-Tarawneh and Mohd Syazwan Abdullah and Abdul Bashah Mat Ali},
	Doi = {https://doi.org/10.1016/j.procs.2010.12.146},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Software Process, Software Process Improvement, Small Software Development Firms},
	Note = {World Conference on Information Technology},
	Pages = {893 - 897},
	Title = {A proposed methodology for establishing software process development improvement for small software development firms},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050910005211},
	Volume = {3},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050910005211},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2010.12.146}}

@article{MUNIR2014375,
	Abstract = {Context
Test driven development (TDD) has been extensively researched and compared to traditional approaches (test last development, TLD). Existing literature reviews show varying results for TDD.
Objective
This study investigates how the conclusions of existing literature reviews change when taking two study quality dimension into account, namely rigor and relevance.
Method
In this study a systematic literature review has been conducted and the results of the identified primary studies have been analyzed with respect to rigor and relevance scores using the assessment rubric proposed by Ivarsson and Gorschek 2011. Rigor and relevance are rated on a scale, which is explained in this paper. Four categories of studies were defined based on high/low rigor and relevance.
Results
We found that studies in the four categories come to different conclusions. In particular, studies with a high rigor and relevance scores show clear results for improvement in external quality, which seem to come with a loss of productivity. At the same time high rigor and relevance studies only investigate a small set of variables. Other categories contain many studies showing no difference, hence biasing the results negatively for the overall set of primary studies. Given the classification differences to previous literature reviews could be highlighted.
Conclusion
Strong indications are obtained that external quality is positively influenced, which has to be further substantiated by industry experiments and longitudinal case studies. Future studies in the high rigor and relevance category would contribute largely by focusing on a wider set of outcome variables (e.g. internal code quality). We also conclude that considering rigor and relevance in TDD evaluation is important given the differences in results between categories and in comparison to previous reviews.},
	Author = {Hussan Munir and Misagh Moayyed and Kai Petersen},
	Doi = {https://doi.org/10.1016/j.infsof.2014.01.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Test-driven development (TDD), Test-last development (TLD), Internal code quality, External code quality, Productivity},
	Number = {4},
	Pages = {375 - 394},
	Title = {Considering rigor and relevance when evaluating test driven development: A systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000135},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584914000135},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2014.01.002}}

@article{TRINIDAD2008883,
	Abstract = {Software Product Lines (SPL) and agile methods share the common goal of rapidly developing high-quality software. Although they follow different approaches to achieve it, some synergies can be found between them by (i) applying agile techniques to SPL activities so SPL development becomes more agile; and (ii) tailoring agile methodologies to support the development of SPL. Both options require an intensive use of feature models, which are usually strongly affected by changes on requirements. Changing large-scale feature models as a consequence of changes on requirements is a well-known error-prone activity. Since one of the objectives of agile methods is a rapid response to changes in requirements, it is essential an automated error analysis support in order to make SPL development more agile and to produce error-free feature models. As a contribution to find the intended synergies, this article sets the basis to provide an automated support to feature model error analysis by means of a framework which is organized in three levels: a feature model level, where the problem of error treatment is described; a diagnosis level, where an abstract solution that relies on Reiter's theory of diagnosis is proposed; and an implementation level, where the abstract solution is implemented by using Constraint Satisfaction Problems (CSP). To show an application of our proposal, a real case study is presented where the Feature-Driven Development (FDD) methodology is adapted to develop an SPL. Current proposals on error analysis are also studied and a comparison among them and our proposal is provided. Lastly, the support of new kinds of errors and different implementation levels for the proposed framework are proposed as the focus of our future work.},
	Author = {P. Trinidad and D. Benavides and A. Dur{\'a}n and A. Ruiz-Cort{\'e}s and M. Toro},
	Doi = {https://doi.org/10.1016/j.jss.2007.10.030},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Feature models, Agile methods, Error analysis, Theory of diagnosis, Constraint programming},
	Note = {Agile Product Line Engineering},
	Number = {6},
	Pages = {883 - 896},
	Title = {Automated error analysis for the agilization of feature modeling},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121207002543},
	Volume = {81},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121207002543},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2007.10.030}}

@article{MCAVOY2007552,
	Abstract = {This paper presents a qualitative investigation of learning failures associated with the introduction of a new software development methodology by a project team. This paper illustrates that learning is more than the cognitive process of acquiring a new skill; learning also involves changes in behaviour and even beliefs. Extreme Programming (XP), like other software development methodologies, provides a set of values and guidelines as to how software should be developed. As these new values and guidelines involve behavioural changes, the study investigates the introduction of XP as a new learning experience. Researchers use the concepts of single and double-loop learning to illustrate how social actors learn to perform tasks effectively and to determine the best task to perform. The concept of triple-loop learning explains how this learning process can be ineffective, accordingly it is employed to examine why the introduction of XP was ineffective in the team studied. While XP should ideally foster double-loop learning, triple-loop learning can explain why this does not necessarily occur. Research illustrates how power factors influence learning among groups of individuals; this study focuses on one specific power factor -- the power inherent in the desire to conform. The Abilene Paradox describes how groups can make ineffective decisions that are contrary to that which group members personally desire or believe. Ineffective decision-making occurs due to the desire to conform among group members; this was shown as the cause of ineffective learning in the software team studied. This desire to conform originated in how the project team cohered as a group, which was, in turn, influenced by the social values embraced by XP.},
	Author = {John McAvoy and Tom Butler},
	Doi = {https://doi.org/10.1016/j.infsof.2007.02.012},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {XP adoption, Triple-loop learning, Social factors, Qualitative approach, Participant observation},
	Note = {Qualitative Software Engineering Research},
	Number = {6},
	Pages = {552 - 563},
	Title = {The impact of the Abilene Paradox on double-loop learning in an agile team},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584907000146},
	Volume = {49},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584907000146},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2007.02.012}}

@article{WANG20121287,
	Abstract = {In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift ``from agile to lean''. However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes.},
	Author = {Xiaofeng Wang and Kieran Conboy and Oisin Cawley},
	Doi = {https://doi.org/10.1016/j.jss.2012.01.061},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Agile software development, Lean software development, Scrum, Leagile, Kanban, Experience report, Software engineering},
	Note = {Special Issue: Agile Development},
	Number = {6},
	Pages = {1287 - 1299},
	Title = {``Leagile'' software development: An experience report analysis of the application of lean approaches in agile software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000404},
	Volume = {85},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212000404},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.01.061}}

@article{DUBINSKY2006693,
	Abstract = {Roles' playing is common in our lives. We play different roles with our family, at work as well as in other environments. Role allocation in software development projects is also accepted though it may be implemented differently by different software development methods. In a previous work [Y. Dubinsky, O. Hazzan, Roles in agile software development teams, in: 5th International Conference on Extreme Programming and Agile Processes in Software Engineering, 2004, pp. 157--165] we have found that personal roles may raise teammates' personal accountability while maintaining the essence of the software development method. In this paper we present our role scheme, elaborate on its implementation and explain how it can be used to derive metrics. We illustrate our ideas by data gathered in student projects in the university.},
	Author = {Yael Dubinsky and Orit Hazzan},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.06.013},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Role scheme, Project metrics, Project-based course},
	Note = {Agile Methodologies for Software Production},
	Number = {11},
	Pages = {693 - 699},
	Title = {Using a role scheme to derive software project metrics},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106000713},
	Volume = {52},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106000713},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.06.013}}

@article{SENAPATHI20121255,
	Abstract = {While past research has contributed to the understanding of how organizations adopt agile methodologies (AM), little is known about their post-adoptive usage in organizations. By integrating theories from systems development methodologies, diffusion of innovations, and agile methodology literature, this paper proposes a new model that identifies a set of critical factors pertinent to post-adoptive usage of agile practices. This model is used to inform analysis of post-adoptive usage of agile practices in two major organizations. The results indicate relative advantage, team attitude and technical competence, championing, and top management support (TMS) are the key factors determining the extent to which agile practices can be assimilated into an organization. Specifically, both findings and this model confirm that the deeper the assimilation of agile practices into the organization, the better understanding of how assimilation leads to specific improvements in its systems development outcomes.},
	Author = {Mali Senapathi and Ananth Srinivasan},
	Doi = {https://doi.org/10.1016/j.jss.2012.02.025},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Post-adoptive agile usage, Effectiveness, Kanban, Scrum},
	Note = {Special Issue: Agile Development},
	Number = {6},
	Pages = {1255 - 1268},
	Title = {Understanding post-adoptive agile usage: An exploratory cross-case analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000489},
	Volume = {85},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212000489},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.02.025}}

@article{PINO20101662,
	Abstract = {For software process improvement -- SPI -- there are few small organizations using models that guide the management and deployment of their improvement initiatives. This is largely because a lot of these models do not consider the special characteristics of small businesses, nor the appropriate strategies for deploying an SPI initiative in this type of organization. It should also be noted that the models which direct improvement implementation for small settings do not present an explicit process with which to organize and guide the internal work of the employees involved in the implementation of the improvement opportunities. In this paper we propose a lightweight process, which takes into account appropriate strategies for this type of organization. Our proposal, known as a ``Lightweight process to incorporate improvements'', uses the philosophy of the Scrum agile method, aiming to give detailed guidelines for supporting the management and performance of the incorporation of improvement opportunities within processes and their putting into practice in small companies. We have applied the proposed process in two small companies by means of the case study research method, and from the initial results, we have observed that it is indeed suitable for small businesses.},
	Author = {Francisco J. Pino and Oscar Pedreira and F{\'e}lix Garc{\'\i}a and Miguel Rodr{\'\i}guez Luaces and Mario Piattini},
	Doi = {https://doi.org/10.1016/j.jss.2010.03.077},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Software process improvement, Small software companies, Model to guide improvements, Agile methods, Scrum, SPI},
	Number = {10},
	Pages = {1662 - 1677},
	Title = {Using Scrum to guide the execution of software process improvement in small organizations},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412121000138X},
	Volume = {83},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S016412121000138X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2010.03.077}}

@article{WANG2010477,
	Abstract = {This paper presents a strategy for fault detection and diagnosis (FDD) of HVAC systems involving sensor faults at the system level. Two schemes are involved in the system-level FDD strategy, i.e. system FDD scheme and sensor fault detection, diagnosis and estimation (FDD&E) scheme. In the system FDD scheme, one or more performance indices (PIs) are introduced to indicate the performance status (normal or faulty) of each system. Regression models are used as the benchmarks to validate the PIs computed from the actual measurements. The reliability of the system FDD is affected by the health of sensor measurements. A method based on principal component analysis (PCA) is used to detect and diagnose the sensor bias and to correct the sensor bias prior to the use of the system FDD scheme. Two interaction analyses are conducted. One is the impact of system faults on sensor FDD&E. The other is the impact of corrected sensor faults on the system FDD. It is found that the sensor FDD&E method can work well in identifying biased sensors and recovering biases even if system faults coexist, and the system FDD method is effective in diagnosing the system-level faults using processed measurements by the sensor FDD&E.},
	Author = {Shengwei Wang and Qiang Zhou and Fu Xiao},
	Doi = {https://doi.org/10.1016/j.enbuild.2009.10.017},
	Issn = {0378-7788},
	Journal = {Energy and Buildings},
	Keywords = {HVAC system, Fault detection, Fault diagnosis, Model-based, Sensor fault},
	Number = {4},
	Pages = {477 - 490},
	Title = {A system-level fault detection and diagnosis strategy for HVAC systems involving sensor faults},
	Url = {http://www.sciencedirect.com/science/article/pii/S0378778809002552},
	Volume = {42},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0378778809002552},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.enbuild.2009.10.017}}

@article{VLAANDEREN201158,
	Abstract = {Context
Although agile software development methods such as SCRUM and DSDM are gaining popularity, the consequences of applying agile principles to software product management have received little attention until now.
Objective
In this paper, this gap is filled by the introduction of a method for the application of SCRUM principles to software product management.
Method
A case study research approach is employed to describe and evaluate this method.
Results
This has resulted in the `agile requirements refinery', an extension to the SCRUM process that enables product managers to cope with complex requirements in an agile development environment. A case study is presented to illustrate how agile methods can be applied to software product management.
Conclusions
The experiences of the case study company are provided as a set of lessons learned that will help others to apply agile principles to their software product management process.},
	Author = {Kevin Vlaanderen and Slinger Jansen and Sjaak Brinkkemper and Erik Jaspers},
	Doi = {https://doi.org/10.1016/j.infsof.2010.08.004},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software product management, Requirements management, Requirements refinery, Agile development, SCRUM},
	Number = {1},
	Pages = {58 - 70},
	Title = {The agile requirements refinery: Applying SCRUM principles to software product management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584910001539},
	Volume = {53},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584910001539},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2010.08.004}}

@article{LAYMAN2006654,
	Abstract = {With the recent emergence of agile software development technologies, the software community is awaiting sound, empirical investigation of the impacts of agile practices in a live setting. One means of conducting such research is through industrial case studies. There are a number of influencing factors that contribute to the success of such a case study. In this paper, we describe a case study performed at Sabre Airline SolutionsTM evaluating the effects of adopting Extreme Programming (XP) practices with a team that had characteristically plan-driven risk factors. We compare the team's business-related results (productivity and quality) to two published sources of industry averages. Our case study found that the Sabre team yielded above-average post-release quality and average to above-average productivity. We discuss our experience in conducting this case study, including specifics of how data was collected, the rationale behind our process of data collection, and what obstacles were encountered during the case study. We identify four factors that potentially impact the outcome of industrial case studies: availability of data, tool support, cooperative personnel and project status. Recognizing and planning for these factors is essential to conducting industrial case studies.},
	Author = {Lucas Layman and Laurie Williams and Lynn Cunningham},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.06.009},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Software engineering, Case study, Agile software development, Extreme programming},
	Note = {Agile Methodologies for Software Production},
	Number = {11},
	Pages = {654 - 667},
	Title = {Motivations and measurements in an agile case study},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106000671},
	Volume = {52},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106000671},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.06.009}}

@article{CHUNG2005864,
	Abstract = {The next generation mobile communication system should be able to accommodate various services, including voice and Internet dial-up. As is well known, voice service has symmetrical, i.e. equal uplink and downlink bandwidth requirements, whereas Internet dial-up traffic usually has asymmetrical, i.e. unequal uplink and downlink bandwidth requirements. Frequency division duplex (FDD) usually allocates the same bandwidth to both uplink and downlink, whereas time division duplex (TDD) can change the number of time slots allocated to uplink and downlink to allocate different bandwidth to uplink and downlink. Thus, for scenarios with asymmetrical traffic, TDD is a better solution. To provide better quality of service during handoff, make-before-break soft handoff is preferable to break-before-make hard handoff. Furthermore, as forced termination of an ongoing call is less tolerable than blocking of a new call, handoff calls are usually given priority over new calls. We consider cellular multiservice TDD networks supporting both symmetrical and asymmetrical traffic with soft handoff and handoff queueing. First, we derive the mathematical model of the considered system with multi-dimensional birth--death process. Second, since the asymmetrical traffic may lead to the waste of uplink or downlink bandwidth, the effects of the traffic mix and the number of uplink slots on the system performance are studied. Third, the comparison of TDD and FDD with the presence of asymmetrical traffic is performed. Fourth, as the handoff region area increases, more calls will enter into or originate at that region. Thus, we study the effect of handoff region area on performance. Fifth, as a handoff call that cannot obtain the required channel upon arrival will be rejected if the handoff queue is full, the effect of handoff queue size on the performance is studied.},
	Author = {Shun-Ping Chung and Shih-Chin Chien},
	Doi = {https://doi.org/10.1016/j.comcom.2004.11.009},
	Issn = {0140-3664},
	Journal = {Computer Communications},
	Keywords = {Asymmetrical traffic, Soft handoff, TDD, Handoff queue, Traffic mix},
	Number = {8},
	Pages = {864 - 879},
	Title = {Analysis of multiservice cellular networks with asymmetrical traffic and handoff queue},
	Url = {http://www.sciencedirect.com/science/article/pii/S0140366404003925},
	Volume = {28},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0140366404003925},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.comcom.2004.11.009}}

@article{SVERRISDOTTIR2014257,
	Abstract = {New methods have emerged in last* decades for managing projects and develop software. The agile ideology was generally defined with the agile manifesto in 2001 and is widely used for software project management. Scrum is the most common method within agile and has become one of the most popular tools in software development in Iceland. The objective of this research was to shed light on how Scrum is applied. The role of the product owner is studied; the knowledge and skills he is required to have, according to the Scrum method. This is compared to the perception of a number of actual project owners of their role. Information was gathered by semi structured interviews with a limited number of product owners that have had that role for at least 1 year. The results show that the majority of the participants in the survey are using different project management methods. They apply the methods that best fit their own operations. The understanding of the role and responsibility of the product owner is quite different between organizations but seldom in perfect conformance with the official Scrum method. Cases were reported where there are two product owners for the same product. One is then responsible for business aspect but the other is responsible for technical aspects of the product. Scrum has a strong position in software development with its defined roles, collaboration emphasis, understanding, visibility, effective process and fast development.},
	Author = {Hrafnhildur Sif Sverrisdottir and Helgi Thor Ingason and Haukur Ingi Jonasson},
	Doi = {https://doi.org/10.1016/j.sbspro.2014.03.030},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {Project Management, Responsibility, The role of the product owner (PO) ;Scrum, Agile, Technical knowledge, Software Development},
	Note = {Selected papers from the 27th IPMA (International Project Management Association), World Congress, Dubrovnik, Croatia, 2013},
	Pages = {257 - 267},
	Title = {The Role of the Product Owner in Scrum-comparison between Theory and Practices},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877042814021211},
	Volume = {119},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877042814021211},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2014.03.030}}

@article{GERMAIN200517,
	Abstract = {The emergence of various software development methodologies raises the need to evaluate and compare their efficiencies. One way of performing such a comparison is to have different teams apply different process models in the implementation of multiple versions of common specifications. This study defines a new cognitive activity classification scheme which has been used to record the effort expended by six student teams producing parallel implementations of the same software requirements specifications. Three of the teams used a process based on the Unified Process for Education (UPEDU), a teaching-oriented process derived from the Rational Unified Process. The other three teams used a process built around the principles of the Extreme Programming (XP) methodology. Important variations in effort at the cognitive activity level between teams shows that the classification could scarcely be used without categorization at a higher level. However, the relative importance of a category of activities aimed at defining ``active'' behaviour was shown to be almost constant for all teams involved, possibly showing a fundamental behaviour pattern. As secondary observations, aggregate variations by process model tend to be small and limited to a few activities, and coding-related activities dominate the effort distribution for all the teams.},
	Author = {{\'E}ric Germain and Pierre N. Robillard},
	Doi = {https://doi.org/10.1016/j.jss.2004.02.022},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Empirical software engineering, Process measurement, Cognitive activity, Productivity},
	Note = {Software Engineering Education and Training},
	Number = {1},
	Pages = {17 - 27},
	Title = {Engineering-based processes and agile methodologies for software development: a comparative case study},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120400038X},
	Volume = {75},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S016412120400038X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2004.02.022}}

@article{YU2014911,
	Abstract = {Context
Agile software development is an alternative software development methodology that originated from practice to encourage collaboration between developers and users, to leverage rapid development cycles, and to respond to changes in a dynamic environment. Although agile practices are widely used in organizations, academics call for more theoretical research to understand the value of agile software development methodologies.
Objective
This study uses shared mental models theory as a lens to examine practices from agile software methodologies to understand how agile practices enable software development teams to work together to complete tasks and work together effectively as a team.
Method
A conceptual analysis of specific agile practices was conducted using the lens of shared mental models theory. Three agile practices from Xtreme Programming and Scrum are examined in detail, system metaphor, stand-up meeting, and on-site customer, using shared mental models theory.
Results
Examining agile practices using shared mental models theory elucidates how agile practices improve collaboration during the software development process. The results explain how agile practices contribute toward a shared understanding and enhanced collaboration within the software development team.
Conclusions
This conceptual analysis demonstrates the value of agile practices in developing shared mental models (i.e. shared understanding) among developers and customers in software development teams. Some agile practices are useful in developing a shared understanding about the tasks to be completed, while other agile practices create shared mental models about team processes and team interactions. To elicit the desired outcomes of agile software development methods, software development teams should consider whether or not agile practices are used in a manner that enhances the team's shared understanding. Using three specific agile practices as examples, this research demonstrates how theory, such as shared mental models theory, can enhance our understanding regarding how agile practices are useful in enhancing collaboration in the workplace.},
	Author = {Xiaodan Yu and Stacie Petter},
	Doi = {https://doi.org/10.1016/j.infsof.2014.02.010},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Agile software development, Agile practices, Shared mental models theory, Extreme programming, Scrum},
	Number = {8},
	Pages = {911 - 921},
	Title = {Understanding agile software development practices using shared mental models theory},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914000524},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584914000524},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2014.02.010}}

@article{CUBRIC2013119,
	Abstract = {The aim of this paper is to describe, evaluate and discuss a new method for teaching agile project management and similar subjects in higher education. Agile is not only a subject domain in this work, the teaching method itself is based on Scrum, a popular agile methodology mostly used in software development projects. The method is supported by wikis, a natural platform for simulation of software development environments. The findings from the evaluation indicate that the method enables the creation of ``significant learning'', which prepares students for life-long learning and increases their employability. However, the knowledge gains, resulting from wiki interactions are found to be more quantitative than qualitative. The results also imply that despite the active promotion of agile values of communication and feedback, issues regarding the teamwork are still emerging. The engagement of the teacher in the learning and teaching process was discovered to be a motivational factor for the team cohesion. This paper could be of interest to anyone planning to teach agile in the higher education settings, but also to a wider academic community interested in applying agile methods in their own teaching practice.},
	Author = {Marija Cubric},
	Doi = {https://doi.org/10.1016/j.ijme.2013.10.001},
	Issn = {1472-8117},
	Journal = {The International Journal of Management Education},
	Keywords = {Agile, CSCL, Experiential learning, Project management, Scrum, Wikis},
	Number = {3},
	Pages = {119 - 131},
	Title = {An agile method for teaching agile in business schools},
	Url = {http://www.sciencedirect.com/science/article/pii/S1472811713000359},
	Volume = {11},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1472811713000359},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijme.2013.10.001}}

@article{ORLOWSKI20141175,
	Abstract = {The aim of this article is to present the project framework for constructing a Software Process Simulation Modeling (SPSM) system. SPSM systems can be used as a virtual environment for the selection of methods and tools of project management in IT support organizations. The constructed system simulates the Scrum methodology, including the management processes and the project roles. For the implementation of Scrum processes, the Scrum ontology is proposed and for the competences of the roles of project team members, a fuzzy-logic representation. As a result we present the hybrid fuzzy-ontological system. The framework of the design processes proposed in the article was verified on the basis of real courses of project management processes in a large IT company.},
	Author = {Cezary Or{\l}owski and Irena Bach-D{\k a}browska and Pawe{\l} Kap{\l}a{\'n}ski and W{\l}odzimierz Wysocki},
	Doi = {https://doi.org/10.1016/j.procs.2014.08.214},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {SPSM, Software Process Simulation Modelling, Ontology, Fuzzy Logics, Scrum ;},
	Note = {Knowledge-Based and Intelligent Information & Engineering Systems 18th Annual Conference, KES-2014 Gdynia, Poland, September 2014 Proceedings},
	Pages = {1175 - 1184},
	Title = {Hybrid Fuzzy-ontological Project Framework of a Team Work Simulation System},
	Url = {http://www.sciencedirect.com/science/article/pii/S187705091401179X},
	Volume = {35},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S187705091401179X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2014.08.214}}

@article{RIZWANJAMEELQURESHI2008654,
	Abstract = {The concept of agile process models has gained great popularity in software (SW) development community in past few years. Agile models promote fast development. This property has certain drawbacks, such as poor documentation and bad quality. Fast development promotes use of agile process models in small-scale projects. This paper modifies and evaluates extreme programming (XP) process model and proposes a novel adaptive process mode based on these modifications.},
	Author = {M. Rizwan Jameel Qureshi and S.A. Hussain},
	Doi = {https://doi.org/10.1016/j.advengsoft.2007.08.001},
	Issn = {0965-9978},
	Journal = {Advances in Engineering Software},
	Keywords = {XP, Adaptive software development, SDLC, OSSD, CBD},
	Number = {8},
	Pages = {654 - 658},
	Title = {An adaptive software development process model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0965997807001603},
	Volume = {39},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0965997807001603},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.advengsoft.2007.08.001}}

@article{WOOD2013660,
	Abstract = {Context
Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team's success? Perhaps the more specific software engineering team factors, for example the agile development method's collective ownership and code management, are decisive.
Objective
To assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams.
Method
We studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams' adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project's outcomes. We used Williams et al.'s (2004a) [33] Shodan measures of XP methods, and regression analysis.
Results
All three types of variables are associated with the project's performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance.
Conclusion
Client and team foci of the XP method are its critical active ingredients.},
	Author = {Stephen Wood and George Michaelides and Chris Thomson},
	Doi = {https://doi.org/10.1016/j.infsof.2012.10.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software development, Extreme programming, Agile methods, Teamwork, Cooperation, Performance},
	Number = {4},
	Pages = {660 - 672},
	Title = {Successful extreme programming: Fidelity to the methodology or good teamworking?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002091},
	Volume = {55},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584912002091},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2012.10.002}}

@article{YAMAMOTO20123216,
	Abstract = {Blanket and selective Ge growth on Si is investigated using reduced pressure chemical vapor deposition. To reduce the threading dislocation density (TDD) at low thickness, Ge deposition with cyclic annealing followed by HCl etching is performed. In the case of blanket Ge deposition, a TDD of 1.3106cm2 is obtained, when the Ge layer is etched back from 4.5m thickness to 1.8m. The TDD is not increased relative to the situation before etching. The root mean square of roughness of the 1.8m thick Ge is about 0.46nm, which is of the same level as before HCl etching. Further etching shows increased surface roughness caused by non-uniform strain distribution near the interface due to misfit dislocations and threading dislocations. The TDD also becomes higher because the etchfront of Ge reaches areas with high dislocation density near the interface. In the case of selective Ge growth, a slightly lower TDD is observed in smaller windows caused by a weak pattern size dependence on Ge thickness. A significant decrease of TDD of selectively grown Ge is also observed by increasing the Ge thickness. An about 10 times lower TDD at the same Ge thickness is demonstrated by applying a combination of deposition and etching processes during selective Ge growth.},
	Author = {Yuji Yamamoto and Grzegorz Kozlowski and Peter Zaumseil and Bernd Tillack},
	Doi = {https://doi.org/10.1016/j.tsf.2011.10.095},
	Issn = {0040-6090},
	Journal = {Thin Solid Films},
	Keywords = {Hetero epitaxy, Germanium, Chemical vapor deposition, Annealing, Etching, Dislocation},
	Note = {ICSI-7},
	Number = {8},
	Pages = {3216 - 3221},
	Title = {Low threading dislocation Ge on Si by combining deposition and etching},
	Url = {http://www.sciencedirect.com/science/article/pii/S0040609011018359},
	Volume = {520},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0040609011018359},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.tsf.2011.10.095}}

@article{GOLFARELLI20132357,
	Abstract = {Most agile methods divide a project into sprints (iterations), and include a sprint planning phase that is critical to ensure the project success. Several factors impact on the optimality of a sprint plan, which makes the planning problem difficult. In this paper we formalize the planning problem and propose an optimization model that, given the estimates made by the project team and a set of development constraints, produces a multi-sprint optimal plan that maximizes the business value perceived by users. To cope with the inherent flexibility and uncertainty of agile projects, our approach ensures that a baseline plan can be revised and re-optimized during project execution without disrupting it, which we call smooth replanning. The planning problem is converted into a generalized assignment problem, given a linear programming formulation, and solved using the IBM ILOG CPLEX Optimizer. Our model is validated on both real and synthetic projects. In particular, a case study on two real projects confirms the effectiveness of our approach; as to efficiency, for medium-sized problems an exact solution is found in a few minutes, while for large problems a heuristic solution that is less than 1% far from the exact one is returned in a few seconds. Finally, some smooth replanning tests investigate the trade-off between plan quality and stability.},
	Author = {Matteo Golfarelli and Stefano Rizzi and Elisa Turricchia},
	Doi = {https://doi.org/10.1016/j.jss.2013.04.028},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Agile methods, Scrum, Optimization models, Software engineering, Linear programming},
	Number = {9},
	Pages = {2357 - 2370},
	Title = {Multi-sprint planning and smooth replanning: An optimization model},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213001039},
	Volume = {86},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121213001039},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2013.04.028}}

@article{SCHNEIDER2005121,
	Abstract = {Criticism is sometimes leveled at the academic Software Engineering community on the basis that current educational practices are too document-centric. Both students and practitioners have suggested that one of the popular, lighter-weight, agile methods would be a better choice. This paper examines the educational goals for undergraduate Software Engineering education and considers how they might be met by the practices of eXtreme Programming. Our judgment is that education about some agile practices could be beneficial for small-scale development. However, as it stands now, eXtreme Programming as a package does not lend itself for use in educating about large-scale system development in tertiary education.},
	Author = {Jean-Guy Schneider and Lorraine Johnston},
	Doi = {https://doi.org/10.1016/j.jss.2003.09.025},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {eXtreme programming, Agile methodologies, Tertiary education, Software engineering education},
	Note = {The new context for software engineering education and training},
	Number = {2},
	Pages = {121 - 132},
	Title = {eXtreme Programming----helpful or harmful in educating undergraduates?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121203002929},
	Volume = {74},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121203002929},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2003.09.025}}

@article{BRAVO20131759,
	Abstract = {The advances in network and collaboration technologies enable the creation of powerful environments for collaborative programming. One such environment is COLLECE, a groupware system to support collaborative edition, compilation and execution of programs in a synchronous distributed fashion, which includes advanced tools for communication, coordination and workspace awareness. The article analyses firstly some usability and design issues, discussing strengths and weaknesses of the system as a basis for the development of groupware tools to support collaborative programming. Then, the focus is on a number of experimental activities carried out. COLLECE was used to conduct a set of experimental activities about work productivity and program quality when comparing the activity of pair and solo programmers, and to analyse potential associations between ways of working and collaborating, and specific characteristics of the programs produced.},
	Author = {Crescencio Bravo and Rafael Duque and Jes{\'u}s Gallardo},
	Doi = {https://doi.org/10.1016/j.jss.2012.08.039},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Collaborative programming, Distributed pair programming, Collaboration and interaction analysis, Collaborative learning environments},
	Number = {7},
	Pages = {1759 - 1771},
	Title = {A groupware system to support collaborative programming: Design and experiences},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212002439},
	Volume = {86},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212002439},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.08.039}}

@article{KOSINAR2013175,
	Abstract = {Requirements on software products are becoming more and more complicated and software systems of today are characterized by increasing complexity and size. Therefore, software systems can no longer be developed feasibly without the processes supported by appropriate methods. We propose a method for configuration and modification of software processes in companies based on gathered knowledge and our approach allows to support and optimize the processes with formal methods of modelling and machine-learning based simulations.},
	Author = {Michal Ko{\v s}in{\'a}r and Radoslav {\v S}trba},
	Doi = {https://doi.org/10.3182/20130925-3-CZ-3023.00028},
	Issn = {1474-6670},
	Journal = {IFAC Proceedings Volumes},
	Keywords = {formalization, modelling, agile methods, SCRUM, Discrete-Event Simulation, software process, effort estimation, multi-agent, OWL, TIL, PROLOG, simulation, healthcare information systems},
	Note = {12th IFAC Conference on Programmable Devices and Embedded Systems},
	Number = {28},
	Pages = {175 - 180},
	Title = {Simulations of Agile Software Processes for Healthcare Information Systems Development Based on Machine Learning Methods},
	Url = {http://www.sciencedirect.com/science/article/pii/S1474667015373201},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1474667015373201},
	Bdsk-Url-2 = {https://doi.org/10.3182/20130925-3-CZ-3023.00028}}

@article{PEDRYCZ2006700,
	Abstract = {Agile methodologies and Extreme programming are the new and highly promising endeavors in Software Engineering. By addressing the important issue of dealing with continuously changing requirements we are faced with panoply of new problems and a genuine need to revisit some principles and classic models of software developments. When it comes to the management of software projects and a way in which we are looking at the software processes and underlying practices, it becomes apparent that in the management practices the issue of uncertainty needs to be quantified and fully addressed. Similarly, it becomes of interest to develop lightweight models of software quality and software processes that are easy to construct and modify as well are transparent to the developer and manager. Given these arguments, in this study we propose logic models based upon the mechanisms of multivalued and fuzzy logic. The realization of such models gives rise to so-called logic networks that are easy to construct, calibrate and interpret.},
	Author = {Witold Pedrycz},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.06.014},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Agile methodology, Extreme programming, Logic framework of modeling, Uncertainty, Logic networks, Fuzzy logic, Learning and calibration of logic models},
	Note = {Agile Methodologies for Software Production},
	Number = {11},
	Pages = {700 - 707},
	Title = {Quantitative logic-based framework for agile methodologies},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106000725},
	Volume = {52},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106000725},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.06.014}}

@article{CANFORA20071317,
	Abstract = {Pair programming has attracted an increasing interest from practitioners and researchers: there is initial empirical evidence that it has positive effects on quality and overall delivery time, as demonstrated by several controlled experiments. The practice does not only regard coding, since it can be applied to any other phase of the software process: analysis, design, and testing. Because of the asymmetry between design and coding, applying pair programming to the design phase might not produce the same benefits as those it produces in the development phase. In this paper, we report the findings of a controlled experiment on pair programming, applied to the design phase and performed in a software company. The results of the experiment suggest that pair programming slows down the task, yet improves quality. Furthermore we compare our results with those of a previous exploratory experiment involving students, and we demonstrate how the outcomes exhibit very similar trends.},
	Author = {Gerardo Canfora and Aniello Cimitile and Felix Garcia and Mario Piattini and Corrado Aaron Visaggio},
	Doi = {https://doi.org/10.1016/j.jss.2006.11.004},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Software engineering, Pair designing, Empirical studies},
	Note = {The Impact of Barry Boehm's Work on Software Engineering Education and Training},
	Number = {8},
	Pages = {1317 - 1327},
	Title = {Evaluating performances of pair designing in industry},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121206003414},
	Volume = {80},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121206003414},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2006.11.004}}

@article{DRURYGROGAN2014506,
	Abstract = {Context
While project management success factors have long been established via the golden triangle, little is known about how project iteration objectives and critical decisions relate to these success factors. It seems logical that teams' iteration objectives would reflect project management success factors, but this may not always be the case. If not, how are teams' objectives for iterations differing from the golden triangle of project management success factors?
Objective
This study identifies iteration objectives and the critical decisions that relate to the golden triangle of project management success factors in agile software development teams working in two-week iterations.
Method
The author conducted semi-structured interviews with members across three different agile software development teams using a hybrid of XP and Scrum agile methodologies. Iteration Planning and Retrospective meetings were also observed. Interview data was transcribed, coded and reviewed by the researcher and two independently trained research assistants. Data analysis involved organizing the data to identify iteration objectives and critical decisions to identify whether they relate to project management success factors.
Results
Agile teams discussed four categories of iteration objectives: Functionality, Schedule, Quality and Team Satisfaction. Two of these objectives map directly to two aspects of the golden triangle: schedule and quality. The agile teams' critical decisions were also examined to understand the types of decisions the teams would have made differently to ensure success, which resulted in four categories of such decisions: Quality, Dividing Work, Iteration Amendments and Team Satisfaction.
Conclusion
This research has contributed to the software development and project management literature by examining iteration objectives on agile teams and how they relate to the golden triangle of project management success factors to see whether these teams incorporate the golden triangle factors in their objectives and whether they include additional objectives in their iterations. What's more, this research identified four critical decisions related to the golden triangle. These findings provide important insight to the continuing effort to better assess project management success, particularly for agile teams.},
	Author = {Meghann L. Drury-Grogan},
	Doi = {https://doi.org/10.1016/j.infsof.2013.11.003},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Team performance, Project management success factors, Iteration objectives, Team objectives, Agile software development},
	Note = {Performance in Software Development},
	Number = {5},
	Pages = {506 - 515},
	Title = {Performance on agile teams: Relating iteration objectives and critical decisions to project management success factors},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913002176},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584913002176},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2013.11.003}}

@article{FUJIKURA201238,
	Abstract = {Nano-indentation measurements on freestanding GaN substrates clearly showed, for the first time, that the hardness of the GaN crystal can be controlled by changing the growth conditions for hydride vapor phase epitaxy (HVPE). The hardness of the GaN crystal is probably governed by heterogeneous nucleation of dislocations through a nitrogen vacancy-related mechanism. The observed changes in the nano-indentation hardness can be explained in terms of the dependence on growth condition of the concentration of nitrogen vacancies in the GaN crystal. This control of the crystal hardness has a significant effect on the dislocation-reducing process during the HVPE-growth of freestanding GaN substrates. According to the theory, the threading dislocation density (TDD) should decrease continuously with increasing growth thickness. However, as a result of the accumulation of growth-induced stress, the reduction of TDD for a freestanding GaN substrate with a less-hard crystal stopped at a certain critical thickness and became saturated at around the mid-106/cm2 range. This saturation behavior of TDDs can be overcome by making the GaN crystal harder by changing the HVPE conditions, giving freestanding GaN substrates with extremely low TDDs in the range 105/cm2.},
	Author = {Hajime Fujikura and Yuichi Oshima and Takeshi Megro and Toshiya Saito},
	Doi = {https://doi.org/10.1016/j.jcrysgro.2011.12.019},
	Issn = {0022-0248},
	Journal = {Journal of Crystal Growth},
	Keywords = {A1. Characterization, A1. Defects, A2. Growth from vapor, A3. Hydride vapor phase epitaxy, B1. Nitride, B2. Semiconducting gallium compounds},
	Note = {The 7th International Workshop on Bulk Nitride Semiconductors},
	Number = {1},
	Pages = {38 - 43},
	Title = {Hardness control for improvement of dislocation reduction in HVPE-grown freestanding GaN substrates},
	Url = {http://www.sciencedirect.com/science/article/pii/S0022024811010384},
	Volume = {350},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0022024811010384},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jcrysgro.2011.12.019}}

@article{DRURY20121239,
	Abstract = {The obstacles facing decision making in Agile development are critical yet poorly understood. This research examines decisions made across four stages of the iteration cycle: Iteration Planning, Iteration Execution, Iteration Review and Iteration Retrospective. A mixed method approach was employed, whereby a focus group was initially conducted with 43 Agile developers and managers to determine decisions made at different points of the iteration cycle. Subsequently, six illustrative mini cases were purposefully conducted as examples of the six obstacles identified in these focus groups. This included interviews with 18 individuals in Agile projects from five different organizations: a global consulting organization, a multinational communications company, two multinational software development companies, and a large museum organization. This research contributes to Agile software development literature by analyzing decisions made during the iteration cycle and identifying six key obstacles to these decisions. Results indicate the six decision obstacles are unwillingness to commit to decisions; conflicting priorities; unstable resource availability; and lack of: implementation; ownership; empowerment. These six decision obstacles are mapped to descriptive decision making principles to demonstrate where the obstacles affect the decision process. The effects of these obstacles include a lack of longer-term, strategic focus for decisions, an ever-growing backlog of delayed work from previous iterations, and a lack of team engagement.},
	Author = {Meghann Drury and Kieran Conboy and Ken Power},
	Doi = {https://doi.org/10.1016/j.jss.2012.01.058},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Agile decision making, Agile project management, Agile software development, Case study, Decision making, Decision obstacles, Focus group, Iteration decisions, Iteration Planning, Iteration Review, Retrospective, Scrum, Software engineering, Team decisions},
	Note = {Special Issue: Agile Development},
	Number = {6},
	Pages = {1239 - 1254},
	Title = {Obstacles to decision making in Agile software development teams},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000374},
	Volume = {85},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212000374},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.01.058}}

@article{CHOI20084273,
	Abstract = {Although several recent studies have successfully reduced the threading dislocation density (TDD) in Ge films grown on Si, high surface roughness is still problematic for useful nanoscale lithography and device fabrication. In this work, we achieved both low TDD and surface roughness by repeating a deposition--annealing cycle consisting of the following steps: low temperature deposition, high temperature and high rate deposition, high temperature hydrogen annealing. The root-mean-square roughness of the 3-cycle sample is in the range of 0.4--0.6nm for 1010m2 scan field atomic force microscopy (AFM) images. The TDD measured by plan-view TEM is 0.8--1107cm2 with a 1.44m thickness sample. Furthermore, a 4-cycle sample reveals further improvement in surface planarity and pit density in the AFM images with a thickness of 2.38m Ge. The high temperature and high rate Ge deposition combined with high-temperature hydrogen annealing efficiently reduces not only the TDD, but also the surface roughness.},
	Author = {Donghun Choi and Yangsi Ge and James S. Harris and Joel Cagnon and Susanne Stemmer},
	Doi = {https://doi.org/10.1016/j.jcrysgro.2008.07.029},
	Issn = {0022-0248},
	Journal = {Journal of Crystal Growth},
	Keywords = {A1. Atomic force microscopy, A2. Single crystal growth, A3. Chemical vapor deposition processes, B2. Semiconducting germanium},
	Number = {18},
	Pages = {4273 - 4279},
	Title = {Low surface roughness and threading dislocation density Ge growth on Si (001)},
	Url = {http://www.sciencedirect.com/science/article/pii/S0022024808005551},
	Volume = {310},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0022024808005551},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jcrysgro.2008.07.029}}

@article{PETERSEN20091479,
	Abstract = {Recent empirical studies have been conducted identifying a number of issues and advantages of incremental and agile methods. However, the majority of studies focused on one model (Extreme Programming) and small projects. To draw more general conclusions we conduct a case study in large-scale development identifying issues and advantages, and compare the results with previous empirical studies on the topic. The principle results are that (1) the case study and literature agree on the benefits while new issues arise when using agile in large-scale and (2) an empirical research framework is needed to make agile studies comparable.},
	Author = {Kai Petersen and Claes Wohlin},
	Doi = {https://doi.org/10.1016/j.jss.2009.03.036},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Agile, Incremental, State of the art, Case study},
	Note = {SI: QSIC 2007},
	Number = {9},
	Pages = {1479 - 1490},
	Title = {A comparison of issues and advantages in agile and incremental development between state of the art and an industrial case},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121209000855},
	Volume = {82},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121209000855},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2009.03.036}}

@article{CHANDRASEKARAN20093708,
	Abstract = {Mixed Software Programming refers to a novel software development paradigm resulting from efforts to combine two different programming approaches: Solo Programming and Pair Programming. Solo Programming refers to the traditional practice of assigning a single developer to develop a software module and Pair Programming refers to a relatively new approach where two developers work simultaneously on developing a module. In Mixed Programming, given a set of modules to be developed, a chosen subset of modules may be developed using Solo Programming and the remaining modules using Pair Programming. Motivated by applications in Mixed Software Programming, we consider the following generalization of classical fractional 1-matching problem: Given an undirected simple graph G=(V;E), and a positive number F, find values for xe,eE, satisfying the following: 1.x{0,12,1}eE.2.e(i)xe1iV, where (i)={eE:e=(i,j)},iV.3.Maximize {2eExeF|{iV:e(i)xe=1}|}. We show that this problem is solvable in strongly polynomial time. Our primary focus in this paper is on obtaining the structure of the optimal solution for an arbitrary instance of the problem.},
	Author = {R. Chandrasekaran and M. Dawande},
	Doi = {https://doi.org/10.1016/j.dam.2009.07.012},
	Issn = {0166-218X},
	Journal = {Discrete Applied Mathematics},
	Keywords = {Matching, Mixed software programming, Structural analysis},
	Number = {18},
	Pages = {3708 - 3720},
	Title = {Structural analysis of a fractional matching problem},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166218X09003084},
	Volume = {157},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0166218X09003084},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.dam.2009.07.012}}

@article{YU2014550,
	Abstract = {Faults occurring in improper routine operations and poor preventive maintenance of heating, ventilating, air conditioning, and refrigeration systems (HVAC&R) equipment result in excessive energy consumption. An air-handling unit (AHU) is one of the most extensively operated equipment in large commercial buildings. This device is typically customized and lacks of quality system integration, which can result in hardware failures and controller errors. This paper aims to provide a systematic review of existing fault detection and diagnosis (FDD) methods for an AHU therefore inspire new approaches with high performance in reality. For this goal, the background of AHU systems, general FDD framework and typical faults in AHUs, is described. Ten desirable characteristics used in a review of FDD in chemical process control are introduced to evaluate the methodologies and results. A new categorization method is proposed to better interpret the different and most recent approaches. The main FDD methodologies and hybrid approaches are described and commented to illustrate the use of evaluation standard parameters for improving the performance of FDD on AHUs.},
	Author = {Yuebin Yu and Denchai Woradechjumroen and Daihong Yu},
	Doi = {https://doi.org/10.1016/j.enbuild.2014.06.042},
	Issn = {0378-7788},
	Journal = {Energy and Buildings},
	Keywords = {Fault detection and diagnosis, Air-handling units, Analytical-based, Desirable characteristics},
	Pages = {550 - 562},
	Title = {A review of fault detection and diagnosis methodologies on air-handling units},
	Url = {http://www.sciencedirect.com/science/article/pii/S0378778814005246},
	Volume = {82},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0378778814005246},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.enbuild.2014.06.042}}

@article{VANDERLINDEN2003149,
	Abstract = {This article describes the architecture of an EPR system developed for the PropeR project. This EPR system not only aims at supporting home care of stroke patients, but is also designed in such a way that it can be ported to other medical services without much effort. We will briefly describe the Stroke Service and the related PropeR project. Starting from a list of requirements to construct a generic EPR system we will outline the architecture and describe the standards and methods used. Subsequently we describe the implementation and the problems encountered. In the discussion, we will go into the advantages and disadvantages of the tools and techniques we have used.},
	Author = {Helma van der Linden and Gerrit Boers and Huibert Tange and Jan Talmon and Arie Hasman},
	Doi = {https://doi.org/10.1016/S1386-5056(03)00032-7},
	Issn = {1386-5056},
	Journal = {International Journal of Medical Informatics},
	Keywords = {EPR, OMG HDTF, Archetypes, eXtreme Programming, Open source, OpenEMed},
	Note = {MIE 2002 Special Issue},
	Number = {2},
	Pages = {149 - 160},
	Title = {PropeR: a multi disciplinary EPR system},
	Url = {http://www.sciencedirect.com/science/article/pii/S1386505603000327},
	Volume = {70},
	Year = {2003},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1386505603000327},
	Bdsk-Url-2 = {https://doi.org/10.1016/S1386-5056(03)00032-7}}

@article{OLIVEIRA2014342,
	Abstract = {A common problem associated with ready-made software is that businesses are forced to alter some of their processes in order to fully utilize the product. On the other hand, custom-made applications are specifically designed to accommodate the needs of the client's activities, so the final product will be based on their input during the software development process, and will be fine-tuned to fit the way your business operates. With this approach, an information system designed towards the management of agricultural properties, the Agrifootprint system, was developed. The main goal of the Agrifootprint system was to develop and implement a Web-based Geographic Information System, based on Open-Source technology, with a custom-made, comprehensive and user-friendly geospatial system, in order to integrate and manage data from several small agriculture enterprises. The development process was based in Agile methodologies: SCRUM and Evolutionary Prototyping. This option for agile development process brought the establishment of a relationship of complicity between the customer and the development team due to more real understanding of the requirements raised and the system that will be produced from these.},
	Author = {Tiago H. Moreira de Oliveira and Marco Painho and V{\'\i}tor Santos and Ot{\'a}vio Sian and Andr{\'e} Barriguinha},
	Doi = {https://doi.org/10.1016/j.protcy.2014.10.100},
	Issn = {2212-0173},
	Journal = {Procedia Technology},
	Keywords = {Geographic Information Systems, Agricultural information systems, Module View Controller, Agile, SCRUM, Evolutionary Prototyping},
	Note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
	Pages = {342 - 354},
	Title = {Development of an Agricultural Management Information System based on Open-source Solutions},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212017314003272},
	Volume = {16},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212017314003272},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.protcy.2014.10.100}}

@article{HEDIN2005133,
	Abstract = {We find the extreme programming methodology highly suitable for introducing undergraduate students to software engineering. To be able to apply this methodology at a reasonable teaching cost for large student groups, we have developed two courses that work in tandem: a team programming course taken by more than 100 students, and a coaching course taken by around 25 students. In this paper we describe our view of how extreme programming fits into the software engineering curriculum, our approach to teaching it, and our experiences, based on two years of running these courses. Particularly important aspects of our set up include team coaching (by older students), fixed working hours, and colocation during development. Our experiences so far are very positive, and we see that students get a good basic understanding of the important concepts in software engineering, rooted in their own practical experience.},
	Author = {G{\"o}rel Hedin and Lars Bendix and Boris Magnusson},
	Doi = {https://doi.org/10.1016/j.jss.2003.09.026},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Note = {The new context for software engineering education and training},
	Number = {2},
	Pages = {133 - 146},
	Title = {Teaching extreme programming to large groups of students},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121203002930},
	Volume = {74},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121203002930},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2003.09.026}}

@article{SHRIVASTAVA2014417,
	Abstract = {There has been is a growing trend in software development through distributed agile approach and so, the study of risks in such environments becomes imperative. A number of studies have discussed about the problems faced by distributed agile teams. This study attempts to consolidate the existing studies on risks in distributed agile development. It helps in uncovering the areas of risk management in distributed agile, in which extensive work has been done and also presents the type of work that needs further consideration.},
	Author = {Suprika Vasudeva Shrivastava and Urvashi Rathod},
	Doi = {https://doi.org/10.1016/j.sbspro.2014.04.208},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {Distributed Software Development (DSD), Distributed Scrum, Distributed Extreme Programming, Global Software Development (GSD), Risks},
	Note = {International Conference on Trade, Markets and Sustainability (ICTMS-2013)},
	Pages = {417 - 424},
	Title = {Risks in Distributed Agile Development: A Review},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877042814031188},
	Volume = {133},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877042814031188},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2014.04.208}}

@article{ADOLPH20121269,
	Abstract = {Social factors are significant cost drivers for the process of software development. In this field study we generate a grounded theory of how people manage the process of software development. The main concern of engineers involved in the process of software development is getting the job done. To get the job done, people engage in a four-stage process of Reconciling Perspectives. Reconciling Perspectives represents an attempt to converge individuals' points of view or perspectives about a software project. The process emphasizes the importance of individuals' abilities to both reach out and engage in negotiations and create shelter from environmental noise to bring a software project to fruition.},
	Author = {Steve Adolph and Philippe Kruchten and Wendy Hall},
	Doi = {https://doi.org/10.1016/j.jss.2012.01.059},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Software engineering, Software team, Agile manifesto agile software development, Scrum, Shared mental model, Grounded theory},
	Note = {Special Issue: Agile Development},
	Number = {6},
	Pages = {1269 - 1286},
	Title = {Reconciling perspectives: A grounded theory of how people manage the process of software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000386},
	Volume = {85},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212000386},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.01.059}}

@article{MULLER20071460,
	Abstract = {Objective: Comparison of program defects caused by programmer pairs and solo developers. Design: Analysis of programs developed during two counter balanced experiments. Setting: Programming lab at University. Experimental units: 42 programs developed by computer science students participating in an extreme programming lab course. Main outcome measures: Programmer pairs make as many algorithmic mistakes but fewer expression mistakes than solo programmers. Results: The second result is significant on the 5% level. Conclusions: For simple problems, pair programming seems to lead to fewer mistakes than solo programming.},
	Author = {Matthias M. M{\"u}ller},
	Doi = {https://doi.org/10.1016/j.jss.2006.10.032},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Pair programming, Solo programming, Defect comparison, Eureka problems},
	Note = {Evaluation and Assessment in Software Engineering},
	Number = {9},
	Pages = {1460 - 1471},
	Title = {Do programmer pairs make different mistakes than solo programmers?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121206002974},
	Volume = {80},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121206002974},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2006.10.032}}

@article{LIN20121364,
	Abstract = {A layer-based method for rapid software development is presented in this paper. It follows the guidelines suggested by Extreme Programming (XP) that require highly expressive programming languages (i.e., Java) and CASE tools. As in XP, this method addresses rapid software development for small- or medium-sized projects. Further, for effective guidance on the development, it directs the construction of system components by imposing an architecture-based concept of layered specification and construction of these components through its activities. Since the method follows the guidelines suggested by XP and supports effective guidance by a layered development of architectural components, team productivities can be greatly enhanced with less (but effective) overheads on specification work. The method uses UML and Petri nets as its modeling tool; for illustration, an example application is presented that specifies and directs the development of a software system with business-oriented Internet services.},
	Author = {Lendy Lin and Weipang Yang and Jyhjong Lin},
	Doi = {https://doi.org/10.1016/j.camwa.2012.03.082},
	Issn = {0898-1221},
	Journal = {Computers & Mathematics with Applications},
	Keywords = {Software system, Development method, Layer-based, UML, Petri nets},
	Note = {Advanced Technologies in Computer, Consumer and Control},
	Number = {5},
	Pages = {1364 - 1375},
	Title = {A layer-based method for rapid software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0898122112002878},
	Volume = {64},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0898122112002878},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.camwa.2012.03.082}}

@article{GEORGE2004337,
	Abstract = {Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18% more functional black-box test cases. However, the TDD programmers took 16% more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports the perception that TDD has the potential for increasing the level of unit testing in the software industry.},
	Author = {Boby George and Laurie Williams},
	Doi = {https://doi.org/10.1016/j.infsof.2003.09.011},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software engineering, Test driven development, Extreme programming, Agile methodologies},
	Note = {Special Issue on Software Engineering, Applications, Practices and Tools from the ACM Symposium on Applied Computing 2003},
	Number = {5},
	Pages = {337 - 342},
	Title = {A structured experiment of test-driven development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584903002040},
	Volume = {46},
	Year = {2004},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584903002040},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2003.09.011}}

@article{ZHAO2014419,
	Abstract = {Most existing Fault Detection and Diagnostic (FDD) methods for chillers are primarily tested and evaluated in a controlled laboratory environment. The controlled laboratory environment is usually configured and installed to satisfy the requirements of the FDD methods and therefore contain rich data sets. However, some measurements required by the FDD methods may not be commonly available on the field chillers. Therefore, field demonstrations and commercialization of chiller FDD is a big challenge since many practical issues must be addressed. In this study, a decoupling-based FDD method which can deal with multiple simultaneous faults was fully implemented online and evaluated in the field test environment. A step by step process of implementing the evaluated FDD method for the real application is described to aid readers to apply and use the method. Several occurrences of multiple simultaneous faults were found on the test chiller. The field test results show that the decoupling-based FDD method has potential to be incorporated within commercial FDD products or embedded into the control system onboard the chiller to monitor the health of the chiller's operation.},
	Author = {Xinzhi Zhao and Mo Yang and Haorong Li},
	Doi = {https://doi.org/10.1016/j.enbuild.2014.01.003},
	Issn = {0378-7788},
	Journal = {Energy and Buildings},
	Keywords = {Chillers, Decoupling-based FDD, Decoupling features, Field test},
	Pages = {419 - 430},
	Title = {Field implementation and evaluation of a decoupling-based fault detection and diagnostic method for chillers},
	Url = {http://www.sciencedirect.com/science/article/pii/S0378778814000243},
	Volume = {72},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0378778814000243},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.enbuild.2014.01.003}}

@article{CHOI2009844,
	Abstract = {From a recent field survey with a group of professional pair programmers, it was revealed that the programmers perceive a partner's personality, cognitive style and communication skills as the top three factors that lead to prudent pair programming. Based on this finding, the factors personality and communication skills, along with gender were selected for an experiment to analyze if a partner's human, intrinsic values influence the pair programming experience, specifically in the levels of satisfaction, compatibility, communication, and confidence. A total of 128 students majoring in Management Information Systems, Information Systems, and Information Technology participated in the experiment. Of the 68 undergraduates, 40 were first-year students and 28 were juniors; the remaining 60 were Master's degree graduate students. The students were formed into a total of 64 pairs based on their personality, level of communication skills, and gender. A total of three visits were made. During the first two visits, a set of four programming problems was used in four programming sessions lasting 45min each; two were individual programming sessions and two were pair programming sessions. At the end of each visit, a questionnaire was administered and collected. The questionnaire results revealed that the various Myers--Briggs Type Indicator (MBTI) personality combinations did not significantly influence the levels of communication, satisfaction, confidence, and compatibility. The pairs that exhibited a high level of communication between partners did not necessarily experience a high level of satisfaction or exhibit compatibility between partners, nor did they have a high level of confidence regarding the finished product. The communication skill level seemed to have an impact on communication only. Similar to many previous gender-focused literatures, the same gender pairs did exhibit significantly higher levels of communication, satisfaction and compatibility than the mixed gender pairs. Within the same gender pairs, the female--female pairs showed a much higher level than the male--male pairs in those categories. Contrariwise, the same gender pairs did not show a significantly higher confidence level than the mixed gender pairs about their finished product.},
	Author = {Kyungsub Steve Choi and Fadi P. Deek and Il Im},
	Doi = {https://doi.org/10.1016/j.chb.2008.09.005},
	Issn = {0747-5632},
	Journal = {Computers in Human Behavior},
	Keywords = {Pair dynamics, Pair programming, Team programming, Extreme programming},
	Note = {Including the Special Issue: The Use of Support Devices in Electronic Learning Environments},
	Number = {4},
	Pages = {844 - 852},
	Title = {Pair dynamics in team collaboration},
	Url = {http://www.sciencedirect.com/science/article/pii/S0747563208001817},
	Volume = {25},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0747563208001817},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.chb.2008.09.005}}

@article{LEE2011527,
	Abstract = {This study investigates whether a visual programming environment called Etoys could enable teachers to create software applications meeting their own instructional needs. Twenty-four teachers who participated in the study successfully developed their own educational computer programs in the educational technology course employing cognitive apprenticeship and pair programming approaches as the primary instructional strategies. Two educational software programs created by the participating teachers were described in order to explain what they were trying to do using Etoys and how they accomplished their goals. The results of an anonymous survey evaluating the difficulty of and the attitude toward learning Etoys indicate that teachers enjoyed learning Etoys and would like to continue to use it in the future although they found it was slightly more difficult, compared to their self-evaluated computer skill. The strengths and weaknesses of Etoys, the difficult computer programming concepts, and the educational implications of Etoys programming were also discussed.},
	Author = {Young-Jin Lee},
	Doi = {https://doi.org/10.1016/j.compedu.2010.09.018},
	Issn = {0360-1315},
	Journal = {Computers & Education},
	Keywords = {Interactive learning environment, Media in education, Programming and programming languages, Teaching/learning strategies},
	Number = {2},
	Pages = {527 - 538},
	Title = {Empowering teachers to create educational software: A constructivist approach utilizing Etoys, pair programming and cognitive apprenticeship},
	Url = {http://www.sciencedirect.com/science/article/pii/S0360131510002812},
	Volume = {56},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0360131510002812},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.compedu.2010.09.018}}

@article{VONWANGENHEIM20132675,
	Abstract = {Due to the increasing use of agile methods, teaching SCRUM as an agile project management methodology has become more and more important. In order to teach students to be able to apply SCRUM in concrete situations, often educational (simulation) games are used. However, most of these games have been developed more for professional trainings than taking into consideration typical restrictions of university courses (such as, class duration and low financial resources for instructional materials). Therefore, we present a manual paper and pencil game to reinforce and teach the application of SCRUM in undergraduate computing programmes complementing theoretical lectures. The game has been developed following a systematic instructional design process and based on our teaching experience. It has been applied several times in two undergraduate project management courses. We evaluated motivation, user experience and the game's contribution to learning through case studies on Kirkpatrick's level one based on the perception of the students. First results indicate the potential of the game to contribute to the learning of SCRUM in an engaging way, keeping students immersed in the learning task. In this regard, the game offers a low-budget alternative to complement traditional instructional strategies for teaching SCRUM in the classroom.},
	Author = {Christiane Gresse von Wangenheim and Rafael Savi and Adriano Ferreti Borgatto},
	Doi = {https://doi.org/10.1016/j.jss.2013.05.030},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {SCRUM, Teaching, Game},
	Number = {10},
	Pages = {2675 - 2687},
	Title = {SCRUMIA---An educational game for teaching SCRUM in computing courses},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213001295},
	Volume = {86},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121213001295},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2013.05.030}}

@article{GENG201312,
	Abstract = {This paper considers a frequency-division duplex (FDD) two-way channel with channel estimation error, where channel gains are independent of each other. It derives the exact closed-form outage probability expressions in the FDD system with analog network coding (ANC) by use of probability theory. To provide more insights, an approximated version for the exact outage probability is also developed in the medium-to-high signal-to-noise ratio (SNR) region. The simulation results show that the derived exact outage probabilities match the results of Monte Carlo simulations in all SNR regions, while the approximated outage probabilities also approach the simulation results when the channel condition is good. It is interesting that ANC in the FDD two-way channel is proved to outperform that of in the time-division duplex (TDD) channel by the computer simulation.},
	Author = {Xuan GENG and Feng LIU and Chen HE},
	Doi = {https://doi.org/10.1016/S1005-8885(13)60021-7},
	Issn = {1005-8885},
	Journal = {The Journal of China Universities of Posts and Telecommunications},
	Keywords = {FDD, outage probability, two-way channel, ANC},
	Number = {2},
	Pages = {12 - 18},
	Title = {Outage analysis of ANC in the FDD two-way fading channel with channel estimation error},
	Url = {http://www.sciencedirect.com/science/article/pii/S1005888513600217},
	Volume = {20},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1005888513600217},
	Bdsk-Url-2 = {https://doi.org/10.1016/S1005-8885(13)60021-7}}

@article{PAASIVAARA20141556,
	Abstract = {Context
Communities of practice---groups of experts who share a common interest or topic and collectively want to deepen their knowledge---can be an important part of a successful lean and agile adoption in particular in large organizations.
Objective
In this paper, we present a study on how a large organization within Ericsson with 400 persons in 40 Scrum teams at three sites adopted the use of Communities of Practice (CoP) as part of their transformation from a traditional plan-driven organization to lean and agile.
Methods
We collected data by 52 semi-structured interviews on two sites, and longitudinal non-participant observation of the transformation during over 20 site visits over a period of two years.
Results
The organization had over 20 CoPs, gathering weekly, bi-weekly or on a need basis. CoPs had several purposes including knowledge sharing and learning, coordination, technical work, and organizational development. Examples of CoPs include Feature Coordination CoPs to coordinate between teams working on the same feature, a Coaching CoP to discuss agile implementation challenges and successes and to help lead the organizational continuous improvement, an end-to-end CoP to remove bottlenecks from the flow, and Developers CoPs to share good development practices. Success factors of well-functioning CoPs include having a good topic, passionate leader, proper agenda, decision making authority, open community, supporting tools, suitable rhythm, and cross-site participation when needed. Organizational support include creating a supportive atmosphere and providing a suitable infrastructure for CoPs.
Conclusions
In the case organization, CoPs were initially used to support the agile transformation, and as part of the distributed Scrum implementation. As the transformation progressed, the CoPs also took on the role of supporting continuous organizational improvements. CoPs became a central mechanism behind the success of the large-scale agile implementation in the case organization that helped mitigate some of the most pressing problems of the agile transformation.},
	Author = {Maria Paasivaara and Casper Lassenius},
	Doi = {https://doi.org/10.1016/j.infsof.2014.06.008},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Communities of practice, Large-scale agile software development, Scaling agile},
	Note = {Special issue: Human Factors in Software Development},
	Number = {12},
	Pages = {1556 - 1577},
	Title = {Communities of practice in a large distributed agile software development organization -- Case Ericsson},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914001475},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584914001475},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2014.06.008}}

@article{MOE2010480,
	Abstract = {Context
Software development depends significantly on team performance, as does any process that involves human interaction.
Objective
Most current development methods argue that teams should self-manage. Our objective is thus to provide a better understanding of the nature of self-managing agile teams, and the teamwork challenges that arise when introducing such teams.
Method
We conducted extensive fieldwork for 9months in a software development company that introduced Scrum. We focused on the human sensemaking, on how mechanisms of teamwork were understood by the people involved.
Results
We describe a project through Dickinson and McIntyre's teamwork model, focusing on the interrelations between essential teamwork components. Problems with team orientation, team leadership and coordination in addition to highly specialized skills and corresponding division of work were important barriers for achieving team effectiveness.
Conclusion
Transitioning from individual work to self-managing teams requires a reorientation not only by developers but also by management. This transition takes time and resources, but should not be neglected. In addition to Dickinson and McIntyre's teamwork components, we found trust and shared mental models to be of fundamental importance.},
	Author = {Nils Brede Moe and Torgeir Dings{\o}yr and Tore Dyb{\aa}},
	Doi = {https://doi.org/10.1016/j.infsof.2009.11.004},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Agile software development, Scrum, Software engineering, Teamwork, Empirical software engineering, Case study},
	Note = {TAIC-PART 2008},
	Number = {5},
	Pages = {480 - 491},
	Title = {A teamwork model for understanding an agile team: A case study of a Scrum project},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909002043},
	Volume = {52},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584909002043},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2009.11.004}}

@article{PANCUR2011557,
	Abstract = {Context
Test-driven development is an approach to software development, where automated tests are written before production code in highly iterative cycles. Test-driven development attracts attention as well as followers in professional environment; however empirical evidence of its superiority regarding its effect on productivity, code and tests compared to test-last development is still fairly limited. Moreover, it is not clear if the supposed benefits come from writing tests before code or maybe from high iterativity/short development cycles.
Objective
This paper describes a family of controlled experiments comparing test-driven development to micro iterative test-last development with emphasis on productivity, code properties (external quality and complexity) and tests (code coverage and fault-finding capabilities).
Method
Subjects were randomly assigned to test-driven and test-last groups. Controlled experiments were conducted for two years, in an academic environment and in different developer contexts (pair programming and individual programming contexts). Number of successfully implemented stories, percentage of successful acceptance tests, McCabe's code complexity, code coverage and mutation score indicator were measured.
Results
Experimental results and their selective meta-analysis show no statistically significant differences between test-driven development and iterative test-last development regarding productivity (2(6)=4.799, p=1.0, r=.107, 95% CI (confidence interval): .149 to .349), code complexity (2(6)=8.094, p=.46, r=.048, 95% CI: .254 to .341), branch coverage (2(6)=13.996, p=.059, r=.182, 95% CI: .081 to .421), percentage of acceptance tests passed (one experiment, Mann--Whitney U=125.0, p=.98, r=.066) and mutation score indicator (2(4)=3.807, p=.87, r=.128, 95% CI: .162 to .398).
Conclusion
According to our findings, the benefits of test-driven development compared to iterative test-last development are small and thus in practice relatively unimportant, although effects are positive. There is an indication of test-driven development endorsing better branch coverage, but effect size is considered small.},
	Author = {Matja{\v z} Pan{\v c}ur and Mojca Ciglari{\v c}},
	Doi = {https://doi.org/10.1016/j.infsof.2011.02.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Empirical software engineering, Controlled experiment, Test-driven development, Iterative test-last development},
	Note = {Special Section: Best papers from the APSEC},
	Number = {6},
	Pages = {557 - 573},
	Title = {Impact of test-driven development on productivity, code and tests: A controlled experiment},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911000346},
	Volume = {53},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584911000346},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2011.02.002}}

@article{PEDRYCZ2011739,
	Abstract = {Modern software development relies on collaborative work as a means for sharing knowledge, distributing tasks and responsibilities, reducing risk of failures, and increasing the overall quality of the software product. Such objectives are achieved with a continuous share of the programmers' daily working life that inevitably influences the programmers' job satisfaction. One of the major challenges in process management is to determine the causes of this satisfaction. Traditional research models job satisfaction with social aspects of collaborative work like communication, work sustainability, and work environment. This study reflects on existing models of job satisfaction in collaborative environments, creates one for modern software development processes, and validates it with a retrospective comparative survey run on a sample of 108 respondents. In addition, the work investigates the impact on job satisfaction and its model of the agile practice of Pair Programming that pushes job sharing to the extreme. With this intent, the questionnaire also collected feedback from pair programmers whose responses were used for a comparative analysis. The results demonstrate that Pair Programming has actually a strong positive effect on satisfaction, work sustainability, and communication.},
	Author = {Witold Pedrycz and Barbara Russo and Giancarlo Succi},
	Doi = {https://doi.org/10.1016/j.jss.2010.12.018},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Job satisfaction, Pair programming, Log linear model},
	Number = {5},
	Pages = {739 - 752},
	Title = {A model of job satisfaction for collaborative development processes},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121210003407},
	Volume = {84},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121210003407},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2010.12.018}}

@article{OTERO201272,
	Abstract = {The objective of the present research was to investigate the adsorption of non-ionic pesticide S-Metolachlor on two different organohydrotalcites (OHTs): one intercalated with dodecylsulfate (HT-DDS) and the other one with tetradecanedioate (HT-TDD) anions, both of them obtained by the coprecipitation method. The adsorbents and the adsorption products were characterized by X-ray diffraction and FT-IR spectroscopy. The results indicated that the adsorption of S-Metolachlor on the organohydrotalcite HT-TDD was higher than that on HT-DDS, but desorption was lower. The amount of S-Metolachlor adsorbed increased with temperature, especially for HT-TDD. S-Metolachlor desorption from HT-DDS and HT-TDD was characterized using two experimental procedures, which indicated the higher reversibility of S-Metolachlor adsorption from HT-DDS compared to HT-TDD. The results suggest the possibility to use both organohydrotalcites uptaking S-Metolachlor from contaminated water.},
	Author = {R. Otero and J.M. Fern{\'a}ndez and M.A. Ulibarri and R. Celis and F. Bruna},
	Doi = {https://doi.org/10.1016/j.clay.2012.04.022},
	Issn = {0169-1317},
	Journal = {Applied Clay Science},
	Keywords = {S-Metolachlor, Organohydrotalcite, Dodecylsufate anion, Tetradecanedioate anion, Adsorption, Water treatmen},
	Pages = {72 - 79},
	Title = {Adsorption of non-ionic pesticide S-Metolachlor on layered double hydroxides intercalated with dodecylsulfate and tetradecanedioate anions},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169131712001287},
	Volume = {65-66},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0169131712001287},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.clay.2012.04.022}}

@article{JOUGLA2010350,
	Abstract = {The aim of this study was to determine the effects of active vs. passive recovery on performance of a rugby-specific intermittent test in rugby union players. Seven male rugby players (20.6$\pm$0.5 yrs; 181.9$\pm$10.0cm; 94.5$\pm$12.8kg) performed in random order, over two separate sessions, a specific repeated-sprint rugby test, the Narbonne test (64 consecutive actions: 1, scrummaging; 2, agility sprinting; 3, tackling; 4, straight sprinting) with 30s of passive or active recovery (running at 50% of maximal aerobic speed). The Narbonne tests were completed before (pre-test) and after (post-test) a 30-min rugby match. During the Narbonne test, scrum forces, agility and sprint times, heart rate and rate of perceived exertion were measured. Scrum forces were lower in active (74.9$\pm$13.4kg) than in passive recovery (90.4$\pm$20.9kg), only during the post-test (p<0.05). Fatigue index (%) (p<0.05) and total sprint time (s) (p<0.01) were significantly greater in active than in passive recovery, both during the pre-test (11.5$\pm$5.7% vs. 6.7$\pm$4.5% and 18.1$\pm$1.3s vs. 16.9$\pm$0.9s) and the post-test (7.3$\pm$3.3% vs. 4.3$\pm$1.5% and 18.3$\pm$1.6s vs. 16.9$\pm$1.1s). Consequently, the results indicated that passive recovery enabled better performance during the Narbonne test. However, it is obviously impractical to suggest that players should stand still during and following repeated-sprint bouts: the players have to move to ensure they have taken an optimal position.},
	Author = {A. Jougla and J.P. Micallef and D. Mottet},
	Doi = {https://doi.org/10.1016/j.jsams.2009.04.004},
	Issn = {1440-2440},
	Journal = {Journal of Science and Medicine in Sport},
	Keywords = {Narbonne test, Scrum force, Sprint time, Heart rate, Rate of perceived exertion},
	Number = {3},
	Pages = {350 - 355},
	Title = {Effects of active vs. passive recovery on repeated rugby-specific exercises},
	Url = {http://www.sciencedirect.com/science/article/pii/S1440244009001017},
	Volume = {13},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1440244009001017},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jsams.2009.04.004}}

@article{RABERTO2001319,
	Abstract = {This paper introduces an agent-based artificial financial market in which heterogeneous agents trade one single asset through a realistic trading mechanism for price formation. Agents are initially endowed with a finite amount of cash and a given finite portfolio of assets. There is no money-creation process; the total available cash is conserved in time. In each period, agents make random buy and sell decisions that are constrained by available resources, subject to clustering, and dependent on the volatility of previous periods. The model proposed herein is able to reproduce the leptokurtic shape of the probability density of log price returns and the clustering of volatility. Implemented using extreme programming and object-oriented technology, the simulator is a flexible computational experimental facility that can find applications in both academic and industrial research projects.},
	Author = {Marco Raberto and Silvano Cincotti and Sergio M. Focardi and Michele Marchesi},
	Doi = {https://doi.org/10.1016/S0378-4371(01)00312-0},
	Issn = {0378-4371},
	Journal = {Physica A: Statistical Mechanics and its Applications},
	Keywords = {Artificial financial markets, Heterogeneous agents, Financial time series, Econophysics},
	Note = {Application of Physics in Economic Modelling},
	Number = {1},
	Pages = {319 - 327},
	Title = {Agent-based simulation of a financial market},
	Url = {http://www.sciencedirect.com/science/article/pii/S0378437101003120},
	Volume = {299},
	Year = {2001},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0378437101003120},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0378-4371(01)00312-0}}

@article{ACUNA2009627,
	Abstract = {This article analyses the relationships between personality, team processes, task characteristics, product quality and satisfaction in software development teams. The data analysed here were gathered from a sample of 35 teams of students (105 participants). These teams applied an adaptation of an agile methodology, eXtreme Programming (XP), to develop a software product. We found that the teams with the highest job satisfaction are precisely the ones whose members score highest for the personality factors agreeableness and conscientiousness. The satisfaction levels are also higher when the members can decide how to develop and organize their work. On the other hand, the level of satisfaction and cohesion drops the more conflict there is between the team members. Finally, the teams exhibit a significant positive correlation between the personality factor extraversion and software product quality.},
	Author = {Silvia T. Acu{\~n}a and Marta G{\'o}mez and Natalia Juristo},
	Doi = {https://doi.org/10.1016/j.infsof.2008.08.006},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Personality factors, Team processes, Task characteristics, Software quality, Job satisfaction},
	Number = {3},
	Pages = {627 - 639},
	Title = {How do personality, team processes and task characteristics relate to job satisfaction and software quality?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584908001080},
	Volume = {51},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584908001080},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2008.08.006}}

@article{DONG2014197,
	Abstract = {Although energy-efficient building technologies are emerging, a key challenge is how to effectively maintain building energy performance over the evolving lifecycle of the building. Field experience shows that energy savings of 5--30% are typically achievable simply by applying energy Fault Detection and Diagnostics (FDD) and correcting faults diagnosed in buildings. Model-based FDD in buildings is a challenging task, not only because the task itself is difficult, but also because the workflow and information exchange behind the task is very complex and error prone. This complexity arises from several aspects. Firstly, creating a baseline building energy performance model suitable for FDD is both time and labor consuming. Secondly, the FDD module typically has its own ad-hoc platform, and the integration of this platform with the existing Building Energy Management System (BEMS) is technically challenging due to the incompatible interoperability. Finally, the information exchange itself is complex due to the existence of multiple functioning modules to make FDD workflow happen. To perform an efficient and effective FDD with the BEMS in buildings, information is needed to flow among an as-built building static information module, a building energy performance simulation module, a building operational data acquisition module and a FDD module. In such a complex process, it is challenging to ensure the information integrity and consistence. In this paper, we propose a Building Information Modeling (BIM) enabled information infrastructure for FDD, which streamlines the information exchange process and therefore has the potential to improve the efficiency of similar works in practice. The proposed information infrastructure was deployed and implemented in a real building for a FDD case study.},
	Author = {Bing Dong and Zheng O'Neill and Zhengwei Li},
	Doi = {https://doi.org/10.1016/j.autcon.2014.04.007},
	Issn = {0926-5805},
	Journal = {Automation in Construction},
	Keywords = {Building Information Modeling, Fault Detection and Diagnostics, Information infrastructure, Real-time implementation, Data schema},
	Pages = {197 - 211},
	Title = {A BIM-enabled information infrastructure for building energy Fault Detection and Diagnostics},
	Url = {http://www.sciencedirect.com/science/article/pii/S0926580514000946},
	Volume = {44},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0926580514000946},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.autcon.2014.04.007}}

@article{LITVINOV2006619,
	Abstract = {Kinetics of interstitial oxygen loss and oxygen-related thermal double donor (TDD) generation upon heat treatments of Ge:O crystals at 350$\,^{\circ}$C have been studied. The TDD concentration (NTDD) was derived from Hall effect measurements in the temperature range 77--400K. The bistability of the first TDD species was taken into account. The interstitial oxygen concentration ([Oi]) in the crystals was determined from measurements of the intensity of the infrared absorption band at 855cm1 at room temperature with the use of a recently obtained calibration coefficient CO=1.051017cm2. From an analysis of the [Oi](t) and NTDD(t) kinetics a confirmation of recent suggestions about faster diffusivity of small oxygen clusters compared to the diffusivity of single interstitial oxygen atoms was obtained. Average numbers (N) of oxygen atoms lost per TDD species created, N=[Oi]/NTDD, were calculated at different stages of the TDD generation. The obtained values of N are consistent with those expected in accordance with the recent models of the TDD structure. In particular, an average number of oxygen atoms per TDD species was about 5 at initial stages of the heat treatment when the first members (the TDD2 and TDD3 species) of the TDD family were dominant. N was found to be about 10 after extended anneals at 350$\,^{\circ}$C when the TDD6 and TDD7 species were dominant.},
	Author = {V.V. Litvinov and L.I. Murin and V.P. Markevich and A.R. Peaker and J.L. Lindstr{\"o}m},
	Doi = {https://doi.org/10.1016/j.mssp.2006.08.011},
	Issn = {1369-8001},
	Journal = {Materials Science in Semiconductor Processing},
	Keywords = {Germanium, Oxygen, Thermal donors},
	Note = {Proceedings of Symposium T E-MRS 2006 Spring Meeting on Germanium based semiconductors from materials to devices},
	Number = {4},
	Pages = {619 - 624},
	Title = {Oxygen loss and thermal double donor formation in germanium},
	Url = {http://www.sciencedirect.com/science/article/pii/S136980010600151X},
	Volume = {9},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S136980010600151X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.mssp.2006.08.011}}

@article{MULLER2005166,
	Abstract = {This paper reports on two controlled experiments comparing pair programming with single developers who are assisted by an additional anonymous peer code review phase. The experiments were conducted in the summer semester 2002 and 2003 at the University of Karlsruhe with 38 computer science students. Instead of comparing pair programming to solo programming this study aims at finding a technique by which a single developer produces similar program quality as programmer pairs do but with moderate cost. The study has one major finding concerning the cost of the two development methods. Single developers are as costly as programmer pairs, if both programmer pairs and single developers with an additional review phase are forced to produce programs of similar level of correctness. In conclusion, programmer pairs and single developers become interchangeable in terms of development cost. As this paper reports on the results of small development tasks the comparison could not take into account long time benefits of either technique.},
	Author = {Matthias M. M{\"u}ller},
	Doi = {https://doi.org/10.1016/j.jss.2004.12.019},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Pair programming, Peer reviews, Empirical software engineering, Controlled experiment},
	Number = {2},
	Pages = {166 - 179},
	Title = {Two controlled experiments concerning the comparison of pair programming to peer review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121205000038},
	Volume = {78},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121205000038},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2004.12.019}}

@article{BLOM20101511,
	Abstract = {This article discusses the question if Scrum and XP is well suited for development of Computational Science and Engineering (CSE) software. The reason for choosing Scrum and XP is based on two facts. Firstly, CSE developers are not used to formal processes for software development and Scrum and XP are informal in nature, hence easier to adopt. Secondly CSE projects are often once-off and aimed at producing scientific results rather than commercial software, so the overhead of a process or methodology needs to be kept at a minimum. Scrum and XP are focused on producing software, thus keeping everything else, i.e. the overhead to a minimum, a fact that benefits CSE development. In this article, the characteristics of Scrum and XP in particular and agile development in general are evaluated against the needs of CSE developers. The results show that almost all the key points in both methodologies are well suited for CSE development and that agile development in general is a good match with CSE.},
	Author = {Martin Blom},
	Doi = {https://doi.org/10.1016/j.procs.2010.04.168},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Scrum, XP, Computational science and engineering},
	Note = {ICCS 2010},
	Number = {1},
	Pages = {1511 - 1517},
	Title = {Is Scrum and XP suitable for CSE Development?},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050910001699},
	Volume = {1},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050910001699},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2010.04.168}}

@article{COMAN2014124,
	Abstract = {Considering that pair programming has been extensively studied for more than a decade, it can seem quite surprising that there is such a lack of consensus on both its best use and its benefits. We argue that pair programming is not a replacement of usual developer interactions, but rather a formalization and enhancement of naturally occurring interactions. Consequently, we study and classify a broader range of developer interactions, evaluating them for type, purpose and patterns of occurrence, with the aim to identify situations in which pair programming is likely to be truly needed and thus most beneficial. We study the concrete pair programming practices in both academic and industrial settings. All interactions between teammates were recorded as backup behavior activities. In each of these two projects, developers were free to interact when needed. All team interactions were self-recorded by the teammates. The analysis of the interaction tokens shows two salient features: solo work is an important component of teamwork and team interactions have two main purposes, namely cooperation and collaboration. Cooperative backup behavior occurs when a developer provides help to a teammate. Collaborative backup behavior occurs when the teammates are sharing the same goal toward solving an issue. We found that collaborative backup behavior, which occurred much less often, is close to the formal definition of pair programming. This study suggests that mandatory pair programming may be less efficient in organizations where solo work could be done and when some interactions are for cooperative activities. Based on these results, we discussed the potential implications concerning the best use of pair programming in practice, a more effective evaluation of its use, its potential benefits and emerging directions of future research.},
	Author = {Irina D. Coman and Pierre N. Robillard and Alberto Sillitti and Giancarlo Succi},
	Doi = {https://doi.org/10.1016/j.jss.2013.12.037},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Backup-behavior, Pair-programming, Field study},
	Pages = {124 - 134},
	Title = {Cooperation, collaboration and pair-programming: Field studies on backup behavior},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121214000107},
	Volume = {91},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121214000107},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2013.12.037}}

@article{TURNU2006610,
	Abstract = {The goal of this work is to study the effects of the adoption of agile practices on open source development. In particular, we started to evaluate the effects of TDD (Test Driven Development) since it is easer to apply in a distributed environment than most other agile practices. In order to reach this goal we used the simulation modeling approach. We developed a simulation model of open source software development process. The model was tuned using data from a real FLOSS project: Apache HTTP Server. To introduce the TDD practice in our FLOSS simulation model, we made some assumptions based on empirical results. The two FLOSS development models (nonTDD and TDD) were compared. The one incorporating the agile practice yields better results in terms of code quality.},
	Author = {Ivana Turnu and Marco Melis and Alessandra Cau and Alessio Setzu and Giulio Concas and Katiuscia Mannaro},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.06.005},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Software process simulation, Open source, Extreme programming, Test driven development},
	Note = {Agile Methodologies for Software Production},
	Number = {11},
	Pages = {610 - 618},
	Title = {Modeling and simulation of open source development using an agile practice},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106000634},
	Volume = {52},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106000634},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.06.005}}

@article{CHOUDHARI2012761,
	Abstract = {Software is developed with prior requirements and it is maintained continuously with rapid progresses in domain, technology, economy, and other fields. Software maintenance is vital and it requires more efforts and resources than development phase. There are fewer methods available for maintenance effort estimation. An iterative maintenance life cycle using extreme programming is a model for software maintenance based on agile methodology that uses RC stories as requirement artifacts. Agile methodology uses analogy and expert opinion based estimation techniques such as planning poker. But in software maintenance, historical data and experts may not be present or accurate. Therefore, a heuristic method is required for the calculation of maintenance efforts. In this paper, we propose a Software Maintenance Effort Estimation Model (SMEEM) for software maintenance estimation. The SMEEM technique uses story points to calculate the volume of maintenance and value adjustment factors that are affecting story points for effort estimation. The proposed model will be illustrated with various types of maintenance projects. The model generates accurate and effective results as compared to other software maintenance effort estimation models. This model is applicable only for agile and extreme programming based maintenance environment.},
	Author = {Jitender Choudhari and Ugrasen Suman},
	Doi = {https://doi.org/10.1016/j.protcy.2012.05.124},
	Issn = {2212-0173},
	Journal = {Procedia Technology},
	Keywords = {Story point, software maintenance, RC story, maintenance project effort estimation, SMEEM},
	Note = {2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012},
	Pages = {761 - 765},
	Title = {Story Points Based Effort Estimation Model for Software Maintenance},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212017312004033},
	Volume = {4},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212017312004033},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.protcy.2012.05.124}}

@article{DEOMELO2013412,
	Abstract = {Context
The management of software development productivity is a key issue in software organizations, where the major drivers are lower cost and shorter time-to-market. Agile methods, including Extreme Programming and Scrum, have evolved as ``light'' approaches that simplify the software development process, potentially leading to increased team productivity. However, little empirical research has examined which factors do have an impact on productivity and in what way, when using agile methods.
Objective
Our objective is to provide a better understanding of the factors and mediators that impact agile team productivity.
Method
We have conducted a multiple-case study for 6months in three large Brazilian companies that have been using agile methods for over 2years. We have focused on the main productivity factors perceived by team members through interviews, documentation from retrospectives, and non-participant observation.
Results
We developed a novel conceptual framework, using thematic analysis to understand the possible mechanisms behind such productivity factors. Agile team management was found to be the most influential factor in achieving agile team productivity. At the intra-team level, the main productivity factors were team design (structure and work allocation) and member turnover. At the inter-team level, the main productivity factors were how well teams could be effectively coordinated by proper interfaces and other dependencies and avoiding delays in providing promised software to dependent teams.
Conclusion
Teams should be aware of the influence and magnitude of turnover, which has been shown negative for agile team productivity. Team design choices remain an important factor impacting team productivity, even more pronounced on agile teams that rely on teamwork and people factors. The intra-team coordination processes must be adjusted to enable productive work by considering priorities and pace between teams. Finally, the revised conceptual framework for agile team productivity supports further tests through confirmatory studies.},
	Author = {Claudia de O. Melo and Daniela S. Cruzes and Fabio Kon and Reidar Conradi},
	Doi = {https://doi.org/10.1016/j.infsof.2012.09.004},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Agile software development, Team productivity factors, Team management, Thematic analysis, Industrial case studies},
	Note = {Special Section: Component-Based Software Engineering (CBSE), 2011},
	Number = {2},
	Pages = {412 - 427},
	Title = {Interpretative case studies on agile team productivity and management},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912001875},
	Volume = {55},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584912001875},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2012.09.004}}

@article{POZOGARCIA2013818,
	Abstract = {The performance of a modern industrial plant can be severely affected by the performance of its key devices, such as valves. In particular, valve stiction can cause poor performance in control loops and can consequently lower the efficiency of the plant and the quality of the product. This paper presents an integrated FDD system for valve stiction which employs various FDD methods in a parallel configuration. A reliability index was integrated into each method in order to estimate their degree of influence in the final diagnosis of the system. Each method and the integrated system were tested using industrial data.},
	Author = {Octavio Pozo Garcia and Vesa-Matti Tikkala and Alexey Zakharov and Sirkka-Liisa J{\"a}ms{\"a}-Jounela},
	Doi = {https://doi.org/10.1016/j.conengprac.2013.02.014},
	Issn = {0967-0661},
	Journal = {Control Engineering Practice},
	Keywords = {Fault detection and diagnosis, Valve, Oscillations, Shape analysis, Stiction, Industrial application},
	Number = {6},
	Pages = {818 - 828},
	Title = {Integrated FDD system for valve stiction in a paperboard machine},
	Url = {http://www.sciencedirect.com/science/article/pii/S0967066113000324},
	Volume = {21},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0967066113000324},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.conengprac.2013.02.014}}

@article{HANNAY20091110,
	Abstract = {Several experiments on the effects of pair versus solo programming have been reported in the literature. We present a meta-analysis of these studies. The analysis shows a small significant positive overall effect of pair programming on quality, a medium significant positive overall effect on duration, and a medium significant negative overall effect on effort. However, between-study variance is significant, and there are signs of publication bias among published studies on pair programming. A more detailed examination of the evidence suggests that pair programming is faster than solo programming when programming task complexity is low and yields code solutions of higher quality when task complexity is high. The higher quality for complex tasks comes at a price of considerably greater effort, while the reduced completion time for the simpler tasks comes at a price of noticeably lower quality. We conclude that greater attention should be given to moderating factors on the effects of pair programming.},
	Author = {Jo E. Hannay and Tore Dyb{\aa} and Erik Arisholm and Dag I.K. Sj{\o}berg},
	Doi = {https://doi.org/10.1016/j.infsof.2009.02.001},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Pair programming, Evidence-based software engineering, Systematic review, Meta-analysis, Fixed effects, Random effects},
	Note = {Special Section: Software Engineering for Secure Systems},
	Number = {7},
	Pages = {1110 - 1122},
	Title = {The effectiveness of pair programming: A meta-analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584909000123},
	Volume = {51},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584909000123},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2009.02.001}}

@article{TARHAN2014477,
	Abstract = {Context
Although Agile software development models have been widely used as a base for the software project life-cycle since 1990s, the number of studies that follow a sound empirical method and quantitatively reveal the effect of using these models over Traditional models is scarce.
Objective
This article explains the empirical method of and the results from systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process adapted in two projects of a middle-size, telecommunication software development company. The Incremental Process is an adaption of the Waterfall Model whereas the newly introduced Agile Process is a combination of the Unified Software Development Process, Extreme Programming, and Scrum.
Method
The method followed to perform the analyses and comparison is benefited from the combined use of qualitative and quantitative methods. It utilizes; GQM Approach to set measurement objectives, CMMI as the reference model to map the activities of the software development processes, and a pre-defined assessment approach to verify consistency of process executions and evaluate measure characteristics prior to quantitative analysis.
Results
The results of the comparison showed that the Agile Process had performed better than the Incremental Process in terms of productivity (79%), defect density (57%), defect resolution effort ratio (26%), Test Execution V&V Effectiveness (21%), and effort prediction capability (4%). These results indicate that development performance and product quality achieved by following the Agile Process was superior to those achieved by following the Incremental Process in the projects compared.
Conclusion
The acts of measurement, analysis, and comparison enabled comprehensive review of the two development processes, and resulted in understanding their strengths and weaknesses. The comparison results constituted objective evidence for organization-wide deployment of the Agile Process in the company.},
	Author = {Ayca Tarhan and Seda Gunes Yilmaz},
	Doi = {https://doi.org/10.1016/j.infsof.2013.12.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Empirical method, Quantitative analysis, Qualitative analysis, Software measurement, Process performance, Agile development},
	Note = {Performance in Software Development},
	Number = {5},
	Pages = {477 - 494},
	Title = {Systematic analyses and comparison of development performance and product quality of Incremental Process and Agile Process},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913002310},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584913002310},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2013.12.002}}

@article{VANWAARDENBURG20132154,
	Abstract = {Context
While renowned agile methods like XP and Scrum were initially intended for projects with small teams, traditional enterprise environments, i.e. environments where plan-driven development is prevalent, have also become attracted by the promises of a faster time to market through agility. Agile software development methods emphasize lightweight software development. Projects within enterprise environments, however, are typically confronted with a large and complex IT landscape, where mission-critical information is at play whose criticality requires prudence regarding design and development. In many an organization, both approaches are used simultaneously.
Objective
Find out which challenges the co-existence of agile methods and plan-driven development brings, and how organizations deal with those challenges.
Method
We present a grounded theory of the challenges of using agile methods in traditional enterprise environments, based on a Grounded Theory research involving 21 agile practitioners from two large enterprise organizations in the Netherlands.
Results
We organized the challenges under two factors: Increased landscape complexity and Lack of business involvement. For both factors, we identify successful mitigation strategies. These mitigation strategies concern the communication between the agile and traditional part of the organization, and the timing of that communication.
Conclusion
Agile practices can coexist with plan-driven development. One should, however, keep in mind the context and take actions to mitigate the challenges incurred.},
	Author = {Guus van Waardenburg and Hans van Vliet},
	Doi = {https://doi.org/10.1016/j.infsof.2013.07.012},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Agile development, Enterprise environment, Grounded theory},
	Number = {12},
	Pages = {2154 - 2171},
	Title = {When agile meets the enterprise},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584913001584},
	Volume = {55},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584913001584},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2013.07.012}}

@article{DAMM20053,
	Abstract = {This paper identifies and presents an approach to software component-level testing that in a cost effective way can move defect detection earlier in the development process. A department at Ericsson AB introduced a test automation tool for component-level testing in two projects together with the concept test-driven development (TDD), a practice where the test code is written before the product code. The implemented approach differs from how TDD is used in Extreme Programming (XP) in that the tests are written for components exchanging XMLs instead of writing tests for every method in every class. This paper describes the implemented test automation tool, how test-driven development was implemented with the tool, and experiences from the implementation. Preliminary results indicate that the concept decreases the development lead-time significantly.},
	Author = {Lars-Ola Damm and Lars Lundberg and David Olsson},
	Doi = {https://doi.org/10.1016/j.entcs.2004.02.090},
	Issn = {1571-0661},
	Journal = {Electronic Notes in Theoretical Computer Science},
	Keywords = {Software test automation, test-driven development, component testing, unit testing, test tool},
	Note = {Proceedings of the International Workshop on Test and Analysis of Component Based Systems (TACoS 2004)},
	Pages = {3 - 15},
	Title = {Introducing Test Automation and Test-Driven Development: An Experience Report},
	Url = {http://www.sciencedirect.com/science/article/pii/S1571066104052739},
	Volume = {116},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1571066104052739},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.entcs.2004.02.090}}

@article{DYBA2008833,
	Abstract = {Agile software development represents a major departure from traditional, plan-based approaches to software engineering. A systematic review of empirical studies of agile software development up to and including 2005 was conducted. The search strategy identified 1996 studies, of which 36 were identified as empirical studies. The studies were grouped into four themes: introduction and adoption, human and social factors, perceptions on agile methods, and comparative studies. The review investigates what is currently known about the benefits and limitations of, and the strength of evidence for, agile methods. Implications for research and practice are presented. The main implication for research is a need for more and better empirical studies of agile software development within a common research agenda. For the industrial readership, the review provides a map of findings, according to topic, that can be compared for relevance to their own settings and situations.},
	Author = {Tore Dyb{\aa} and Torgeir Dings{\o}yr},
	Doi = {https://doi.org/10.1016/j.infsof.2008.01.006},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Empirical software engineering, Evidence-based software engineering, Systematic review, Research synthesis, Agile software development, XP, Extreme programming, Scrum},
	Number = {9},
	Pages = {833 - 859},
	Title = {Empirical studies of agile software development: A systematic review},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584908000256},
	Volume = {50},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584908000256},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2008.01.006}}

@article{PRETSCHNER2004315,
	Abstract = {The spiraling nature of evolutionary software development processes produces executable parts of the system at the end of each loop. It is argued that these parts should consist not only of programming language code, but of executable graphical system models. As a main benefit of the use of more abstract, yet formal, modeling languages, a method for model based test sequence generation for reactive systems on the grounds of Constraint Logic Programming as well as its implementation in the CASE tool AutoFocus is presented.},
	Author = {Alexander Pretschner and Heiko L{\"o}tzbeyer and Jan Philipps},
	Doi = {https://doi.org/10.1016/S0164-1212(03)00076-1},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Cleanroom SW engineering, Constraint logic programming, Extreme programming, Incremental development, Rapid prototyping, Reactive systems, Test case generation},
	Note = {Rapid system prototyping},
	Number = {3},
	Pages = {315 - 329},
	Title = {Model based testing in incremental system development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121203000761},
	Volume = {70},
	Year = {2004},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121203000761},
	Bdsk-Url-2 = {https://doi.org/10.1016/S0164-1212(03)00076-1}}

@article{EDINGRIMHEDEN2013967,
	Abstract = {This paper presents a study of the integration of agile methods into mechatronics design education, as performed at KTH Royal Institute of Technology. The chosen method, Scrum, and the context of the studied capstone course are presented. With the integration of Scrum into the capstone projects, an educational favorable alternative is identified, to previously used design methodologies such as more traditional stage-gate methods as the Waterfall or method or the V-model. This is due to the emphasis on early prototyping, quick feedback and incremental development. It still might not be the favorable method for use in large scale industrial development projects where formal procedures might still be preferred, but the pedagogical advantages in mechatronics education are valuable. Incremental development and rapid prototyping for example gives many opportunities for students to reflect and improve. The Scrum focus on self-organizing teams also provides a platform to practice project organization, by empowering students to take responsibility for the product development process. Among the results of this study, it is shown that it is possible and favorable to integrate Scrum in a mechatronics capstone course and that this can enhance student preparation for a future career as mechatronics designers or product developers. It is also shown that this prepares the students with a larger flexibility to handle the increased complexity in mechatronics product development and thereby enabling the project teams to deliver results faster, more reliable and with higher quality.},
	Author = {Martin Edin Grimheden},
	Doi = {https://doi.org/10.1016/j.mechatronics.2013.01.003},
	Issn = {0957-4158},
	Journal = {Mechatronics},
	Keywords = {Mechatronics education, Agile methods, Capstone course, Design-centric education},
	Number = {8},
	Pages = {967 - 973},
	Title = {Can agile methods enhance mechatronics design education?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957415813000044},
	Volume = {23},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957415813000044},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.mechatronics.2013.01.003}}

@article{LUI2006915,
	Abstract = {Agile Software Development methodologies have grown in popularity both among academic researchers and industrial practitioners. Among the various methodologies or practices proposed, pair programming, which is concerned with two programmers collaborating on design, coding and testing, has become a controversial focus of interest. Even though some success stories have been reported with the use of pair-programming in real software development environment, many people remain rather skeptical of the claims on pair-programming productivity. Previous studies in pair programming have only addressed the basic understanding of the productivity of pairs and they have not addressed the variation in productivity between pairs of varying skills and experience, such as between novice--novice and expert--expert. Statistical productivity measurements reported by different researchers also seem to lead to contradictory conclusions. Until now, the literature has not addressed how those results and experiments were related to each other. In this paper, we propose a controlled experiment called repeat-programming which can facilitate the understanding of relationships between human experience and programming productivity. Repeat-programming can be performed when controversial issues in non-traditional programming methodologies and development productivity need to be investigated into. To illustrate how the proposed empirical experiment can put arguable, divisive problems into perspective, we have examined the productivity in pair programming as a case study. With repeat-programming, we are able to (i) better understand why results of previous pair programming control experiments reached different conclusions as to the productivity of pair programming and (ii) most importantly, present a case in which novice--novice pairs against novice solos are much more productive than expert--expert pairs against expert solos.},
	Author = {Kim Man Lui and Keith C.C. Chan},
	Doi = {https://doi.org/10.1016/j.ijhcs.2006.04.010},
	Issn = {1071-5819},
	Journal = {International Journal of Human-Computer Studies},
	Keywords = {Programming model, Pair programming, Programmer productivity},
	Number = {9},
	Pages = {915 - 925},
	Title = {Pair programming productivity: Novice--novice vs. expert--expert},
	Url = {http://www.sciencedirect.com/science/article/pii/S1071581906000644},
	Volume = {64},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1071581906000644},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhcs.2006.04.010}}

@article{MOE2012853,
	Abstract = {Context
Agile software development changes the nature of collaboration, coordination, and communication in software projects.
Objective
Our objective was to understand the challenges of shared decision-making in agile software development teams.
Method
We designed a multiple case study consisting of four projects in two software product companies that recently adopted Scrum. We collected data in semi-structured interviews, through participant observations, and from process artifacts.
Results
We identified three main challenges to shared decision-making in agile software development: alignment of strategic product plans with iteration plans, allocation of development resources, and performing development and maintenance tasks in teams.
Conclusion
Agile software development requires alignment of decisions on the strategic, tactical, and operational levels in order to overcome these challenges. Agile development also requires a transition from specialized skills to redundancy of functions and from rational to naturalistic decision-making. This takes time; the case companies needed from one to two years to change from traditional, hierarchical decision-making to shared decision-making in software development projects.},
	Author = {Nils Brede Moe and Ayb{\"u}ke Aurum and Tore Dyb{\aa}},
	Doi = {https://doi.org/10.1016/j.infsof.2011.11.006},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Agile software development, Scrum, Decision-making, Self-management, Alignment, Teamwork},
	Note = {Special Issue: Voice of the Editorial Board},
	Number = {8},
	Pages = {853 - 865},
	Title = {Challenges of shared decision-making: A multiple case study of agile software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911002308},
	Volume = {54},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584911002308},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2011.11.006}}

@article{LAYMAN2006781,
	Abstract = {We conducted an industrial case study of a distributed team in the USA and the Czech Republic that used Extreme Programming. Our goal was to understand how this globally-distributed team created a successful project in a new problem domain using a methodology that is dependent on informal, face-to-face communication. We collected quantitative and qualitative data and used grounded theory to identify four key factors for communication in globally-distributed XP teams working within a new problem domain. Our study suggests that, if these critical enabling factors are addressed, methodologies dependent on informal communication can be used on global software development projects.},
	Author = {Lucas Layman and Laurie Williams and Daniela Damian and Hynek Bures},
	Doi = {https://doi.org/10.1016/j.infsof.2006.01.004},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Global software development, Extreme programming, Case study},
	Note = {Special Issue Section: Distributed Software Development},
	Number = {9},
	Pages = {781 - 794},
	Title = {Essential communication practices for Extreme Programming in a global software development team},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584906000024},
	Volume = {48},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584906000024},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2006.01.004}}

@article{BIPP2008231,
	Abstract = {We present the results of an extensive and substantial case study on pair programming, which was carried out in courses for software development at the University of Dortmund, Germany. Thirteen software development teams with about 100 students took part in the experiments. The groups were divided into two sets with different working conditions. In one set, the group members worked on their projects in pairs. Even though the paired teams could only use half of the workstations the teams of individual workers could use, the paired teams produced nearly as much code as the teams of individual workers at the same time. In addition, the code produced by the paired teams was easier to read and to understand. This facilitates finding errors and maintenance.},
	Author = {Tanja Bipp and Andreas Lepper and Doris Schmedding},
	Doi = {https://doi.org/10.1016/j.infsof.2007.05.006},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Pair programming, Empirical software engineering, Quality of software},
	Number = {3},
	Pages = {231 - 240},
	Title = {Pair programming in software development teams -- An empirical study of its benefits},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584907000596},
	Volume = {50},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584907000596},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2007.05.006}}

@article{YAUNG2014140,
	Abstract = {Rapid and accurate threading dislocation density (TDD) characterization of direct-gap GaAsyP1y photovoltaic materials using molten KOH defect selective etching (DSE) is demonstrated. TDDs measured using molten KOH DSE show close agreement with those from both electron beam-induced current mapping and planar view transmission electron microscopy, provided TDD<107cm2. H3PO4 DSE is also demonstrated as an accurate method for characterizing TDD of GaP substrates. Taken together, the DSE methods described here enable TDD characterization over large areas (>105m2) from substrate to GaAsyP1y device layer.},
	Author = {Kevin Nay Yaung and Stephanie Tomasulo and Jordan R. Lang and Joseph Faucher and Minjoo Larry Lee},
	Doi = {https://doi.org/10.1016/j.jcrysgro.2014.07.005},
	Issn = {0022-0248},
	Journal = {Journal of Crystal Growth},
	Keywords = {A1. Defects, A1. Etching, A3. Molecular beam epitaxy, B2. Semiconducting gallium arsenide, B2. Semiconducting III--V materials, B3. Solar cells},
	Pages = {140 - 145},
	Title = {Defect selective etching of GaAsyP1y photovoltaic materials},
	Url = {http://www.sciencedirect.com/science/article/pii/S0022024814004412},
	Volume = {404},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0022024814004412},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jcrysgro.2014.07.005}}

@article{ANGIONI2006619,
	Abstract = {Extreme Programming (XP) is an Agile Methodology (AM) which does not require any specific supporting tool for being successfully applied. Despite this starting observation, there are many reasons leading a XP team to adopt Web based tools to support XP practices. For example, such tools could be useful for process and product data collection and analysis or for supporting distributed development. In this article, we describe XPSuite, a tool composed of two parts: XPSwiki, a tool for managing XP projects and XP4IDE, a plug-in for integrating XPSwiki with an Integrated Development Environment (IDE). Moreover, we will show how the full Object Oriented implementation provides a powerful support for extracting all data represented in the model that the system implements.},
	Author = {M. Angioni and D. Carboni and S. Pinna and R. Sanna and N. Serra and A. Soro},
	Doi = {https://doi.org/10.1016/j.sysarc.2006.06.006},
	Issn = {1383-7621},
	Journal = {Journal of Systems Architecture},
	Keywords = {Extreme programming, XP tools, Agile methodologies, Process metrics, Distributed development},
	Note = {Agile Methodologies for Software Production},
	Number = {11},
	Pages = {619 - 626},
	Title = {Integrating XP project management in development environments},
	Url = {http://www.sciencedirect.com/science/article/pii/S1383762106000646},
	Volume = {52},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1383762106000646},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sysarc.2006.06.006}}

@article{BRYANT2008519,
	Abstract = {Computer programming is generally understood to be highly challenging and since its inception a wide range of approaches, tools and methodologies have been developed to assist in managing its complexity. Relatively recently the potential benefits of collaborative software development have been formalised in the practice of pair programming. Here we attempt to `unpick' the pair programming process through the analysis of verbalisations from a number of commercial studies. We focus particularly on the roles of the two programmers and what their key characteristics and behaviours might be. In particular, we dispute two existing claims: (i) that the programmer who is not currently typing in code (``the navigator'') is constantly reviewing what is typed and highlighting any errors (i.e. acting as a reviewer) and (ii) that the navigator focuses on a different level of abstraction as a way of ensuring coverage at all necessary levels (i.e. acting as a foreman). We provide an alternative model for these roles (``the tag team'') in which the driver and navigator play much more equal roles. We also suggest that a key factor in the success of pair programming may be the associated increase in talk at an intermediate level of abstraction.},
	Author = {Sallyann Bryant and Pablo Romero and Benedict du Boulay},
	Doi = {https://doi.org/10.1016/j.ijhcs.2007.03.005},
	Issn = {1071-5819},
	Journal = {International Journal of Human-Computer Studies},
	Keywords = {Pair programming, Verbal protocol analysis, Extreme programming},
	Note = {Collaborative and social aspects of software development},
	Number = {7},
	Pages = {519 - 529},
	Title = {Pair programming and the mysterious role of the navigator},
	Url = {http://www.sciencedirect.com/science/article/pii/S1071581907000456},
	Volume = {66},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1071581907000456},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhcs.2007.03.005}}

@article{KHAMIS20132128,
	Abstract = {Software agents are the basic building blocks in many software systems especially those based on artificial intelligence methods, e.g., reinforcement learning based multi-agent systems (MASs). However, testing software agents is considered a challenging problem. This is due to the special characteristics of agents which include its autonomy, distributed nature, intelligence, and heterogeneous communication protocols. Following the test-driven development (TDD) paradigm, we present a framework that allows MAS developers to write test scenarios that test each agent individually. The framework relies on the concepts of building mock agents and testing common agent interaction design patterns. We analyze the most common agent interaction patterns including pair and mediation patterns in order to provide stereotype implementation for their corresponding test cases. These implementations serve as test building blocks and are provided as a set of ready-for-reuse components in our repository. This way, the developer can concentrate on testing the business logic itself and spare him/her the burden of implementing tests for the underlying agent interaction patterns. Our framework is based on standard components such as the JADE agent platform, the JUnit framework, and the eclipse plug-in architecture. In this paper, we present in details the design and function of the framework. We demonstrate how we can use the proposed framework to define more stereotypes in the code repository and provide a detailed analysis of the code coverage for our designed stereotype test code implementations.},
	Author = {Mohamed A. Khamis and Khaled Nagi},
	Doi = {https://doi.org/10.1016/j.engappai.2013.04.009},
	Issn = {0952-1976},
	Journal = {Engineering Applications of Artificial Intelligence},
	Keywords = {Multi-agent unit tests, Test-driven development, Mock agent, Agent social design patterns, Code generation, Code coverage},
	Number = {9},
	Pages = {2128 - 2142},
	Title = {Designing multi-agent unit tests using systematic test design patterns-(extended version)},
	Url = {http://www.sciencedirect.com/science/article/pii/S0952197613000754},
	Volume = {26},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0952197613000754},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.engappai.2013.04.009}}

@article{TSAI201269,
	Abstract = {This paper introduces an ultrasonic, vibration-assisted, chemical mechanical polishing (UV-CMP) method and an ultrasonic, vibration-assisted, traditional diamond disk (UV-TDD) dressing method. A copper substrate is polished by traditional CMP and UV-CMP. UV-CMP combines the functions of traditional CMP and ultrasonic machining (USM) with small-amplitude, high-frequency tool vibration to improve the fabrication process and machining efficiency. The removal rate of the copper substrate, torque force, and polished surface morphology of CMP and UV-CMP are compared. The polishing pad is also dressed by traditional diamond disk (TDD) and UV-TDD. The pad cut rate, torque force, and pad surface profiles of TDD and UV-TDD are also investigated in experiments. Experimental results reveal that UV-TDD can produce twice the pad cut rate and reduce torque force compared to TDD. Consequently, a dressing time reduction by half is expected, and hence, the diamond life is extended. It is found that the removal rate of the copper substrate polished by UV-CMP is increased by approximately 50--90% relative to that of traditional CMP because in UV-CMP, a passive layer on the copper surface, formed by the chemical action of the slurry, will be removed not only by the mechanical action of CMP but also by ultrasonic action. In addition, the surface roughness improves and the torque force reduces dramatically. This result suggests that the combination processes of CMP/USM and TDD/USM are feasible methods for improving polishing and dressing efficiency.},
	Author = {Ming-Yi Tsai and Wei-Zheng Yang},
	Doi = {https://doi.org/10.1016/j.ijmachtools.2011.09.009},
	Issn = {0890-6955},
	Journal = {International Journal of Machine Tools and Manufacture},
	Keywords = {Chemical mechanical polishing, Diamond disk, Pad, Ultrasonic machining},
	Number = {1},
	Pages = {69 - 76},
	Title = {Combined ultrasonic vibration and chemical mechanical polishing of copper substrates},
	Url = {http://www.sciencedirect.com/science/article/pii/S0890695511001933},
	Volume = {53},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0890695511001933},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijmachtools.2011.09.009}}

@article{ALSHAYEB2005269,
	Abstract = {Software project tracking and project plan adjustment are two important software engineering activities. The class growth shows the design evolution of the software. The System Design Instability (SDI) metric indicates the progress of an object-oriented (OO) project once the project is set in motion. The SDI metric provides information on project evolution to project managers for possible adjustment to the project plan. The objectives of this paper are to test if the System Design Instability metric can be used to estimate and re-plan software projects in an XP-like agile process and study system design evolution in the Agile software process. We present an empirical study of the class growth and the SDI metric in two OO systems, developed using an agile process similar to Extreme Programming (XP). We analyzed the system evolutionary data collected on a daily basis from the two systems. We concluded that: the systems' class growth follows observable trends, the SDI metric can indicate project progress with certain trends, and the SDI metric is correlated with XP activities. In both of the analyzed systems, we observed two consistent jumps in the SDI metric values in early and late development phases. Part of the results agrees with a previous empirical study in another environment.},
	Author = {Mohammad Alshayeb and Wei Li},
	Doi = {https://doi.org/10.1016/j.jss.2004.02.002},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Empirical study, System design instability (SDI) metric, Design evolution in agile software process, Extreme programming},
	Number = {3},
	Pages = {269 - 274},
	Title = {An empirical study of system design instability metric and design evolution in an agile software process},
	Url = {http://www.sciencedirect.com/science/article/pii/S016412120400007X},
	Volume = {74},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S016412120400007X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2004.02.002}}

@article{VANVALKENHOEF20111227,
	Abstract = {Context
Extreme Programming (XP) is one of the most popular agile software development methodologies. XP is defined as a consistent set of values and practices designed to work well together, but lacks practices for project management and especially for supporting the customer role. The customer representative is constantly under pressure and may experience difficulties in foreseeing the adequacy of a release plan.
Objective
To assist release planning in XP by structuring the planning problem and providing an optimization model that suggests a suitable release plan.
Method
We develop an optimization model that generates a release plan taking into account story size, business value, possible precedence relations, themes, and uncertainty in velocity prediction. The running-time feasibility is established through computational tests. In addition, we provide a practical heuristic approach to velocity estimation.
Results
Computational tests show that problems with up to six themes and 50 stories can be solved exactly. An example provides insight into uncertainties affecting velocity, and indicates that the model can be applied in practice.
Conclusion
An optimization model can be used in practice to enable the customer representative to take more informed decisions faster. This can help adopting XP in projects where plan-driven approaches have traditionally been used.},
	Author = {Gert van Valkenhoef and Tommi Tervonen and Bert de Brock and Douwe Postmus},
	Doi = {https://doi.org/10.1016/j.infsof.2011.05.007},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Extreme programming, Project management, Customer role, Integer programming},
	Note = {AMOST 2010},
	Number = {11},
	Pages = {1227 - 1235},
	Title = {Quantitative release planning in extreme programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584911001340},
	Volume = {53},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584911001340},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2011.05.007}}

@article{HANKS2008530,
	Abstract = {Pair programming, in which two individuals share a single computer to collaboratively develop software, has been shown to have many benefits in industry and in education. One drawback of pair programming is its collocation requirement, which limits its use to situations where the partners can physically meet. A tool that supported distributed pair programming, in which the partners could pair from separate locations, would remove this impediment. This paper discusses the development and empirical evaluation of such a tool. A significant feature of this tool is the presence of a second cursor that supports gesturing. Students who used the tool in their introductory programming course performed as well as collocated students on their programming assignments and final exam. These students also spent less time working by themselves. They also felt that the gesturing feature was useful and used it regularly.},
	Author = {Brian Hanks},
	Doi = {https://doi.org/10.1016/j.ijhcs.2007.10.003},
	Issn = {1071-5819},
	Journal = {International Journal of Human-Computer Studies},
	Keywords = {Distributed pair programming, Gesturing, Introductory programming, Empirical software engineering, Computer-supported cooperative work},
	Note = {Collaborative and social aspects of software development},
	Number = {7},
	Pages = {530 - 544},
	Title = {Empirical evaluation of distributed pair programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S1071581907001395},
	Volume = {66},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1071581907001395},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhcs.2007.10.003}}

@article{SESSLER2002421,
	Abstract = {Summary
The family of international standards for mobile communications IMT-2000 includes amongst others the UMTS Terrestrial Radio Access (UTRA) proposal, which consists of two modes: Frequency Division Duplex (FDD) and Time Division Duplex (TDD). Both are wideband CDMA systems. CDMA systems are Multiple Access Interference (MAI) limited. Conventional detectors like the RAKE receiver do not decrease the MAI, this leads to a limited Bit Error Rate (BER) performance. For further improvement of system capacity Multiuser Detectors (MUD) should be applied. In this paper a non-linear approach employing Radial Basis Functions (RBF) is shown. The adaption of this algorithm to UTRA, its complexity and the BER-performance is discussed.},
	Author = {Gunther M.A. Sessler and Ralf Machauer and Friedrich K. Jondral},
	Doi = {https://doi.org/10.1078/1434-8411-54100132},
	Issn = {1434-8411},
	Journal = {AEU - International Journal of Electronics and Communications},
	Keywords = {Multiuser detection, CDMA, Radial basis functions},
	Number = {6},
	Pages = {421 - 425},
	Title = {A Non-Linear Multiuser Detector for UMTS},
	Url = {http://www.sciencedirect.com/science/article/pii/S1434841104701223},
	Volume = {56},
	Year = {2002},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1434841104701223},
	Bdsk-Url-2 = {https://doi.org/10.1078/1434-8411-54100132}}

@article{MULLER2006335,
	Abstract = {The drawback of pair programming is the nearly doubled personnel cost. The extra cost of pair programming originates from the strict rule of extreme programming where every line of code should be developed by a pair of developers. Is this rule not a waste of resources? Is it not possible to gain a large portion of the benefits of pair programming by only a small fraction of the meeting time of a pair programming session? We conducted a preliminary study to answer this question by splitting the pair programming process into a pair design and a pair implementation phase. The pair implementation phase is compared to a solo implementation phase, which in turn was preceeded by a pair design phase, as well. The study is preliminary as its major goal was to identify an appropriate sample size for subsequent experiments. The data from this study suggest that there is no difference in terms of development cost between a pair and a solo implementation phase if the cost for developing programs of similar level of correctness is concerned.},
	Author = {Matthias M. M{\"u}ller},
	Doi = {https://doi.org/10.1016/j.infsof.2005.09.008},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Pair programming, Preliminary study, Post-development test-cases},
	Note = {EASE 2005},
	Number = {5},
	Pages = {335 - 344},
	Title = {A preliminary study on the impact of a pair design phase on pair programming and solo programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584905001412},
	Volume = {48},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584905001412},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2005.09.008}}

@article{TOLFO20081955,
	Abstract = {The adoption of extreme programming (XP) method requires a very peculiar cultural context in software development companies. However, stakeholders do not always consider this matter and tend to stand to technical requirements of the method. Hence this paper aims at identifying aspects of organizational culture that may influence favorably or unfavorably the use of XP. In order to identify those aspects, this study analyzes dimensions of organizational culture under the perspective of practices and values of XP. This paper is based on the review of the literature of the area and empirical observations carried out with six software companies. This study does not intend to develop a tool for measurement of XP's compatibility with the organizational culture of each company. It intends to provide parameters (favorable and unfavorable aspects) for previous consideration of the convenience of XP implementation.},
	Author = {Cristiano Tolfo and Raul Sidnei Wazlawick},
	Doi = {https://doi.org/10.1016/j.jss.2008.01.014},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Software development companies, Organizational culture, Extreme programming},
	Number = {11},
	Pages = {1955 - 1967},
	Title = {The influence of organizational culture on the adoption of extreme programming},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121208000174},
	Volume = {81},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121208000174},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2008.01.014}}

@article{PINHEIRO2013332,
	Abstract = {The decision may be defined as a result of a process of choice, given an identified problem or when the decision maker faces an opportunity of creation, optimization or improvement in an environment. Considering that agile methodologies, in focus Framework SCRUM, are always more popular in Development Software Companies, and noticing that the mentioned companies cannot always apply every characteristics of the framework, this paper presents an hybrid application of methodologies from Verbal Decision Analysis (VDA) framework to select some of the SCRUM approaches to be applied in the company, considering the elicitation of preferences of a decision maker. The work intends to provide an evaluation of Project Management approaches applied in the Software Development and examine them toward to identify which are the most preferable ones, aided by the application of a hybrid model of decision making. The hybrid model aims at classifying alternatives using ORCLASS method, through the developed software, and ranking them using a Verbal Decision Analysis method (ZAPROS III-i). Afterward, Specific Practices (SP) of Capability Maturity Model Integration (CMMi) level 2 were chosen, and approaches to attend the SP's were ranked from the most preferable to the least preferable ones, aiming to help enterprises which are not able to reach a complete CMMi qualification.},
	Author = {Pl{\'a}cido Rogerio Pinheiro and Thais Cristina Sampaio Machado and Isabelle Tamanini},
	Doi = {https://doi.org/10.1016/j.procs.2013.05.043},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Verbal Decision Analysis, ZAPROS III-i, ORCLASS, Project Management, Specific Practices, Capability Maturity Model Integration},
	Note = {First International Conference on Information Technology and Quantitative Management},
	Pages = {332 - 339},
	Title = {Dealing the Selection of Project Management through Hybrid Model of Verbal Decision Analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050913001762},
	Volume = {17},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050913001762},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2013.05.043}}

@article{ALSHAYEB20061068,
	Abstract = {Extreme programming (XP) is an agile software process that promotes early and quick production of working code. In this paper, we investigated the relationship among three XP engineering activities: new design, refactoring, and error fix. We found that the more the new design performed to the system the less refactoring and error fix were performed. However, the refactoring and error fix efforts did not seem to be related. We also found that the error fix effort is related to number of days spent on each story, while new design is not. The relationship between the refactoring effort and number of days spent on each story was not conclusive.},
	Author = {Mohammad Alshayeb and Wei Li},
	Doi = {https://doi.org/10.1016/j.infsof.2006.01.005},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Extreme programming, Design evolution, Extreme programming engineering activities, Empirical study},
	Number = {11},
	Pages = {1068 - 1072},
	Title = {An empirical study of relationships among extreme programming engineering activities},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584906000097},
	Volume = {48},
	Year = {2006},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584906000097},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2006.01.005}}

@article{STRODE20121222,
	Abstract = {Agile software development provides a way to organise the complex task of multi-participant software development while accommodating constant project change. Agile software development is well accepted in the practitioner community but there is little understanding of how such projects achieve effective coordination, which is known to be critical in successful software projects. A theoretical model of coordination in the agile software development context is presented based on empirical data from three cases of co-located agile software development. Many practices in these projects act as coordination mechanisms, which together form a coordination strategy. Coordination strategy in this context has three components: synchronisation, structure, and boundary spanning. Coordination effectiveness has two components: implicit and explicit. The theoretical model of coordination in agile software development projects proposes that an agile coordination strategy increases coordination effectiveness. This model has application for practitioners who want to select appropriate practices from agile methods to ensure they achieve coordination coverage in their project. For the field of information systems development, this theory contributes to knowledge of coordination and coordination effectiveness in the context of agile software development.},
	Author = {Diane E. Strode and Sid L. Huff and Beverley Hope and Sebastian Link},
	Doi = {https://doi.org/10.1016/j.jss.2012.02.017},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Agile methods, Agile software development project, Coordination effectiveness, Coordination strategy, Coordination Theory, Extreme Programming, Scrum},
	Note = {Special Issue: Agile Development},
	Number = {6},
	Pages = {1222 - 1238},
	Title = {Coordination in co-located agile software development projects},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121212000465},
	Volume = {85},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121212000465},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2012.02.017}}

@article{FOJTIK20111464,
	Abstract = {The paper shows experiences with using Extreme Programming (XP) for specific projects. XP practices Test-Driven Development, pair programming, short interaction, a team code ownership and acceptance tests. Author writes about possibilities of information technologies when improving communicative skills of children with specific disorders, such as autistic spectrum disorders, Down syndrome, mental retardation, etc. The development of an application stemming from the communication system PECS (The Picture Exchange Communication System) and its Czech variant VOKS is the base of this paper to show specificity of the development and verification of software for the given group of handicapped users. The paper shows suitability of using agile methods of software development for a concrete application which is designed for users with specific disorders. It tries to show advantages and disadvantages of new methodologies, particularly Extreme Programming. This agile method prefers fast reaction to a change before the plan completion, which proved to be important in the case of the developed software. There were plenty of changes and new requirements during the development, and their solution was more important for the output quality than following the time schedule of the development.},
	Author = {Rostislav Fojtik},
	Doi = {https://doi.org/10.1016/j.procs.2011.01.032},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Agile methodology, Autism, Down syndrome, Extreme Programming, Mental retardation, VOKS, Testing, Unit test},
	Note = {World Conference on Information Technology},
	Pages = {1464 - 1468},
	Title = {Extreme Programming in development of specific software},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050911000330},
	Volume = {3},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050911000330},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2011.01.032}}

@article{CHOI20081114,
	Abstract = {With the recent advent of agile software process methods, a number of seldom used and unorthodox practices have come to the forefront in the field of computer programming. One such practice is that of pair programming, which is characterized by two programmers sharing the same computer for collaborative programming purposes. The very nature of pair programming implies a psychological and social interaction between the participating programmers and thus brings into play a unique element that we do not see with the conventional individual programming model. This paper focuses on the effects that one of these psychosocial factors, a programmer's personality type, may have on the pair programming environment. In this study, a group of university students, 68 undergraduate students and 60 master's degree graduate students, each of whom had been personality type profiled using the Myers--Briggs Type Indicator (MBTI) model, was split into three sub-groups. One group consisted of subjects who were alike in MBTI type. Another group consisted of subjects who were opposite to each other in MBTI type, and the last group was comprised of subjects who were diverse -- partially alike and partially opposite -- in MBTI type. Through two pair programming sessions, the pairs in each group were assessed for their output, in code productivity. The result showed that the sub-group of subjects who were diverse in MBTI type exhibited higher productivity than both alike and opposite groups. In a comparison between alike and opposite groups, the productivity of the opposite group was greater than that of the alike group.},
	Author = {Kyungsub S. Choi and Fadi P. Deek and Il Im},
	Doi = {https://doi.org/10.1016/j.infsof.2007.11.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Pair programming, Team programming, Collaborative programming, Myers--Briggs Type Indicator, Personality},
	Number = {11},
	Pages = {1114 - 1126},
	Title = {Exploring the underlying aspects of pair programming: The impact of personality},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584907001292},
	Volume = {50},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584907001292},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2007.11.002}}

@article{UMAR20125603,
	Abstract = {The purpose of this study is to investigate the influence of learning style preference on learning C++ language, especially i n terms of the quality of codes written and the number of errors made in programming. A total of 120 computing students were involved in this seven-week study. They were randomly assigned to either a group that received the metaphor with pair programming (MPP), to another group that received pair programming (PP) only, or to a third group that received only the metaphor instruction (MI). Participants in both the MPP and PP groups worked in pairs based on the visual-verbal learning style dimension, and those in the MI group worked individually when completing programming tasks. Two computer programming coursework were used to measure the student's coding performance. The results indicated that visual students in the MPP group performed significantly better in the quality of codes and made fewer mistakes during coding than those in the PP and MI groups. Also, the MPP method did help the verbal students to excel in C++ coding as compared to the PP and MI groups. The direct correlation was observed between their quality of codes and the number of errors. Metaphors helped the visual students in developing deeper conceptual understanding by linking the known to the newly acquired concepts. In pair programming approach, the verbal students developed better understanding through peer discussions. The understanding of individual learning style is needed in order to enhance coding performance as robustness of a programme is correlated with the accuracy of statements in the programming.},
	Author = {Irfan Naufal Umar and Tie Hui Hui},
	Doi = {https://doi.org/10.1016/j.sbspro.2012.06.482},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {learning style, computer programming, metaphor, pair programming},
	Note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
	Pages = {5603 - 5609},
	Title = {Learning Style, Metaphor and Pair Programming: Do they Influence Performance?},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877042812022185},
	Volume = {46},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877042812022185},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2012.06.482}}

@article{FIRDAUS2014546,
	Abstract = {This paper introduces an enhanced Feature Driven Development (FDD) model for secure software development. In fact, the enhanced model is based on our previous study and its findings which concluded that existing FDD poses limitations to develop secure software. Thus, an enhanced FDD that supports secure software development is proposed. We have implemented this new FDD model and conducted a case study to compare the level of security in the undergraduate and postgraduate level students. The paper illustrates that agility of FDD is not affected significantly, even after adding new phases.},
	Author = {Adila Firdaus and Imran Ghani and Seung Ryul Jeong},
	Doi = {https://doi.org/10.1016/j.sbspro.2014.03.712},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {Agile Methodology, Security, Software Engineering, Feature Driven Development},
	Note = {2nd International Conference on Innovation, Management and Technology Research},
	Pages = {546 - 553},
	Title = {Secure Feature Driven Development (SFDD) Model for Secure Software Development},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877042814028948},
	Volume = {129},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877042814028948},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2014.03.712}}

@article{ZUPPIROLI20131687,
	Abstract = {In this paper we give a new interpretation, from a constructivist perspective, of two recent laboratory experiences in the context of a Software Engineering course at the University of Bologna. The two experiences were quite different both in the tools and in the modalities that were used: in one case a software product line was developed by following the rules of a role- playing game; in the second case students had to develop four software products using a process model chosen among Waterfall, Spiral, and Extreme Programming. Despite these differences, both cases can provide some evidences of the validity of the constructivist approach to teaching. In fact, from the results obtained and from the perceived experience of students emerged clearly the fact that, in both cases, the students working autonomously, in small groups and with a limited presence of a teacher, were able to build their own model of knowledge which resulted to be correct, as proved also by the good quality of the artifacts produced.},
	Author = {Sara Zuppiroli and Maurizio Gabbrielli and Paolo Ciancarini},
	Doi = {https://doi.org/10.1016/j.sbspro.2013.12.191},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {Constructivism, Software Engineering, Computer Science},
	Note = {4th International Conference on New Horizons in Education},
	Pages = {1687 - 1691},
	Title = {Laboratory Experiences in Software Engineering from a Constructivist Perspective},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877042813048143},
	Volume = {106},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877042813048143},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2013.12.191}}

@article{TORT20111088,
	Abstract = {Test-Driven Development (TDD) is an extreme programming development method in which a software system is developed in short iterations. In this paper we present the Test-Driven Conceptual Modeling (TDCM) method, which is an application of TDD for conceptual modeling, and we show how to develop a conceptual schema using it. In TDCM, a system's conceptual schema is incrementally obtained by performing three kinds of tasks: (1) Write a test the system should pass; (2) Change the schema to pass the test; and (3) Refactor the schema to improve its qualities. We also describe an integration approach of TDCM into a broad set of software development methodologies, including the Unified Process development methodology, the MDD-based approaches, the storytest-driven agile methods and the goal and scenario-oriented requirements engineering methods. We deal with schemas written in UML/OCL, but the TDCM method could be adapted to the development of schemas in other languages.},
	Author = {Albert Tort and Antoni Oliv{\'e} and Maria-Ribera Sancho},
	Doi = {https://doi.org/10.1016/j.datak.2011.07.006},
	Issn = {0169-023X},
	Journal = {Data & Knowledge Engineering},
	Keywords = {Conceptual modeling, Testing, TDD, Requirements validation, UML/OCL},
	Number = {12},
	Pages = {1088 - 1111},
	Title = {An approach to test-driven development of conceptual schemas},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X11000978},
	Volume = {70},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0169023X11000978},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.datak.2011.07.006}}
