%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Salima Yahiouche at 2020-10-20 21:48:00 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Santos:1994:ACD:192309.192327,
	Abstract = {This paper describes an algorithm to detect user's mental chunks by analysis of pause lengths in goal-directed human-computer interaction. Identifying and characterizing users' chunks can help in gauging the users' level of expertise. The algorithm described in this paper works with information collected by an automatic logging mechanism. Therefore, it is applicable to situations in which no human intervention is required to perform the analysis, such as adaptive interfaces. An empirical study was conducted to validate the algorithm, showing that mental chunks and their characteristics can indeed be inferred from analysis of human-computer interaction logs. Users performing a variety of goal-directed tasks were monitored. Using an automated logging tool, every command invoked, every operation performed with the input devices, as well as all system responses were recorded. Analysis of the interaction logs was performed by a program that implements a chunk detection algorithm that looks at command sequences and timings. The results support the hypothesis that a significant number of user mental chunks can be detected by our algorithm.},
	Acmid = {192327},
	Address = {New York, NY, USA},
	Author = {Santos, Paulo J. and Badre, Albert N.},
	Booktitle = {Proceedings of the Workshop on Advanced Visual Interfaces},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:24:10 +0100},
	Doi = {10.1145/192309.192327},
	Isbn = {0-89791-733-2},
	Keywords = {chunk detection, chunking, event logging, human-computer interaction, models of the user, novice/expert differences, user study},
	Location = {Bari, Italy},
	Numpages = {9},
	Pages = {69--77},
	Publisher = {ACM},
	Series = {AVI '94},
	Title = {Automatic Chunk Detection in Human-computer Interaction},
	Url = {http://doi.acm.org/10.1145/192309.192327},
	Year = {1994},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/192309.192327},
	Bdsk-Url-2 = {https://doi.org/10.1145/192309.192327}}

@inproceedings{Oner:1995:DRF:201310.201321,
	Abstract = {Recent advances in Field-Programmable Gate Arrays (FPGA) and programmable interconnects have made it possible to build efficient hardware emulation engines. In addition, improvements in Computer-Aided Design (CAD) tools, mainly in synthesis tools, greatly simplify the design of large circuits. The RPM (Rapid Prototype Engine for Multiprocessors) Project leverages these two technological advances. Its goal is to develop a common hardware platform for the emulation of multiprocessor systems with different architectures.
For cost reasons, the use of FPGAs in RPM is limited to the memory controllers, while the rest of the emulator, including the processors, memories and interconnect, is built with off-the-shelf components. A flexible non-intrusive event logging mechanism is included at all levels of the memory hierarchy, making it possible to monitor the emulation in very fine detail. This paper presents the hardware design of RPM.},
	Acmid = {201321},
	Address = {New York, NY, USA},
	Author = {\"{O}ner, Koray and Barroso, Luiz A. and Iman, Sasan and Jeong, Jaeheon and Ramamurthy, Krishnan and Dubois, Michel},
	Booktitle = {Proceedings of the 1995 ACM Third International Symposium on Field-programmable Gate Arrays},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:23:53 +0100},
	Doi = {10.1145/201310.201321},
	Isbn = {0-89791-743-X},
	Keywords = {field-programmable gate arrays, logic emulation, message-passing multicomputers, rapid prototyping, shared-memory multiprocessors},
	Location = {Monterey, California, USA},
	Numpages = {7},
	Pages = {60--66},
	Publisher = {ACM},
	Series = {FPGA '95},
	Title = {The Design of RPM: An FPGA-based Multiprocessor Emulator},
	Url = {http://doi.acm.org/10.1145/201310.201321},
	Year = {1995},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/201310.201321},
	Bdsk-Url-2 = {https://doi.org/10.1145/201310.201321}}

@inproceedings{Hudson:1999:TCP:320719.322590,
	Abstract = {A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done ``by hand,'' making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler.},
	Acmid = {322590},
	Address = {New York, NY, USA},
	Author = {Hudson, Scott E. and John, Bonnie E. and Knudsen, Keith and Byrne, Michael D.},
	Booktitle = {Proceedings of the 12th Annual ACM Symposium on User Interface Software and Technology},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:23:35 +0100},
	Doi = {10.1145/320719.322590},
	Isbn = {1-58113-075-9},
	Keywords = {GOMS, event logs, task modeling, tool support for evaluation, toolkits},
	Location = {Asheville, North Carolina, USA},
	Numpages = {10},
	Pages = {93--102},
	Publisher = {ACM},
	Series = {UIST '99},
	Title = {A Tool for Creating Predictive Performance Models from User Interface Demonstrations},
	Url = {http://doi.acm.org/10.1145/320719.322590},
	Year = {1999},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/320719.322590},
	Bdsk-Url-2 = {https://doi.org/10.1145/320719.322590}}

@inproceedings{Sahoo:2003:CEP:956750.956799,
	Abstract = {As the complexity of distributed computing systems increases, systems management tasks require significantly higher levels of automation; examples include diagnosis and prediction based on real-time streams of computer events, setting alarms, and performing continuous monitoring. The core of autonomic computing, a recently proposed initiative towards next-generation IT-systems capable of 'self-healing', is the ability to analyze data in real-time and to predict potential problems. The goal is to avoid catastrophic failures through prompt execution of remedial actions.This paper describes an attempt to build a proactive prediction and control system for large clusters. We collected event logs containing various system reliability, availability and serviceability (RAS) events, and system activity reports (SARs) from a 350-node cluster system for a period of one year. The 'raw' system health measurements contain a great deal of redundant event data, which is either repetitive in nature or misaligned with respect to time. We applied a filtering technique and modeled the data into a set of primary and derived variables. These variables used probabilistic networks for establishing event correlations through prediction algorithms. We also evaluated the role of time-series methods, rule-based classification algorithms and Bayesian network models in event prediction.Based on historical data, our results suggest that it is feasible to predict system performance parameters (SARs) with a high degree of accuracy using time-series models. Rule-based classification techniques can be used to extract machine-event signatures to predict critical events with up to 70% accuracy.},
	Acmid = {956799},
	Address = {New York, NY, USA},
	Author = {Sahoo, R. K. and Oliner, A. J. and Rish, I. and Gupta, M. and Moreira, J. E. and Ma, S. and Vilalta, R. and Sivasubramaniam, A.},
	Booktitle = {Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:23:19 +0100},
	Doi = {10.1145/956750.956799},
	Isbn = {1-58113-737-0},
	Keywords = {critical event prediction, large-scale clusters, system event log},
	Location = {Washington, D.C.},
	Numpages = {10},
	Pages = {426--435},
	Publisher = {ACM},
	Series = {KDD '03},
	Title = {Critical Event Prediction for Proactive Management in Large-scale Computer Clusters},
	Url = {http://doi.acm.org/10.1145/956750.956799},
	Year = {2003},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/956750.956799},
	Bdsk-Url-2 = {https://doi.org/10.1145/956750.956799}}

@inproceedings{Kompella:2005:IFL:1251203.1251208,
	Abstract = {Automated, rapid, and effective fault management is a central goal of large operational IP networks. Today's networks suffer from a wide and volatile set of failure modes, where the underlying fault proves difficult to detect and localize, thereby delaying repair. One of the main challenges stems from operational reality: IP routing and the underlying optical fiber plant are typically described by disparate data models and housed in distinct network management systems. We introduce a fault-localization methodology based on the use of risk models and an associated troubleshooting system, SCORE (Spatial Correlation Engine), which automatically identifies likely root causes across layers. In particular, we apply SCORE to the problem of localizing link failures in IP and optical networks. In experiments conducted on a tier-1 ISP backbone, SCORE proved remarkably effective at localizing optical link failures using only IP-layer event logs. Moreover, SCORE was often able to automatically uncover inconsistencies in the databases that maintain the critical associations between the IP and optical networks.},
	Acmid = {1251208},
	Address = {Berkeley, CA, USA},
	Author = {Kompella, Ramana Rao and Yates, Jennifer and Greenberg, Albert and Snoeren, Alex C.},
	Booktitle = {Proceedings of the 2Nd Conference on Symposium on Networked Systems Design \& Implementation - Volume 2},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:13:22 +0100},
	Numpages = {14},
	Pages = {57--70},
	Publisher = {USENIX Association},
	Series = {NSDI'05},
	Title = {IP Fault Localization via Risk Modeling},
	Url = {http://dl.acm.org/citation.cfm?id=1251203.1251208},
	Year = {2005},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1251203.1251208}}

@inproceedings{Huo:2006:ESP:1137702.1137711,
	Abstract = {Software process improvement has been a focus of industry for many years. To assist the procedure and implementation of software process improvement we provide a software process recovery method based on mining project enactment data. The goal of process recovery is to improve the quality of a planned software process. We investigate the enactment of a planned software process from the view of understanding the appropriateness and fitness for purpose of the process model from the viewpoint of the project managers in the context of a small software development organization. We collected empirical data from this organization and then applied our method to a pilot case study. The main contribution of our work is to provide a methodology of software process model recovery which supports software process improvement.},
	Acmid = {1137711},
	Address = {New York, NY, USA},
	Author = {Huo, Ming and Zhang, He and Jeffery, Ross},
	Booktitle = {Proceedings of the 2006 International Workshop on Software Quality},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:22:56 +0100},
	Doi = {10.1145/1137702.1137711},
	Isbn = {1-59593-399-9},
	Keywords = {process mining, software process improvement},
	Location = {Shanghai, China},
	Numpages = {6},
	Pages = {39--44},
	Publisher = {ACM},
	Series = {WoSQ '06},
	Title = {An Exploratory Study of Process Enactment As Input to Software Process Improvement},
	Url = {http://doi.acm.org/10.1145/1137702.1137711},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1137702.1137711},
	Bdsk-Url-2 = {https://doi.org/10.1145/1137702.1137711}}

@inproceedings{Gopinathan:2008:EOP:1449764.1449784,
	Abstract = {In this paper, we consider object protocols that constrain interactions between objects in a program. Several such protocols have been proposed in the literature. For many APIs (such as JDOM, JDBC), API designers constrain how API clients interact with API objects. In practice, API clients violate such constraints, as evidenced by postings in discussion forums for these APIs. Thus, it is important that API designers specify constraints using appropriate object protocols and enforce them. The goal of an object protocol is expressed as a protocol invariant. Fundamental properties such as ownership can be expressed as protocol invariants. We present a language, PROLANG, to specify object protocols along with their protocol invariants, and a tool, INVCOP++, to check if a program satisfies a protocol invariant. INVCOP++ separates the problem of checking if a protocol satisfies its protocol invariant (called protocol correctness), from the problem of checking if a program conforms to a protocol (called program conformance). The former is solved using static analysis, and the latter using runtime analysis. Due to this separation (1) errors made in protocol design are detected at a higher level of abstraction, independent of the program's source code, and (2) performance of conformance checking is improved as protocol correctness has been verified statically. We present theoretical guarantees about the way we combine static and runtime analysis, and empirical evidence that our tool INVCOP++ finds usage errors in widely used APIs. We also show that statically checking protocol correctness greatly optimizes the overhead of checking program conformance, thus enabling API clients to test whether their programs use the API as intended by the API designer.},
	Acmid = {1449784},
	Address = {New York, NY, USA},
	Author = {Gopinathan, Madhu and Rajamani, Sriram K.},
	Booktitle = {Proceedings of the 23rd ACM SIGPLAN Conference on Object-oriented Programming Systems Languages and Applications},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:22:40 +0100},
	Doi = {10.1145/1449764.1449784},
	Isbn = {978-1-60558-215-3},
	Keywords = {aspect oriented programming, invariants, program verification},
	Location = {Nashville, TN, USA},
	Numpages = {16},
	Pages = {245--260},
	Publisher = {ACM},
	Series = {OOPSLA '08},
	Title = {Enforcing Object Protocols by Combining Static and Runtime Analysis},
	Url = {http://doi.acm.org/10.1145/1449764.1449784},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1449764.1449784},
	Bdsk-Url-2 = {https://doi.org/10.1145/1449764.1449784}}

@article{Motahari:2008:PSD:1454159.1454186,
	Abstract = {Business processes (BPs) are central to the operation of both public and private organizations. A business process is a set of coordinated tasks and activities to achieve a business objective or goal. Given the importance of BPs to overall efficiency and effectiveness, the competitiveness of organizations hinges on continuous BP improvement. In the nineties, the focus of BP improvement was on automation: workflow management systems (WfMSs) and other middleware technologies were used to reduce cost and improve efficiency by providing better system integration and automated enactment of operational business processes. Recently, the focus of business process has expanded to monitoring, analysis and understanding of business processes, and such techniques are incorporated in business process management systems (BPMSs).},
	Acmid = {1454186},
	Author = {Motahari, Hamid and Benatallah, Boualem and Saint-Paul, Regis and Casati, Fabio and Andritsos, Periklis},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:12:25 +0100},
	Doi = {10.14778/1454159.1454186},
	Issn = {2150-8097},
	Issue_Date = {August 2008},
	Journal = {Proc. VLDB Endow.},
	Number = {2},
	Numpages = {4},
	Pages = {1412--1415},
	Publisher = {VLDB Endowment},
	Title = {Process Spaceship: Discovering and Exploring Process Views from Event Logs in Data Spaces},
	Url = {http://dx.doi.org/10.14778/1454159.1454186},
	Volume = {1},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.14778/1454159.1454186}}

@inproceedings{Bateman:2009:IUI:1570433.1570443,
	Abstract = {Usage data logged from user interactions can be extremely valuable for evaluating software usability. However, instrumenting software to collect usage data is a time-intensive task that often requires technical expertise as well as an understanding of the usability issues to be explored. We have developed a new technique for software instrumentation that removes the need for programming. Interactive Usability Instrumentation (IUI) allows usability evaluators to work directly with a system's interface to specify what components and what events should be logged. Evaluators are able to create higher-level abstractions on the events they log and are provided with real-time feedback on how events are logged. As a proof of the IUI concept, we have created the UMARA system, an instrumentation system that is enabled by recent advances in aspect-oriented programming. UMARA allows users to instrument software without the need for additional coding, and provides tools for specification, data collection, and data analysis. We report on the use of UMARA in the instrumentation of two large open-source projects; our experiences show that IUI can substantially simplify the process of log-based usability evaluation.},
	Acmid = {1570443},
	Address = {New York, NY, USA},
	Author = {Bateman, Scott and Gutwin, Carl and Osgood, Nathaniel and McCalla, Gordon},
	Booktitle = {Proceedings of the 1st ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:22:21 +0100},
	Doi = {10.1145/1570433.1570443},
	Isbn = {978-1-60558-600-7},
	Keywords = {aspect-oriented programming, instrumentation, software logging, usability},
	Location = {Pittsburgh, PA, USA},
	Numpages = {10},
	Pages = {45--54},
	Publisher = {ACM},
	Series = {EICS '09},
	Title = {Interactive Usability Instrumentation},
	Url = {http://doi.acm.org/10.1145/1570433.1570443},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1570433.1570443},
	Bdsk-Url-2 = {https://doi.org/10.1145/1570433.1570443}}

@inproceedings{Calders:2009:UMD:1529282.1529606,
	Abstract = {In the field of process mining, the goal is to automatically extract process models from event logs. Recently, many algorithms have been proposed for this task. For comparing these models, different quality measures have been proposed. Most of these measures, however, have several disadvantages; they are model-dependent, assume that the model that generated the log is known, or need negative examples of event sequences. In this paper we propose a new measure, based on the minimal description length principle, to evaluate the quality of process models that does not have these disadvantages. To illustrate the properties of the new measure we conduct experiments and discuss the trade-off between model complexity and compression.},
	Acmid = {1529606},
	Address = {New York, NY, USA},
	Author = {Calders, T. and G\"{u}nther, C. W. and Pechenizkiy, M. and Rozinat, A.},
	Booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:11:35 +0100},
	Doi = {10.1145/1529282.1529606},
	Isbn = {978-1-60558-166-8},
	Location = {Honolulu, Hawaii},
	Numpages = {5},
	Pages = {1451--1455},
	Publisher = {ACM},
	Series = {SAC '09},
	Title = {Using Minimum Description Length for Process Mining},
	Url = {http://doi.acm.org/10.1145/1529282.1529606},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1529282.1529606},
	Bdsk-Url-2 = {https://doi.org/10.1145/1529282.1529606}}

@inproceedings{Mwebaze:2010:DLM:1899503.1899527,
	Abstract = {An important challenge facing e-Science is the development of scalable systems and analysis techniques that allow client applications to locate data and services in increasingly large-scale distributed environments. e-Science Systems should achieve three main goals: (i) efficient and selective processing of data, (ii) support network collaboration without clogging distribution networks; and (iii) allow transparency of experiments through repeatability and verifiability of experiments. Several systems have addressed limited combinations of these properties, but we address all three in this work. We describe the architecture and implementation of such a framework in Astro-WISE, an astronomical approach to distributed data processing, discovery and retrieval of datasets that achieves scalability via dynamic linking (data lineage) maintained within the system. We show that lineage data collected during the processing and analysis of datasets can be reused to perform selective reprocessing(at sub-image level)ondatasets while the remainder of the dataset is untouched, a rather difficult process to automate without lineage.},
	Acmid = {1899527},
	Address = {New York, NY, USA},
	Author = {Mwebaze, Johnson and McFarland, John and Booxhorn, Danny and Valentijn, Edwin},
	Booktitle = {Proceedings of the 2010 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:22:06 +0100},
	Doi = {10.1145/1899503.1899527},
	Isbn = {978-1-60558-950-3},
	Keywords = {data lineage, data reduction, provenance, scientific computing, subimage processing, target processing},
	Location = {Bela Bela, South Africa},
	Numpages = {11},
	Pages = {209--219},
	Publisher = {ACM},
	Series = {SAICSIT '10},
	Title = {A Data Lineage Model for Distributed Sub-image Processing},
	Url = {http://doi.acm.org/10.1145/1899503.1899527},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1899503.1899527},
	Bdsk-Url-2 = {https://doi.org/10.1145/1899503.1899527}}

@inproceedings{Tancreti:2011:AHA:2070942.2070972,
	Abstract = {It is important to get an idea of the events occurring in an embedded wireless node when it is deployed in the field, away from the convenience of an interactive debugger. Such visibility can be useful for post-deployment testing, replay-based debugging, and for performance and energy profiling of various software components. Prior software-based solutions to address this problem have incurred high execution overhead and intrusiveness. The intrusiveness changes the intrinsic timing behavior of the application, thereby reducing the fidelity of the collected profile. Prior hardware-based solutions have involved the use of dedicated ASICs or other tightly coupled changes to the embedded node's processor, which significantly limits their applicability.
In this paper, we present Aveksha, a hardware-software approach for achieving the above goals in a non-intrusive manner. Our approach is based on the key insight that most embedded processors have an on-chip debug module (which has traditionally been used for interactive debugging) that provides significant visibility into the internal state of the processor. We design a debug board that interfaces with the on-chip debug module of an embedded node's processor through the JTAG port and provides three modes of event logging and tracing: breakpoint, watchpoint, and program counter polling. Using expressive triggers that the on-chip debug module supports, Aveksha can watch for, and record, a variety of programmable events of interest. A key feature of Aveksha is that the target processor does not have to be stopped during event logging (in the last two of the three modes), subject to a limit on the rate at which logged events occur. Aveksha also performs power monitoring of the embedded wireless node and, importantly, enables power consumption data to be correlated to events of interest.
Aveksha is an operating system-agnostic solution. We demonstrate its functionality and performance using three applications running on Telos motes; two in TinyOS and one in Contiki. We show that Aveksha can trace tasks and other generic events at the function and task-level granularity. We also describe how we used Aveksha to find a subtle bug in the TinyOS low power listening protocol.},
	Acmid = {2070972},
	Address = {New York, NY, USA},
	Author = {Tancreti, Matthew and Hossain, Mohammad Sajjad and Bagchi, Saurabh and Raghunathan, Vijay},
	Booktitle = {Proceedings of the 9th ACM Conference on Embedded Networked Sensor Systems},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:20:14 +0100},
	Doi = {10.1145/2070942.2070972},
	Isbn = {978-1-4503-0718-5},
	Keywords = {JTAG, debugging, tracing, wireless sensor network},
	Location = {Seattle, Washington},
	Numpages = {14},
	Pages = {288--301},
	Publisher = {ACM},
	Series = {SenSys '11},
	Title = {Aveksha: A Hardware-software Approach for Non-intrusive Tracing and Profiling of Wireless Embedded Systems},
	Url = {http://doi.acm.org/10.1145/2070942.2070972},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2070942.2070972},
	Bdsk-Url-2 = {https://doi.org/10.1145/2070942.2070972}}

@inproceedings{Basanya:2011:MSI:2072069.2072115,
	Abstract = {Service integration is central to joined-up government initiatives and requires information on the collaborators and the services they offer, roles of different actors, the resources required, and their goals (individual and shared). These information are largely available in unstructured forms on government portals, publications and other textural sources. This paper explores semantic text mining for extracting service-related information from such sources using Natural Language Processing techniques supported by Service-Oriented Process Ontologies. Our solution framework consists of the following steps: (1) creating domain and service-oriented process ontology, (2) extracting service-related information from textual sources based on the ontology, and finally (3) mining relationship among the services based on the extracted information in Step 2 linked with a pre-defined hierarchy of service delivery goals specifying the objective(s) to be achieved among the orchestrated services. We describe our approach to these tasks and discuss the progress of the work, our experiences and the challenges encountered so far.},
	Acmid = {2072115},
	Address = {New York, NY, USA},
	Author = {Basanya, Rilwan and Ojo, Adegboyega},
	Booktitle = {Proceedings of the 5th International Conference on Theory and Practice of Electronic Governance},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:20:45 +0100},
	Doi = {10.1145/2072069.2072115},
	Isbn = {978-1-4503-0746-8},
	Keywords = {collaborative networks, data mining, information extraction, joined-up government, process mining, service integration, text mining, transformational government},
	Location = {Tallinn, Estonia},
	Numpages = {4},
	Pages = {269--272},
	Publisher = {ACM},
	Series = {ICEGOV '11},
	Title = {Mining Service Integration Opportunities Towards Joined-up Government},
	Url = {http://doi.acm.org/10.1145/2072069.2072115},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2072069.2072115},
	Bdsk-Url-2 = {https://doi.org/10.1145/2072069.2072115}}

@inproceedings{Cherkasova:2011:PMM:1958746.1958752,
	Abstract = {Unstructured data is the largest and fastest growing portion of most enterprise's assets, often representing 70% to 80% of online data. These steep increase in volume of information being produced often exceeds the capabilities of existing commercial databases. MapReduce and its open-source implementation Hadoop represent an economically compelling alternative that offers an efficient distributed computing platform for handling large volumes of data and mining petabytes of unstructured information. It is increasingly being used across the enterprise for advanced data analytics, business intelligence, and enabling new applications associated with data retention, regulatory compliance, e-discovery and litigation issues.
However, setting up a dedicated Hadoop cluster requires a significant capital expenditure that can be difficult to justify. Cloud computing offers a compelling alternative and allows users to rent resources in a "pay-as-you-go" fashion. For example, a list of offered Amazon Web Services includes MapReduce environment for rent. It is an attractive and cost-efficient option for many users because acquiring and maintaining complex, large-scale infrastructures is a difficult and expensive decision. One of the open questions in such environments is the amount of resources that a user should lease from the service provider. Currently, there is no available methodology to easily answer this question, and the task of estimating required resources to meet application performance goals is the solely user's responsibility. The users need to perform adequate application testing, performance evaluation, capacity planning estimation, and then request appropriate amount of resources from the service provider. To address these problems we need to understand: "What do we need to know about a MapReduce job for building an efficient and accurate modeling framework? Can we extract a representative job profile that reflects a set of critical performance characteristics of the underlying application during all job execution phases, i.e., map, shuffle, sort and reduce phases? What metrics should be included in the job profile?" We discuss a profiling technique for MapReduce applications that aims to construct a compact job profile that is comprised of performance invariants which are independent of the amount of resources assigned to the job (i.e., the size of the Hadoop cluster) and the size of the input dataset. The challenge is how to accurately predict application performance in the large production environment and for processing large datasets from the application executions that being run in the smaller staging environment and that process smaller input datasets.
One of the major Hadoop benefits is its ability of dealing with failures (disk, processes, node failures) and allowing the user job to complete. The performance implications of failures depend on their types, when do they happen, and whether a system can offer some spare resources instead of failed ones to the running jobs. We discuss how to enhance the MapReduce performance model for evaluating the failure impact on job completion time and predicting a potential performance degradation.
Sharing a MapReduce cluster among multiple applications is a common practice in such environments. However, a key challenge in these shared environments is the ability to tailor and control resource allocations to different applications for achieving their performance goals and service level objectives (SLOs). Currently, there is no job scheduler for MapReduce environments that given a job completion deadline, could allocate the appropriate amount of resources to the job so that it meets the required SLO. In MapReduce environments, many production jobs are run periodically on new data. For example, Facebook, Yahoo!, and eBay process terabytes of data and event logs per day on their Hadoop clusters for spam detection, business intelligence and different types of optimization. For the production jobs that are routinely executed on the new datasets, can we build on-line job profiles that later are used for resource allocation and performance management by the job scheduler? Wediscuss opportunities and challenges for building the SLO-based Hadoop scheduler.
The accuracy of new performance models might depend on the resource contention, especially, the network contention in the production Hadoop cluster. Typically, service providerstend to over provision network resources to avoid undesirable side effects of network contention. At the same time, it is an interesting modeling question whether such a network contention factor can be introduced, measured, and incorporated in theMapReduce performance model. Benchmarking Hadoop, optimizing cluster parameter settings, designing job schedulers with different performance objectives, and constructing intelligent workload management in shared Hadoop clusters create an exciting list of challenges and opportunities for the performance analysis and modeling in MapReduce environments.},
	Acmid = {1958752},
	Address = {New York, NY, USA},
	Author = {Cherkasova, Ludmila},
	Booktitle = {Proceedings of the 2Nd ACM/SPEC International Conference on Performance Engineering},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:21:18 +0100},
	Doi = {10.1145/1958746.1958752},
	Isbn = {978-1-4503-0519-8},
	Keywords = {capacity planning, job scheduling, mapreduce, measurements, resource allocation, workload profiling},
	Location = {Karlsruhe, Germany},
	Numpages = {2},
	Pages = {5--6},
	Publisher = {ACM},
	Series = {ICPE '11},
	Title = {Performance Modeling in Mapreduce Environments: Challenges and Opportunities},
	Url = {http://doi.acm.org/10.1145/1958746.1958752},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1958746.1958752},
	Bdsk-Url-2 = {https://doi.org/10.1145/1958746.1958752}}

@inproceedings{Candan:2011:RSM:2064730.2064732,
	Abstract = {Today, multimedia data are produced in massive quantities, thanks to a diverse spectrum of applications including entertainment, surveillance, e-commerce, web, and social media. In particular, social media data have three challenging characteristics: data sizes are enormous, data are often multi-faceted, and data are dynamic. Tensors (multi-dimensional arrays) are widely used for representing such high-order dimensional data. Consequently, a system dealing with social media data needs to scale with the tensor volume and the number and diversity of the data facets. This necessitates highly parallelizable, and in many cases cloud-based, frameworks for scalable processing and efficient analysis of large media and social media collections.
Most multimedia applications share a few core operations, including integration/fusion, classification, clustering, graph analysis, near-neighbor search, and similarity search. When performed naively, however, these core operations are often very costly, because the number of objects and object features that need to be considered can be prohibitive. Avoiding this cost requires that redundant work is avoided. Thus, for the next generation cloud-based massive media processing and analysis systems to have transformative impact, the fundamental principles that govern their design must include an awareness of the utilities of data and features to a particular analysis task.
Recently, the observation that - while not all - a significant class of data processing applications can be expressed in terms of a small set of primitives that are, in many cases, easy to parallelize, has led to frameworks, such as MapReduce, which have been successfully applied in data processing, mining, and information retrieval domains. Yet, in many other domains (including many aggregation and join tasks that are hard to parallelize) they significantly lag behind traditional solutions. In particular, many multimedia and social media analysis tasks are in the category of applications that pose significant challenges.
In this talk, I will present an overview of recent developments in the area of scalable multimedia and social media retrieval and analysis in the cloud and our own efforts [1, 2, 3, 4, 5, 6] to build a scalable data processing middleware, called RanKloud, specifically sensitive to the needs and requirements of multimedia and social media analysis applications. RanKloud avoids waste by intelligently partitioning the data and allocating it on available resources to minimize the data replication and indexing overheads and to prune superfluous low-utility processing. It also includes a tensor-based relational data model to support the complete lifecycle (from collection to analysis) of the data, involving various integration and other manipulation steps. RanKloud also addresses the computational cost of various multi-dimensional data analysis operations, including decomposition or structural change detection, by (a) leveraging a priori background knowledge (or metadata) about one or more domain dimensions and (b) by extending compressed sensing (CS) to tensor data to encode the observed tensor streams in the form of compact descriptors.
RanKloud will extend the scope of cloud-based systems to the delivery of efficient and large scale analysis over data with variable utility and, thus, will enable new and efficient applications, tools, and systems for multimedia and social media retrieval and analysis.},
	Acmid = {2064732},
	Address = {New York, NY, USA},
	Author = {Candan, K. Sel\c{c}uk},
	Booktitle = {Proceedings of the 9th Workshop on Large-scale and Distributed Informational Retrieval},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:21:49 +0100},
	Doi = {10.1145/2064730.2064732},
	Isbn = {978-1-4503-0959-2},
	Keywords = {analysis, compressed sensing, data partitioning, mapreduce, multimedia, multiresolution, parallel processing, retrieval, social media, tensor decomposition},
	Location = {Glasgow, Scotland, UK},
	Numpages = {2},
	Pages = {1--2},
	Publisher = {ACM},
	Series = {LSDS-IR '11},
	Title = {RanKloud: Scalable Multimedia and Social Media Retrieval and Analysis in the Cloud},
	Url = {http://doi.acm.org/10.1145/2064730.2064732},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2064730.2064732},
	Bdsk-Url-2 = {https://doi.org/10.1145/2064730.2064732}}

@article{Motahari-Nezhad:2011:ECP:1997885.1997930,
	Abstract = {Understanding, analyzing, and ultimately improving business processes is a goal of enterprises today. These tasks are challenging as business processes in modern enterprises are implemented over several applications and Web services, and the information about process execution is scattered across several data sources. Understanding modern business processes entails identifying the correlation between events in data sources in the context of business processes (event correlation is the process of finding relationships between events that belong to the same process execution instance). In this paper, we investigate the problem of event correlation for business processes that are realized through the interactions of a set of Web services. We identify various ways in which process-related events could be correlated as well as investigate the problem of discovering event correlation (semi-) automatically from service interaction logs. We introduce the concept of process view to represent the process resulting from a certain way of event correlation and that of process space referring to the set of possible process views over process events. Event correlation is a challenging problem as there are various ways in which process events could be correlated, and in many cases, it is subjective. Exploring all the possibilities of correlations is computationally expensive, and only some of the correlated event sets result in process views that are interesting. We propose efficient algorithms and heuristics to identify correlated event sets that lead potentially to interesting process views. To account for its subjectivity, we have designed the event correlation discovery process to be interactive and enable users to guide it toward process views of their interest and organize the discovered process views into a process map that allows users to effectively navigate through the process space and identify the ones of interest. We report on experiments performed on both synthetic and real-world datasets that show the viability and efficiency of the approach.},
	Acmid = {1997930},
	Address = {Secaucus, NJ, USA},
	Author = {Motahari-Nezhad, Hamid Reza and Saint-Paul, Regis and Casati, Fabio and Benatallah, Boualem},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:20:29 +0100},
	Doi = {10.1007/s00778-010-0203-9},
	Issn = {1066-8888},
	Issue_Date = {June 2011},
	Journal = {The VLDB Journal},
	Keywords = {Business processes, Event correlation, Process spaces, Process views},
	Number = {3},
	Numpages = {28},
	Pages = {417--444},
	Publisher = {Springer-Verlag New York, Inc.},
	Title = {Event Correlation for Process Discovery from Web Service Interaction Logs},
	Url = {http://dx.doi.org/10.1007/s00778-010-0203-9},
	Volume = {20},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s00778-010-0203-9}}

@inproceedings{Rubin:2014:ADS:2600821.2600842,
	Abstract = {Modern companies continue investing more and more in the creation, maintenance and change of software systems, but the proper specification and design of such systems continues to be a challenge. The majority of current approaches either ignore real user and system runtime behavior or consider it only informally. This leads to a rather prescriptive top-down approach to software development.
In this paper, we propose a bottom-up approach, which takes event logs (e.g., trace data) of a software system for the analysis of the user and system runtime behavior and for improving the software. We use well-established methods from the area of process mining for this analysis. Moreover, we suggest embedding process mining into the agile development lifecycle.
The goal of this position paper is to motivate the need for foundational research in the area of software process mining (applying process mining to software analysis) by showing the relevance and listing open challenges. Our proposal is based on our experiences with analyzing a big productive touristic system. This system was developed using agile methods and process mining could be effectively integrated into the development lifecycle.},
	Acmid = {2600842},
	Address = {New York, NY, USA},
	Author = {Rubin, Vladimir and Lomazova, Irina and Aalst, Wil M. P. van der},
	Booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:18:12 +0100},
	Doi = {10.1145/2600821.2600842},
	Isbn = {978-1-4503-2754-1},
	Keywords = {Agile Methods, Process Mining, Software Process},
	Location = {Nanjing, China},
	Numpages = {5},
	Pages = {70--74},
	Publisher = {ACM},
	Series = {ICSSP 2014},
	Title = {Agile Development with Software Process Mining},
	Url = {http://doi.acm.org/10.1145/2600821.2600842},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2600821.2600842},
	Bdsk-Url-2 = {https://doi.org/10.1145/2600821.2600842}}

@inproceedings{Autili:2014:CEC:2635868.2661667,
	Abstract = {Choreographies are an emergent Service Engineering (SE) approach to compose together and coordinate services in a distributed way. A choreography formalizes the way business participants coordinate their interactions. The focus is not on orchestrations of the work performed within them, but rather on the exchange of messages between these participants. The problems usually addressed when considering a choreography-based specification of the system to be realized are realizability check, and conformance check. In this paper we describe the CHOReOSynt tool, which has been conceived to deal with an additional problem, namely, automated choreography enforcement. That is, when the goal is to actually realize a service choreography by reusing third-party services, their uncontrolled (or wrongly coordinated) composite behavior may show undesired interactions that preclude the choreography realization. CHOReOSynt solves this problem by automatically synthesizing additional software entities that, when interposed among the services, allow for preventing undesired interactions.},
	Acmid = {2661667},
	Address = {New York, NY, USA},
	Author = {Autili, Marco and Di Ruscio, Davide and Di Salle, Amleto and Perucci, Alexander},
	Booktitle = {Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:18:33 +0100},
	Doi = {10.1145/2635868.2661667},
	Isbn = {978-1-4503-3056-5},
	Keywords = {Choreography Synthesis, Distributed Coordination},
	Location = {Hong Kong, China},
	Numpages = {4},
	Pages = {723--726},
	Publisher = {ACM},
	Series = {FSE 2014},
	Title = {CHOReOSynt: Enforcing Choreography Realizability in the Future Internet},
	Url = {http://doi.acm.org/10.1145/2635868.2661667},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2635868.2661667},
	Bdsk-Url-2 = {https://doi.org/10.1145/2635868.2661667}}

@inproceedings{Perimal-Lewis:2014:HID:2667680.2667687,
	Abstract = {Australian Public Hospitals are continually engaged in various process improvement activities to improve patient care and to improve hospital efficiency as the demand for service intensifies. As a consequence there are many initiatives within the health sector focusing on gaining insight into the underlying health processes which are assessed for compliance with specified Key Performance Indicators (KPIs). Process Mining is classified as a Business Intelligence (BI) tool. The aim of process mining activities is to gain insight into the underlying process or processes. The fundamental element needed for process mining is a historical event log of a process. Generally, these event logs are easily sourced from Process Aware Information Systems (PAIS). Simulation is widely used by hospitals as a tool to study the complex hospital setting and for prediction. Generally, simulation models are constructed by 'hand'. This paper presents a novel way of deriving event logs for health data in the absence of PAIS. The constructed event log is then used as an input for process mining activities taking advantage of existing process mining algorithms aiding the discovery of knowledge of the underlying processes which leads to Health Intelligence (HI). One such output of process mining activity, presented in this paper, is the discovery of process model for simulation using the derived event log as an input for process mining by constructing start-to-end patient journey. The study was undertaken using data from Flinders Medical Centre to gain insight into patient journeys from the point of admission to the Emergency Department (ED) until the patient is discharged from the hospital.},
	Acmid = {2667687},
	Address = {Darlinghurst, Australia, Australia},
	Author = {Perimal-Lewis, Lua and De Vries, Denise and Thompson, Campbell H.},
	Booktitle = {Proceedings of the Seventh Australasian Workshop on Health Informatics and Knowledge Management - Volume 153},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:18:53 +0100},
	Isbn = {978-1-921770-35-7},
	Keywords = {emergency department (ED), event logs, general medicine (GM), hospital key performance indicators, inliers, outliers, patient journey, process mining, simulation model},
	Location = {Auckland, New Zealand},
	Numpages = {9},
	Pages = {59--67},
	Publisher = {Australian Computer Society, Inc.},
	Series = {HIKM '14},
	Title = {Health Intelligence: Discovering the Process Model Using Process Mining by Constructing Start-to-end Patient Journeys},
	Url = {http://dl.acm.org/citation.cfm?id=2667680.2667687},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2667680.2667687}}

@inproceedings{Hansel:2014:MBT:2593833.2593840,
	Abstract = {Model-Based Testing, the task of generating test inputs and oracles from a test model, has been successfully applied in the context of safety-critical real time systems. As these systems grow in complexity, test-models, designed to reflect the systems behaviour, will grow too. Currently testers face situations where test-models are too complex for present test generators.
In this paper, we outline a software tool for the evaluation of the scalability of a combination of approaches for model-based test generation. We chose Networks of Timed Automata (NTA) as the modeling formalism because real-time properties can be specified and the semantics are well-defined. However, the tool input is given as a restricted UML statechart which is internally transformed. We expect this to increase industrial acceptance. The tool will provide the selection, parametrization and generation of a metaheuristic algorithm. The aim is to support test model specific generation algorithms. A simulator for NTAs will enable the metaheuristic to search for test goals in the model. For better performance, it will have an advanced parallelisation. Furthermore, input models will be used for search space reduction for even faster test case generation. The proposed approach allows the inclusion of an oracle generator that is able to provide expected outputs; this enables conformance checking between test models and systems under test.
We plan to implement the outlined tool to enable test case generation even for models that are beyond the scope of currently available generators.},
	Acmid = {2593840},
	Address = {New York, NY, USA},
	Author = {H\"{a}nsel, Joachim},
	Booktitle = {Proceedings of the 7th International Workshop on Search-Based Software Testing},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:19:17 +0100},
	Doi = {10.1145/2593833.2593840},
	Isbn = {978-1-4503-2852-4},
	Keywords = {metaheuristics, model based testing, search based testing, timed automata},
	Location = {Hyderabad, India},
	Numpages = {4},
	Pages = {31--34},
	Publisher = {ACM},
	Series = {SBST 2014},
	Title = {Model Based Test Case Generation with Metaheuristics for Networks of Timed Automata},
	Url = {http://doi.acm.org/10.1145/2593833.2593840},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2593833.2593840},
	Bdsk-Url-2 = {https://doi.org/10.1145/2593833.2593840}}

@inproceedings{Rubin:2014:PMA:2652524.2652583,
	Abstract = {Modern information systems produce tremendous amounts of event data. The area of process mining deals with extracting knowledge from this data. Real-life processes can be effectively discovered, analyzed and optimized with the help of mature process mining techniques. There is a variety of process mining case studies and experience reports from such business areas as healthcare, public, transportation and education. Although nowadays, these techniques are mostly used for discovering business processes.
The goal of this industrial paper is to show that process mining can be applied to software too. Here we present and analyze our experiences on applying process mining in different productive software systems used in the touristic domain. Process models and user interface workflows underlie the functional specifications of the systems we experiment with. When the systems are utilized, user interaction is recorded in event logs. After applying process mining methods to these logs, process and user interface flow models are automatically derived. These resulting models provide insight regarding the real usage of the software, motivate the changes in the functional specifications, enable usability improvements and software redesign.
Thus, with the help of our examples we demonstrate that process mining facilitates new forms of software analysis. The user interaction with almost every software system can be mined in order to improve the software and to monitor and measure its real usage.},
	Acmid = {2652583},
	Address = {New York, NY, USA},
	Articleno = {57},
	Author = {Rubin, Vladimir A. and Mitsyuk, Alexey A. and Lomazova, Irina A. and van der Aalst, Wil M. P.},
	Booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:19:35 +0100},
	Doi = {10.1145/2652524.2652583},
	Isbn = {978-1-4503-2774-9},
	Keywords = {client technology, process mining, software process mining, user interface design},
	Location = {Torino, Italy},
	Numpages = {8},
	Pages = {57:1--57:8},
	Publisher = {ACM},
	Series = {ESEM '14},
	Title = {Process Mining Can Be Applied to Software Too!},
	Url = {http://doi.acm.org/10.1145/2652524.2652583},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2652524.2652583},
	Bdsk-Url-2 = {https://doi.org/10.1145/2652524.2652583}}

@inproceedings{Mittal:2014:PMS:2591062.2591152,
	Abstract = {An undergraduate level Software Engineering courses generally consists of a team-based semester long project and emphasizes on both technical and managerial skills. Software Engineering is a practice-oriented and applied discipline and hence there is an emphasis on hands-on development, process, usage of tools in addition to theory and basic concepts. We present an approach for mining the process data (process mining) from software repositories archiving data generated as a result of constructing software by student teams in an educational setting. We present an application of mining three software repositories: team wiki (used during requirement engineering), version control system (development and maintenance) and issue tracking system (corrective and adaptive maintenance) in the context of an undergraduate Software Engineering course. We propose visualizations, metrics and algorithms to provide an insight into practices and procedures followed during various phases of a software development life-cycle. The proposed visualizations and metrics (learning analytics) provide a multi-faceted view to the instructor serving as a feedback tool on development process and quality by students. We mine the event logs produced by software repositories and derive insights such as degree of individual contributions in a team, quality of commit messages, intensity and consistency of commit activities, bug fixing process trend and quality, component and developer entropy, process compliance and verification. We present our empirical analysis on a software repository dataset consisting of 19 teams of 5 members each and discuss challenges, limitations and recommendations.},
	Acmid = {2591152},
	Address = {New York, NY, USA},
	Author = {Mittal, Megha and Sureka, Ashish},
	Booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:19:52 +0100},
	Doi = {10.1145/2591062.2591152},
	Isbn = {978-1-4503-2768-8},
	Keywords = {Education Data Mining, Learning Analytic, Mining Software Repositories, Process Mining, Software Engineering Education},
	Location = {Hyderabad, India},
	Numpages = {10},
	Pages = {344--353},
	Publisher = {ACM},
	Series = {ICSE Companion 2014},
	Title = {Process Mining Software Repositories from Student Projects in an Undergraduate Software Engineering Course},
	Url = {http://doi.acm.org/10.1145/2591062.2591152},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2591062.2591152},
	Bdsk-Url-2 = {https://doi.org/10.1145/2591062.2591152}}

@inproceedings{Beheshitha:2015:PMA:2723576.2723628,
	Abstract = {Research on self-regulated learning has taken main two paths: self-regulated learning as aptitudes and more recently, self-regulated learning as events. This paper proposes the use of the Fuzzy miner process mining technique to examine the relationship between students' self-reported aptitudes (i.e., achievement goal orientation and approaches to learning) and strategies followed in self-regulated learning. A pilot study is conducted to probe the method and the preliminary results are reported.},
	Acmid = {2723628},
	Address = {New York, NY, USA},
	Author = {Beheshitha, Sanam Shirazi and Ga\v{s}evi\'{c}, Dragan and Hatala, Marek},
	Booktitle = {Proceedings of the Fifth International Conference on Learning Analytics And Knowledge},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:17:01 +0100},
	Doi = {10.1145/2723576.2723628},
	Isbn = {978-1-4503-3417-4},
	Keywords = {clustering, learning patterns, process mining, self-regulated learning},
	Location = {Poughkeepsie, New York},
	Numpages = {5},
	Pages = {265--269},
	Publisher = {ACM},
	Series = {LAK '15},
	Title = {A Process Mining Approach to Linking the Study of Aptitude and Event Facets of Self-regulated Learning},
	Url = {http://doi.acm.org/10.1145/2723576.2723628},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2723576.2723628},
	Bdsk-Url-2 = {https://doi.org/10.1145/2723576.2723628}}

@inproceedings{Santos:2015:MSD:2695664.2696046,
	Abstract = {Process tailoring aims to customize a software process to better suit the specific needs of an organization when executing a software project or due to a social context in which the process is inserted. Tailoring happens, in general, through variations in the process elements, such as activities, artifacts, and control flows. This paper aims to introduce a technique that uses process mining to uncover elements from the software process that are candidates for tailoring. The proposed approach analyzes the execution logs from several process instances that share a common standard process. As a result, execution traces that differ from the standard process flow are identified and assessed to uncover their variable elements. The proposed technique was evaluated with data extracted from a real software development scenario when a large system was under development for a set of Brazilian Federal Institutes of Education, Science and Technology.},
	Acmid = {2696046},
	Address = {New York, NY, USA},
	Author = {Santos, Renata M. S. and Oliveira, Toacy C. and e Abreu, Fernando Brito},
	Booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:17:17 +0100},
	Doi = {10.1145/2695664.2696046},
	Isbn = {978-1-4503-3196-8},
	Keywords = {process mining, process tailoring, software process, variation},
	Location = {Salamanca, Spain},
	Numpages = {4},
	Pages = {1657--1660},
	Publisher = {ACM},
	Series = {SAC '15},
	Title = {Mining Software Development Process Variations},
	Url = {http://doi.acm.org/10.1145/2695664.2696046},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2695664.2696046},
	Bdsk-Url-2 = {https://doi.org/10.1145/2695664.2696046}}

@inproceedings{Huber:2015:NSR:2723839.2723842,
	Abstract = {Adaptive Case Management (ACM) is a new paradigm that facilitates the coordination of knowledge work through case handling. Current ACM systems, however, lack support of providing sophisticated user guidance for next step recommendations and predictions about the case future. In recent years, process mining research developed approaches to make recommendations and predictions based on event logs readily available in process-aware information systems. This paper builds upon those approaches and integrates them into an existing ACM solution. The research goal is to design and develop a prototype that gives next step recommendations and predictions based on process mining techniques in ACM systems. The models proposed, recommend actions that shorten the case running time, mitigate deadline transgressions, support case goals and have been used in former cases with similar properties. They further give case predictions about the remaining time, possible deadline violations, and whether the current case path supports given case goals. A final evaluation proves that the prototype is indeed capable of making proper recommendations and predictions. In addition, starting points for further improvement are discussed.},
	Acmid = {2723842},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Huber, Sebastian and Fietta, Marian and Hof, Sebastian},
	Booktitle = {Proceedings of the 7th International Conference on Subject-Oriented Business Process Management},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:17:36 +0100},
	Doi = {10.1145/2723839.2723842},
	Isbn = {978-1-4503-3312-2},
	Keywords = {adaptive case management, business process management, decision support, process mining, recommender systems},
	Location = {Kiel, Germany},
	Numpages = {9},
	Pages = {3:1--3:9},
	Publisher = {ACM},
	Series = {S-BPM ONE '15},
	Title = {Next Step Recommendation and Prediction Based on Process Mining in Adaptive Case Management},
	Url = {http://doi.acm.org/10.1145/2723839.2723842},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2723839.2723842},
	Bdsk-Url-2 = {https://doi.org/10.1145/2723839.2723842}}

@inproceedings{Brandao:2015:TAI:2814058.2814174,
	Abstract = {In business process models, elements can be scattered (repeated) within different processes, making it difficult to handle changes, analyze process for improvements, or check crosscutting impacts. These scattered elements are named as Aspects. Similar to the aspect-oriented paradigm in programming languages, in BPM, aspect handling has the goal to modularize the crosscutting concerns spread across the models. This process modularization facilitates the management of the process (reuse, maintenance and understanding). The current approaches for aspect identification are made manually; thus, resulting in the problem of subjectivity and lack of systematization. This paper proposes a method to automatically identify aspects in business process from its event logs. The method is based on mining techniques and it aims to solve the problem of the subjectivity identification made by specialists. The initial results from a preliminary evaluation showed evidences that the method identified correctly the aspects present in the process model.},
	Acmid = {2814174},
	Address = {Porto Alegre, Brazil, Brazil},
	Articleno = {99},
	Author = {Brandao, Bruna Christina P. and Santoro, Flavia Maria and Azevedo, Leonardo Guerreiro},
	Booktitle = {Proceedings of the Annual Conference on Brazilian Symposium on Information Systems: Information Systems: A Computer Socio-Technical Perspective - Volume 1},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:17:56 +0100},
	Keywords = {Aspects, Business Process Management, Process Mining},
	Location = {Goiania, Goias, Brazil},
	Numpages = {8},
	Pages = {99:741--99:748},
	Publisher = {Brazilian Computer Society},
	Series = {SBSI 2015},
	Title = {Towards Aspects Identification in Business Process Through Process Mining},
	Url = {http://dl.acm.org/citation.cfm?id=2814058.2814174},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2814058.2814174}}

@inproceedings{Riz:2016:CCH:3021955.3021965,
	Abstract = {The healthcare processes are complex and require a certain level of interdisciplinary cooperation among the various specialists and sectors involved in the processes. Besides this complexity, the Brazilian healthcare area has a notorious problem in its public and private health assistance. These problems are structural, organizational and financial, reflecting in the low valuation of quality and service. The goal of this work is propose an adaptation of Process Mining to healthcare processes in order to contribute in improve the healthcare area in Brazil. In order to achieve this goal a study case was carried out in the Erasto Gaertner hospital, situated in Curitiba - PR, Brazil, that is a national reference in treatment of cancer.},
	Acmid = {3021965},
	Address = {Porto Alegre, Brazil, Brazil},
	Articleno = {8},
	Author = {Riz, Gustavo and Santos, Eduardo Alves Portela and Loures, Eduardo Freitas Rocha},
	Booktitle = {Proceedings of the XII Brazilian Symposium on Information Systems on Brazilian Symposium on Information Systems: Information Systems in the Cloud Computing Era - Volume 1},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:15:55 +0100},
	Isbn = {978-85-7669-317-8},
	Keywords = {Process mining, business rules, conformance check, healthcare, process mapping},
	Location = {Florianopolis, Santa Catarina, Brazil},
	Numpages = {8},
	Pages = {8:52--8:59},
	Publisher = {Brazilian Computer Society},
	Series = {SBSI 2016},
	Title = {Conformance Check in Healthcare with the Supporting of Processes Mining},
	Url = {http://dl.acm.org/citation.cfm?id=3021955.3021965},
	Year = {2016},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3021955.3021965}}

@inproceedings{Horita:2016:GAA:2851613.2851898,
	Abstract = {Process aware information system (PAIS) is important in the recent business environment. Developments of PAIS need to consider contexts about technical and business elements. They are needed to develop PAIS effectively (e.g. monitoring environment and constructing adequate business process). Process mining is an important method for analyzing a business environment and utilizing PAIS development and improvement. LTL checking is an important method for checking a specific property to be satisfied with business processes, but correctly writing formal language like LTL is difficult. In this paper, we use LTL checking and prediction based on decision-tree learning for checking goal achievement, false detection and oversight detection. It helps writing properly LTL formula for representing the correct goal property. We conducted a case study using a real life log of traffic fine management process in Italy.},
	Acmid = {2851898},
	Address = {New York, NY, USA},
	Author = {Horita, Hiroki and Hirayama, Hideaki and Tahara, Yasuyuki and Ohsuga, Akihiko},
	Booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:16:14 +0100},
	Doi = {10.1145/2851613.2851898},
	Isbn = {978-1-4503-3739-7},
	Keywords = {conformance checking, process aware information system, process mining, process modeling},
	Location = {Pisa, Italy},
	Numpages = {5},
	Pages = {1214--1218},
	Publisher = {ACM},
	Series = {SAC '16},
	Title = {Goal Achievement Analysis Based on LTL Checking and Decision Tree for Improvements of PAIS},
	Url = {http://doi.acm.org/10.1145/2851613.2851898},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2851613.2851898},
	Bdsk-Url-2 = {https://doi.org/10.1145/2851613.2851898}}

@inproceedings{Chagas:2016:KUM:2973839.2973851,
	Abstract = {There are two important artifacts in any Architecture-Conformance Checking (ACC) approach: i) the representation of the PA and ii) the representation of the CA. Many times, inside the same ACC approach, distinct meta-models are adopted for representing the PA and the CA. Besides, it is common the adoption of meta-models unsuitable for representing architectural details. This heterogeneity makes the checking algorithms complex since they must cope with instances that comply with two different meta-models or do not have proper architectural abstractions. KDM is an ISO meta-model proposed by OMG whose goal is to become the standard representation of systems in modernization tools. It is able to represent many aspects of a software system, including source code details, architectural abstractions and the dependencies between them. However, up to this moment, there is no research showing how KDM can be used in ACC approaches. Therefore we present an investigation of adopting KDM as the unique meta-model for representing PA and CA in ACC approaches. We have developed a three-steps ACC approach called ArchKDM. In the first step a DSL assists in the PA specification; in the second step an Eclipse plug-in provides the necessary support and in the last step the checking is conducted. We have also evaluate our approach using two real world systems and the results were very promising, revealing no false positives or negatives.},
	Acmid = {2973851},
	Address = {New York, NY, USA},
	Author = {Chagas, Fernando and Durelli, Rafael and Terra, Ricardo and Camargo, Valter},
	Booktitle = {Proceedings of the 30th Brazilian Symposium on Software Engineering},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:16:30 +0100},
	Doi = {10.1145/2973839.2973851},
	Isbn = {978-1-4503-4201-8},
	Keywords = {Architectural Conformance Checking, Architectural Reconciliation, Architecture-Description Language, Architecture-Driven Modernization, Knowledge-Discovery Metamodel},
	Location = {Maring\&aacute;, Brazil},
	Numpages = {10},
	Pages = {103--112},
	Publisher = {ACM},
	Series = {SBES '16},
	Title = {KDM As the Underlying Metamodel in Architecture-Conformance Checking},
	Url = {http://doi.acm.org/10.1145/2973839.2973851},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2973839.2973851},
	Bdsk-Url-2 = {https://doi.org/10.1145/2973839.2973851}}

@inproceedings{Schleier-Smith:2016:RAB:2987550.2987573,
	Abstract = {Real-time predictive applications can demand continuous and agile development, with new models constantly being trained, tested, and then deployed. Training and testing are done by replaying stored event logs, running new models in the context of historical data in a form of backtesting or "what if?" analysis. To replay weeks or months of logs while developers wait, we need systems that can stream event logs through prediction logic many times faster than the real-time rate. A challenge with high-speed replay is preserving sequential semantics while harnessing parallel processing power. The crux of the problem lies with causal dependencies inherent in the sequential semantics of log replay.
We introduce an execution engine that produces serial-equivalent output while accelerating throughput with pipelining and distributed parallelism. This is made possible by optimizing for high throughput rather than the traditional stream processing goal of low latency, and by aggressive sharing of versioned state, a technique we term Multi-Versioned Parallel Streaming (MVPS). In experiments we see that this engine, which we call ReStream, performs as well as batch processing and more than an order of magnitude better than a single-threaded implementation.},
	Acmid = {2987573},
	Address = {New York, NY, USA},
	Author = {Schleier-Smith, Johann and Krogen, Erik T. and Hellerstein, Joseph M.},
	Booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:16:45 +0100},
	Doi = {10.1145/2987550.2987573},
	Isbn = {978-1-4503-4525-5},
	Keywords = {Stream replay, backtesting, distributed stream processing},
	Location = {Santa Clara, CA, USA},
	Numpages = {14},
	Pages = {334--347},
	Publisher = {ACM},
	Series = {SoCC '16},
	Title = {ReStream: Accelerating Backtesting and Stream Replay with Serial-Equivalent Parallel Processing},
	Url = {http://doi.acm.org/10.1145/2987550.2987573},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2987550.2987573},
	Bdsk-Url-2 = {https://doi.org/10.1145/2987550.2987573}}

@inproceedings{Yang:2017:DPR:3097983.3098174,
	Abstract = {We present an approach for improving the performance of complex knowledge-based processes by providing data-driven step-by-step recommendations. Our framework uses the associations between similar historic process performances and contextual information to determine the prototypical way of enacting the process. We introduce a novel similarity metric for grouping traces into clusters that incorporates temporal information about activity performance and handles concurrent activities. Our data-driven recommender system selects the appropriate prototype performance of the process based on user-provided context attributes. Our approach for determining the prototypes discovers the commonly performed activities and their temporal relationships. We tested our system on data from three real-world medical processes and achieved recommendation accuracy up to an F1 score of 0.77 (compared to an F1 score of 0.37 using ZeroR) with 63.2% of recommended enactments being within the first five neighbors of the actual historic enactments in a set of 87 cases. Our framework works as an interactive visual analytic tool for process mining. This work shows the feasibility of data-driven decision support system for complex knowledge-based processes.},
	Acmid = {3098174},
	Address = {New York, NY, USA},
	Author = {Yang, Sen and Dong, Xin and Sun, Leilei and Zhou, Yichen and Farneth, Richard A. and Xiong, Hui and Burd, Randall S. and Marsic, Ivan},
	Booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:14:10 +0100},
	Doi = {10.1145/3097983.3098174},
	Isbn = {978-1-4503-4887-4},
	Keywords = {emergency medical process analysis., process prototype extraction, process recommender system, process trace clustering},
	Location = {Halifax, NS, Canada},
	Numpages = {10},
	Pages = {2111--2120},
	Publisher = {ACM},
	Series = {KDD '17},
	Title = {A Data-driven Process Recommender Framework},
	Url = {http://doi.acm.org/10.1145/3097983.3098174},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3097983.3098174},
	Bdsk-Url-2 = {https://doi.org/10.1145/3097983.3098174}}

@inproceedings{Hernandez:2017:ASA:3144826.3145400,
	Abstract = {Serious games are video games with educational purposes. Players interact in many points during a gameplay. These interactions can be registered, producing data sets with sequence of events which provide relevant information about player's skills. Unfortunately, traditional skill assessment methods present limitations to carry out a detailed analysis of large data sets. Sequence Analysis is a group of techniques which allow to analyze data sets consisting of sequence of events. These techniques have been successfully implemented in different fields, and we consider that they can help overcome these limitations. In this paper, we propose an architecture of skill assessment in learning experiences based on serious games using a set of Sequence Analysis techniques known as Process Mining. First, several in-game events are stored in a log. These events are produced by player's interactions with Key Performance Indicators included in the game. Second, event log is used as input for a Process Mining tool. Discovery process is executed and a behavioural model is provided. Third, an assessment metric must be carried out over the model. Finally, a synthetic experiment is conducted and promising results are obtained.},
	Acmid = {3145400},
	Address = {New York, NY, USA},
	Articleno = {50},
	Author = {Hern\'{a}ndez, Juan Antonio Caballero and Duarte, Manuel Palomo and Dodero, Juan Manuel},
	Booktitle = {Proceedings of the 5th International Conference on Technological Ecosystems for Enhancing Multiculturality},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:14:28 +0100},
	Doi = {10.1145/3144826.3145400},
	Isbn = {978-1-4503-5386-1},
	Keywords = {Event-Based Data Analysis, Game-Based Learning, Process Mining, Sequence Analysis, Serious games, Skill assessment, e-Learning},
	Location = {C\&aacute;diz, Spain},
	Numpages = {9},
	Pages = {50:1--50:9},
	Publisher = {ACM},
	Series = {TEEM 2017},
	Title = {An Architecture for Skill Assessment in Serious Games Based on Event Sequence Analysis},
	Url = {http://doi.acm.org/10.1145/3144826.3145400},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3144826.3145400},
	Bdsk-Url-2 = {https://doi.org/10.1145/3144826.3145400}}

@inproceedings{Siddiqui:2017:FSS:3199858.3199886,
	Abstract = {Research on critical infrastructures (CI)s deals with sensitive data that demands underlying platform to be secure, in addition, testing of CI resilience strategies requires reproducibility of results. Disruption or natural disaster scenarios can not be tested on the physical systems, thus simulations are used for experimentation on CIs and as they are inherently distributed and interdependent which is why here, to model CIs, we use a distributed simulations standard namely High Level Architecture (HLA).
The main goals of HLA are to provide interoperability and flexibility. Security and events logging are neither included nor emphasis of HLA standard. This paper presents a framework to have a supervised and secure messages exchange across distributed simulations using HLA standard.},
	Acmid = {3199886},
	Address = {Piscataway, NJ, USA},
	Author = {Siddiqui, Abbas and Heinimann, Hans},
	Booktitle = {Proceedings of the 21st International Symposium on Distributed Simulation and Real Time Applications},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:10:42 +0100},
	Isbn = {978-1-5386-4028-9},
	Location = {Rome, Italy},
	Numpages = {4},
	Pages = {160--163},
	Publisher = {IEEE Press},
	Series = {DS-RT '17},
	Title = {Framework for Supervised and Secure Distributed Simulations of Critical Infrastructures},
	Url = {http://dl.acm.org/citation.cfm?id=3199858.3199886},
	Year = {2017},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3199858.3199886}}

@inproceedings{Gadler:2017:MLM:3200492.3200544,
	Abstract = {Background. Process mining is a technique to build process models from "execution logs" (i.e., events triggered by the execution of a process). State-of-the-art tools can provide process managers with different graphical representations of such models. Managers use these models to compare them with an ideal process model or to support process improvement. They typically select the representation based on their experience and knowledge of the system. Aim. This work studies how to automatically build process models representing the actual intents (or uses) of users while interacting with a software system. Such intents are expressed as a set of actions performed by a user to a system to achieve specific use goals. Method. This work applies the theory of Hidden Markov Models to mine use logs and automatically model the use of a system. Results. Unlike the models generated with process mining tools, the Hidden Markov Models automatically generated in this study provide the intents of a user and can be used to recommend managers with a faithful representation of the use of their systems. Conclusions. The automatic generation of the Hidden Markov Models can achieve a good level of accuracy in representing the actual user's intents provided the log dataset is carefully chosen. In our study, the information contained in one-month set of logs helped automatically build Hidden Markov Models with superior accuracy and similar expressiveness of the models built together with the company's stakeholder.},
	Acmid = {3200544},
	Address = {Piscataway, NJ, USA},
	Author = {Gadler, Daniele and Mairegger, Michael and Janes, Andrea and Russo, Barbara},
	Booktitle = {Proceedings of the 11th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:14:54 +0100},
	Doi = {10.1109/ESEM.2017.47},
	Isbn = {978-1-5090-4039-1},
	Keywords = {hidden markov chain, log analysis, process modelling},
	Location = {Markham, Ontario, Canada},
	Numpages = {10},
	Pages = {334--343},
	Publisher = {IEEE Press},
	Series = {ESEM '17},
	Title = {Mining Logs to Model the Use of a System},
	Url = {https://doi.org/10.1109/ESEM.2017.47},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/ESEM.2017.47}}

@inproceedings{Giurgiu:2017:PDR:3154448.3154451,
	Abstract = {Uncorrectable errors in dynamic random access memory (DRAM) are a common form of hardware failure in server clusters. Failures are costly both in terms of hardware replacement costs and service disruption. While a large body of work exists on analyzing DRAM reliability in large production clusters, little has been reported on the automatic prediction of such errors ahead of time. In this paper, we present a highly accurate predictive model, based on daily event logs and sensor measurements, in a large fleet of commodity servers going back to 2014. By correlating correctable errors with sensor metrics, we can use ensemble machine learning techniques to predict uncorrectable errors weeks in advance.
In addition, we show how such models can be applied in the wild and consumed by customer support teams. Our goal is to minimize false positives, as healthy DRAMs should not be replaced, while accounting for common limitations, such as missing data points and rare occurences of uncorrectable errors.},
	Acmid = {3154451},
	Address = {New York, NY, USA},
	Author = {Giurgiu, Ioana and Szabo, Jacint and Wiesmann, Dorothea and Bird, John},
	Booktitle = {Proceedings of the 18th ACM/IFIP/USENIX Middleware Conference: Industrial Track},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:15:14 +0100},
	Doi = {10.1145/3154448.3154451},
	Isbn = {978-1-4503-5200-0},
	Keywords = {ensemble machine learning, failure prediction, memory systems, reliability},
	Location = {Las Vegas, Nevada},
	Numpages = {7},
	Pages = {15--21},
	Publisher = {ACM},
	Series = {Middleware '17},
	Title = {Predicting DRAM Reliability in the Field with Machine Learning},
	Url = {http://doi.acm.org/10.1145/3154448.3154451},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3154448.3154451},
	Bdsk-Url-2 = {https://doi.org/10.1145/3154448.3154451}}

@inproceedings{Verenich:2017:WPP:3084100.3084110,
	Abstract = {Predictive business process monitoring methods exploit historical process execution logs to provide predictions about running instances of a process, which enable process workers and managers to preempt performance issues or compliance violations. A number of approaches have been proposed to predict quantitative process performance indicators, such as remaining cycle time, cost, or probability of deadline violation. However, these approaches adopt a black-box approach, insofar as they predict a single scalar value without decomposing this prediction into more elementary components. In this paper, we propose a white-box approach to predict performance indicators of running process instances. The key idea is to first predict the performance indicator at the level of activities, and then to aggregate these predictions at the level of a process instance by means of flow analysis techniques. The paper specifically develops this idea in the context of predicting the remaining cycle time of ongoing process instances. The proposed approach has been evaluated on four real-life event logs and compared against several baselines.},
	Acmid = {3084110},
	Address = {New York, NY, USA},
	Author = {Verenich, Ilya and Nguyen, Hoang and La Rosa, Marcello and Dumas, Marlon},
	Booktitle = {Proceedings of the 2017 International Conference on Software and System Process},
	Date-Added = {2019-09-02 09:42:58 +0100},
	Date-Modified = {2020-10-20 18:15:36 +0100},
	Doi = {10.1145/3084100.3084110},
	Isbn = {978-1-4503-5270-3},
	Keywords = {Flow analysis, Predictive Process Monitoring, Process Mining},
	Location = {Paris, France},
	Numpages = {10},
	Pages = {85--94},
	Publisher = {ACM},
	Series = {ICSSP 2017},
	Title = {White-box Prediction of Process Performance Indicators via Flow Analysis},
	Url = {http://doi.acm.org/10.1145/3084100.3084110},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3084100.3084110},
	Bdsk-Url-2 = {https://doi.org/10.1145/3084100.3084110}}

@article{HWANG2004345,
	Abstract = {Existing work in process mining focuses on the discovery of the underlying process model from their instances. In this paper, we do not assume the existence of a single process model to which all process instances comply, and the goal is to discover a set of frequently occurring temporal patterns. Discovery of temporal patterns can be applied to various application domains to support crucial business decision-making. In this study, we formally defined the temporal pattern discovery problem, and developed and evaluated three different temporal pattern discovery algorithms, namely TP-Graph, TP-Itemset and TP-Sequence. Their relative performances are reported.},
	Author = {San-Yih Hwang and Chih-Ping Wei and Wan-Shiou Yang},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.compind.2003.10.006},
	Issn = {0166-3615},
	Journal = {Computers in Industry},
	Keywords = {Process mining, Knowledge discovery, Data mining, Temporal patterns, Association rules, Sequential patterns},
	Note = {Process / Workflow Mining},
	Number = {3},
	Pages = {345 - 364},
	Title = {Discovery of temporal patterns from process instances},
	Url = {http://www.sciencedirect.com/science/article/pii/S0166361503002008},
	Volume = {53},
	Year = {2004},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0166361503002008},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.compind.2003.10.006}}

@article{VANDERAALST2007713,
	Abstract = {Contemporary information systems (e.g., WfM, ERP, CRM, SCM, and B2B systems) record business events in so-called event logs. Business process mining takes these logs to discover process, control, data, organizational, and social structures. Although many researchers are developing new and more powerful process mining techniques and software vendors are incorporating these in their software, few of the more advanced process mining techniques have been tested on real-life processes. This paper describes the application of process mining in one of the provincial offices of the Dutch National Public Works Department, responsible for the construction and maintenance of the road and water infrastructure. Using a variety of process mining techniques, we analyzed the processing of invoices sent by the various subcontractors and suppliers from three different perspectives: (1) the process perspective, (2) the organizational perspective, and (3) the case perspective. For this purpose, we used some of the tools developed in the context of the ProM framework. The goal of this paper is to demonstrate the applicability of process mining in general and our algorithms and tools in particular.},
	Author = {W.M.P. van der Aalst and H.A. Reijers and A.J.M.M. Weijters and B.F. van Dongen and A.K. Alves de Medeiros and M. Song and H.M.W. Verbeek},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.is.2006.05.003},
	Issn = {0306-4379},
	Journal = {Information Systems},
	Keywords = {Process mining, Social network analysis, Workflow management, Business process management, Business process analysis, Data mining, Petri nets},
	Number = {5},
	Pages = {713 - 732},
	Title = {Business process mining: An industrial application},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437906000305},
	Volume = {32},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0306437906000305},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.is.2006.05.003}}

@article{MARUSTER2008159,
	Abstract = {Redesigning IT systems for specific user groups encompasses a lot of effort with respect to analysing and understanding user behaviour. The goal of this paper is to provide insights into patterns of behaviour of agricultural users, during the usage of a decision support system called OPTIRas{\texttrademark}. This system aids agricultural users in their cultivar selection activities. We analyse logs resulting from OPTIRas{\texttrademark}, and we get insights into user's navigational patterns. We claim that the results of our analysis can be used to support the redesign of decision support systems in order to address specific agricultural users' characteristics.},
	Author = {Laura M{\u a}ru{\c s}ter and Niels R. Faber and Ren{\'e} J. Jorna and Rob van Haren},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.agsy.2008.06.003},
	Issn = {0308-521X},
	Journal = {Agricultural Systems},
	Keywords = {Modelling user behaviour, Agricultural users, Decision support systems, Process mining, Cultivar selection},
	Number = {3},
	Pages = {159 - 166},
	Title = {Analysing agricultural users' patterns of behaviour: The case of OPTIRas{\texttrademark}, a decision support system for starch crop selection},
	Url = {http://www.sciencedirect.com/science/article/pii/S0308521X08000735},
	Volume = {98},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0308521X08000735},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.agsy.2008.06.003}}

@article{SONG2008300,
	Abstract = {Process mining has emerged as a way to analyze processes based on the event logs of the systems that support them. Today's information systems (e.g., ERP systems) log all kinds of events. Moreover, also embedded systems (e.g., medical equipment, copiers, and other high-tech systems) start producing detailed event logs. The omnipresence of event logs is an important enabler for process mining. The primary goal of process mining is to extract knowledge from these logs and use it for a detailed analysis of reality. Lion's share of the efforts in this domain has been devoted to control-flow discovery. Many algorithms have been proposed to construct a process model based on an analysis of the event sequences observed in the log. As a result, other aspects have been neglected, e.g., the organizational setting and interactions among coworkers. Therefore, we focus on organizational mining. We will present techniques to discover organizational models and social networks and show how these models can assist in improving the underlying processes. To do this, we present new process mining techniques but also use existing techniques in an innovative manner. The approach has been implemented in the context of the ProM framework and has been applied in various case studies. In this paper, we demonstrate the applicability of our techniques by analyzing the logs of a municipality in the Netherlands.},
	Author = {Minseok Song and Wil M.P. van der Aalst},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.dss.2008.07.002},
	Issn = {0167-9236},
	Journal = {Decision Support Systems},
	Keywords = {Process mining, Social network analysis, Business process management, Workflow management, Data mining, Petri nets},
	Number = {1},
	Pages = {300 - 317},
	Title = {Towards comprehensive support for organizational mining},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167923608001280},
	Volume = {46},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167923608001280},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.dss.2008.07.002}}

@article{LI2011409,
	Abstract = {During the last years a new generation of process-aware information systems has emerged, which enables process model configurations at buildtime as well as process instance changes during runtime. Respective model adaptations result in a large number of model variants that are derived from the same process model, but slightly differ in structure. Generally, such model variants are expensive to configure and maintain. In this paper we address two scenarios for learning from process model adaptations and for discovering a reference model out of which the variants can be configured with minimum efforts. The first one is characterized by a reference process model and a collection of related process variants. The goal is to improve the original reference process model such that it fits better to the variant models. The second scenario comprises a collection of process variants, while the original reference model is unknown; i.e., the goal is to ``merge'' these variants into a new reference process model. We suggest two algorithms that are applicable in both scenarios, but have their pros and cons. We provide a systematic comparison of the two algorithms and further contrast them with conventional process mining techniques. Comparison results indicate good performance of our algorithms and also show that specific techniques are needed for learning from process configurations and adaptations. Finally, we provide results from a case study in automotive industry in which we successfully applied our algorithms.},
	Author = {Chen Li and Manfred Reichert and Andreas Wombacher},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.datak.2011.01.005},
	Issn = {0169-023X},
	Journal = {Data & Knowledge Engineering},
	Keywords = {Process mining, Process configuration, Process change, Process variant},
	Note = {Business Process Management 2009},
	Number = {5},
	Pages = {409 - 434},
	Title = {Mining business process variants: Challenges, scenarios, algorithms},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X11000127},
	Volume = {70},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0169023X11000127},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.datak.2011.01.005}}

@article{FOLINO20111005,
	Abstract = {A prominent goal of process mining is to build automatically a model explaining all the episodes recorded in the log of some transactional system. Whenever the process to be mined is complex and highly-flexible, however, equipping all the traces with just one model might lead to mixing different usage scenarios, thereby resulting in a spaghetti-like process description. This is, in fact, often circumvented by preliminarily applying clustering methods on the process log in order to identify all its hidden variants. In this paper, two relevant problems that arise in the context of applying such methods are addressed, which have received little attention so far: (i) making the clustering aware of outlier traces, and (ii) finding predictive models for clustering results. The first issue impacts on the effectiveness of clustering algorithms, which can indeed be led to confuse real process variants with exceptional behavior or malfunctions. The second issue instead concerns the opportunity of predicting the behavioral class of future process instances, by taking advantage of context-dependent ``non-structural'' data (e.g., activity executors, parameter values). The paper formalizes and analyzes these two issues and illustrates various mining algorithms to face them. All the algorithms have been implemented and integrated into a system prototype, which has been thoroughly validated over two real-life application scenarios.},
	Author = {Francesco Folino and Gianluigi Greco and Antonella Guzzo and Luigi Pontieri},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.datak.2011.07.002},
	Issn = {0169-023X},
	Journal = {Data & Knowledge Engineering},
	Keywords = {Business processes, Process mining, Clustering, Decision trees},
	Number = {12},
	Pages = {1005 - 1029},
	Title = {Mining usage scenarios in business processes: Outlier-aware discovery and run-time prediction},
	Url = {http://www.sciencedirect.com/science/article/pii/S0169023X11000930},
	Volume = {70},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0169023X11000930},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.datak.2011.07.002}}

@article{JAGADEESHCHANDRABOSE2012117,
	Abstract = {Business processes leave trails in a variety of data sources (e.g., audit trails, databases, and transaction logs). Hence, every process instance can be described by a trace, i.e., a sequence of events. Process mining techniques are able to extract knowledge from such traces and provide a welcome extension to the repertoire of business process analysis techniques. Recently, process mining techniques have been adopted in various commercial BPM systems (e.g., BPM|one, Futura Reflect, ARIS PPM, Fujitsu Interstage, Businesscape, Iontas PDF, and QPR PA). Unfortunately, traditional process discovery algorithms have problems dealing with less structured processes. The resulting models are difficult to comprehend or even misleading. Therefore, we propose a new approach based on trace alignment. The goal is to align traces in such a way that event logs can be explored easily. Trace alignment can be used to explore the process in the early stages of analysis and to answer specific questions in later stages of analysis. Hence, it complements existing process mining techniques focusing on discovery and conformance checking. The proposed techniques have been implemented as plugins in the ProM framework. We report the results of trace alignment on one synthetic and two real-life event logs, and show that trace alignment has significant promise in process diagnostic efforts.},
	Author = {R.P. Jagadeesh Chandra Bose and Wil M.P. van der Aalst},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.is.2011.08.003},
	Issn = {0306-4379},
	Journal = {Information Systems},
	Keywords = {Diagnostics, Conformance, Alignment, Execution patterns, Process mining},
	Note = {Management and Engineering of Process-Aware Information Systems},
	Number = {2},
	Pages = {117 - 141},
	Title = {Process diagnostics using trace alignment: Opportunities, issues, and challenges},
	Url = {http://www.sciencedirect.com/science/article/pii/S0306437911001074},
	Volume = {37},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0306437911001074},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.is.2011.08.003}}

@article{MEJRI2013489,
	Abstract = {This paper is related to process mining, specifically the processes' discovery. Our goal, through this research work, is to build an approach that extracts a reference model, modeled in BPMN language, from the event logs related to different processes, based on the algorithm . We also aim to make the configuration of the extracted process models in BPMN language. So, we developed a plug-in in ProM environment. We tested this plug-in by using test cases for which preliminary results are encouraging.},
	Author = {Asma Mejri and Sonia Ayachi Ghannouchi},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.protcy.2013.12.054},
	Issn = {2212-0173},
	Journal = {Procedia Technology},
	Keywords = {Process Mining, Process Discovery, BPMN, event logs, ProM.},
	Note = {CENTERIS 2013 - Conference on ENTERprise Information Systems / ProjMAN 2013 - International Conference on Project MANagement/ HCIST 2013 - International Conference on Health and Social Care Information Systems and Technologies},
	Pages = {489 - 497},
	Title = {Discovering Reference Process Models in the Context of BPM Projects},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212017313002089},
	Volume = {9},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212017313002089},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.protcy.2013.12.054}}

@article{OKOYE2014203,
	Abstract = {Currently, automated learning systems are widely used for educational and training purposes within various organisations including, schools, universities and further education centres. There has been a big gap between the extraction of useful patterns from data sources to knowledge, as it is crucial that data is made valid, novel, potentially useful and understandable. To meet the needs of intended users, there is requirement for learning systems to embody technologies that support learners in achieving their learning goals and this process don't happen automatically. This paper propose a novel approach for automated learning that is capable of detecting changing trends in learning behaviours and abilities through the use of process mining techniques. The goal is to discover user interaction patterns within learning processes, and respond by making decisions based on adaptive rules centred on captured user profiles. The approach applies semantic annotation of activity logs within the learning process in order to discover patterns automatically by means of semantic reasoning. Therefore, our proposed approach is grounded on Semantic Modelling and Process Mining techniques. To this end, it is possible to apply effective reasoning methods to make inferences over a Learning Process Knowledge-Base that leads to automated discovery of learning patterns or behaviour.},
	Author = {Kingsley Okoye and Abdel-Rahman H. Tawil and Usman Naeem and Rabih Bashroush and Elyes Lamine},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.procs.2014.08.031},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {process model, semantic rules, process mining, user profile, learning behaviour, event logs},
	Note = {The 5th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2014)/ The 4th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH 2014)/ Affiliated Workshops},
	Pages = {203 - 210},
	Title = {A Semantic Rule-based Approach Supported by Process Mining for Personalised Adaptive Learning},
	Url = {http://www.sciencedirect.com/science/article/pii/S187705091400996X},
	Volume = {37},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S187705091400996X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2014.08.031}}

@article{BORREGO20145340,
	Abstract = {A business process (BP) consists of a set of activities which are performed in coordination in an organizational and technical environment and which jointly realize a business goal. In such context, BP management (BPM) can be seen as supporting BPs using methods, techniques, and software in order to design, enact, control, and analyze operational processes involving humans, organizations, applications, and other sources of information. Since the accurate management of BPs is receiving increasing attention, conformance checking, i.e., verifying whether the observed behavior matches a modelled behavior, is becoming more and more critical. Moreover, declarative languages are more frequently used to provide an increased flexibility. However, whereas there exist solid conformance checking techniques for imperative models, little work has been conducted for declarative models. Furthermore, only control-flow perspective is usually considered although other perspectives (e.g., data) are crucial. In addition, most approaches exclusively check the conformance without providing any related diagnostics. To enhance the accurate management of flexible BPs, this work presents a constraint-based approach for conformance checking over declarative BP models (including both control-flow and data perspectives). In addition, two constraint-based proposals for providing related diagnosis are detailed. To demonstrate both the effectiveness and the efficiency of the proposed approaches, the analysis of different performance measures related to a wide diversified set of test models of varying complexity has been performed.},
	Author = {Diana Borrego and Irene Barba},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.eswa.2014.03.010},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {Business process management, Process mining, Conformance checking, Diagnosis, Declarative business process models, Constraint programming},
	Number = {11},
	Pages = {5340 - 5352},
	Title = {Conformance checking and diagnosis for declarative business process models in data-aware scenarios},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417414001390},
	Volume = {41},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417414001390},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2014.03.010}}

@article{MONTANI201433,
	Abstract = {Objectives
Process model comparison and similar process retrieval is a key issue to be addressed in many real-world situations, and a particularly relevant one in medical applications, where similarity quantification can be exploited to accomplish goals such as conformance checking, local process adaptation analysis, and hospital ranking. In this paper, we present a framework that allows the user to: (i) mine the actual process model from a database of process execution traces available at a given hospital; and (ii) compare (mined) process models. The tool is currently being applied in stroke management.
Methods
Our framework relies on process mining to extract process-related information (i.e., process models) from data. As for process comparison, we have modified a state-of-the-art structural similarity metric by exploiting: (i) domain knowledge; (ii) process mining outputs and statistical temporal information. These changes were meant to make the metric more suited to the medical domain.
Results
Experimental results showed that our metric outperforms the original one, and generated output closer than that provided by a stroke management expert. In particular, our metric correctly rated 11 out of 15 mined hospital models with respect to a given query. On the other hand, the original metric correctly rated only 7 out of 15 models. The experiments also showed that the framework can support stroke management experts in answering key research questions: in particular, average patient improvement decreased as the distance (according to our metric) from the top level hospital process model increased.
Conclusions
The paper shows that process mining and process comparison, through a similarity metric tailored to medical applications, can be applied successfully to clinical data to gain a better understanding of different medical processes adopted by different hospitals, and of their impact on clinical outcomes. In the future, we plan to make our metric even more general and efficient, by explicitly considering various methodological and technological extensions. We will also test the framework in different domains.},
	Author = {Stefania Montani and Giorgio Leonardi and Silvana Quaglini and Anna Cavallini and Giuseppe Micieli},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.artmed.2014.07.001},
	Issn = {0933-3657},
	Journal = {Artificial Intelligence in Medicine},
	Keywords = {Process mining and comparison, Graph edit distance, Stroke management},
	Number = {1},
	Pages = {33 - 45},
	Title = {Improving structural medical process comparison by exploiting domain knowledge and mined information},
	Url = {http://www.sciencedirect.com/science/article/pii/S0933365714000815},
	Volume = {62},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0933365714000815},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.artmed.2014.07.001}}

@article{KARRAY201421,
	Abstract = {To meet increasing needs in the field of maintenance, we studied the dynamic aspect of process and services on a maintenance platform, a major challenge in process mining and knowledge engineering. Hence, we propose a dynamic experience feedback approach to exploit maintenance process behaviors in real execution of the maintenance platform. An active learning process exploiting event log is introduced by taking into account the dynamic aspect of knowledge using trace engineering. Our proposal makes explicit the underlying knowledge of platform users by means of a trace-based system called ``PETRA''. The goal of this system is to extract new knowledge rules about transitions and activities in maintenance processes from previous platform executions as well as its user (i.e. maintenance operators) interactions. While following a Knowledge Traces Discovery process and handling the maintenance ontology IMAMO, ``PETRA'' is composed of three main subsystems: tracking, learning and knowledge capitalization. The capitalized rules are shared in the platform knowledge base in order to be reused in future process executions. The feasibility of this method is proven through concrete use cases involving four maintenance processes and their simulation.},
	Author = {Mohamed-Hedi Karray and Brigitte Chebel-Morello and Noureddine Zerhouni},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.knosys.2014.03.010},
	Issn = {0950-7051},
	Journal = {Knowledge-Based Systems},
	Keywords = {Trace-based systems, Process extension, Process mining, Experience reuse, s-Maintenance platform},
	Note = {Enhancing Experience Reuse and Learning},
	Pages = {21 - 39},
	Title = {PETRA: Process Evolution using a TRAce-based system on a maintenance platform},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950705114000938},
	Volume = {68},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950705114000938},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.knosys.2014.03.010}}

@article{MAHENDRAWATHI2015588,
	Abstract = {This paper presents results of process mining implementation in a characteristically unstructured customer fulfilment process in a real Telecommunication Company. The aim of process mining implementation is firstly to discover the typical customer fulfilment business process. It is also aimed at assessing the current rate of completed customer fulfilment, the typical component required for the process and the lead time for different types of customer requests. The steps to achieve the goals are to prepare, extract the data and construct the event log from the company's in house built Customer Relationship Management systems. The event log is then processed using Disco and PROM tools. The complete event log when model with Disco results in a Spaghetti-like process model with 673 different variants. In order to identify typical process, the log is filtered to include only business variants with 1% case occurrence of the total case. This enables the identification of 18 typical business variants, which differ based on the order requested, sequence of activities and occurrence of Return Work Order. Based on the typical variants, the components required to fulfil a certain order are identified. Another important findings are the fact that the completion rate is very low (only 8%). This may due to the fact that the issues faced by the field officer in processing the order and the resolution are either recorded manually or in a different systems. Finally, findings from this study can be used by the company to improve their current business process. It also stressed out the importance of resolving data integration issues in implementation of process mining in real cases.},
	Author = {E.R. Mahendrawathi and Hanim Maria Astuti and Ayu Nastiti},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.procs.2015.12.167},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Process Mining, Unstructured Process, Customer Order Fulfilment},
	Note = {The Third Information Systems International Conference 2015},
	Pages = {588 - 596},
	Title = {Analysis of Customer Fulfilment with Process Mining: A Case Study in a Telecommunication Company},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050915036285},
	Volume = {72},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050915036285},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2015.12.167}}

@article{TAX201663,
	Abstract = {Process mining techniques aim to extract insights in processes from event logs. One of the challenges in process mining is identifying interesting and meaningful event labels that contribute to a better understanding of the process. Our application area is mining data from smart homes for elderly, where the ultimate goal is to signal deviations from usual behavior and provide timely recommendations in order to extend the period of independent living. Extracting individual process models showing user behavior is an important instrument in achieving this goal. However, the interpretation of sensor data at an appropriate abstraction level is not straightforward. For example, a motion sensor in a bedroom can be triggered by tossing and turning in bed or by getting up. We try to derive the actual activity depending on the context (time, previous events, etc.). In this paper we introduce the notion of label refinements, which links more abstract event descriptions with their more refined counterparts. We present a statistical evaluation method to determine the usefulness of a label refinement for a given event log from a process perspective. Based on data from smart homes, we show how our statistical evaluation method for label refinements can be used in practice. Our method was able to select two label refinements out of a set of candidate label refinements that both had a positive effect on model precision.},
	Author = {Niek Tax and Natalia Sidorova and Reinder Haakma and Wil M.P. van der Aalst},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.procs.2016.08.096},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Label refinement, Process Mining, Sensor Networks},
	Note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 20th International Conference KES-2016},
	Pages = {63 - 72},
	Title = {Log-based Evaluation of Label Splits for Process Models},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050916318853},
	Volume = {96},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050916318853},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2016.08.096}}

@article{LI2017260,
	Abstract = {Business Process Management (BPM) is a quickly developing management theory in recent years. The goal of BPM is to improve corporate performance by managing and optimizing the businesses process in and among enterprises. The goal is easier to achieve with the closed-loop feedback mechanism from business process execution to redesign in BPM life cycle, where the business process itself and the set of activities in BPM are viewed as a controlled object and a controller respectively. In this feedback control system, process mining plays an important role in generating feedback of process execution for redesign. However, the existing discovery methods cannot mine certain special structures from execution logs (e.g., implicit dependency, implicit place and short loops) correctly and their mining efficiencies cannot meet the requirements of online process mining. In this paper, we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery. A case study is presented for introducing how the mined model can be used in business process evolution. Results of experiments are described to show the improvements of the proposed algorithm compared with others.},
	Author = {Chuanyi Li and Jidong Ge and Liguo Huang and Haiyang Hu and Budan Wu and Hao Hu and Bin Luo},
	Date-Added = {2019-09-02 09:32:28 +0100},
	Date-Modified = {2019-09-02 09:32:28 +0100},
	Doi = {https://doi.org/10.1016/j.jss.2016.03.013},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Software cybernetics, Process discovery, Petri nets},
	Pages = {260 - 273},
	Title = {Software cybernetics in BPM: Modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121216000844},
	Volume = {124},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121216000844},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2016.03.013}}

@inproceedings{7557452,
	Abstract = {This paper describes a method for automatically detecting key performance indicator (KPI) thresholds by dividing and aggregating process instances on the basis of differences in process models. The thresholds can be used as an analysis axis of data exploration to investigate process models that are discovered from huge logs. The proposed method enables users to minimize the time needed to detect KPI thresholds through trial and error. We applied the method to real-life logs and experiment results showed that thresholds were detected for two types of KPIs. Although one type did not correlate with process patterns, the other highly correlated with them. Such findings are usually obtained from the domain knowledge of business users and analysis results acquired by data analysts with technical expertise. However, with our approach the thresholds can be detected automatically and this helps to expand process analysis for end users.},
	Author = {M. {Abe} and M. {Kudo}},
	Booktitle = {2016 IEEE International Conference on Services Computing (SCC)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SCC.2016.32},
	Keywords = {business data processing;data handling;business process analysis;KPI threshold detection;key performance indicator;data exploration;Monitoring;Analytical models;Context;Data models;Engines;Insurance;process analysis;business monitoring;process discovery;KPI;graph edit distance},
	Month = {June},
	Pages = {187-194},
	Title = {Analyzing Business Processes by Automatically Detecting KPI Thresholds},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/SCC.2016.32}}

@inproceedings{7549356,
	Abstract = {The strategic transition of media organizations to personalized information delivery has urged the need for richer methods to analyze the customers. Though useful in supporting the creation of recommender strategies, the current data mining techniques create complex models requiring often an understanding of techniques in order to interpret the results. This situation together with the recommender technologies deluge and the particularities of the news industry pose challenges to the news organization in making decisions about the most suitable strategy. Therefore, we propose process mining as a high-level, end-to-end solution to provide insights into the consumers' behavior and content dynamics. Specifically, we explore if it allows news organizations to analyze independently and effectively their data in order to support them in defining recommender strategies. The solution was implemented in a case study with the third largest news provider in Norway and yielded preliminary positive results. To our knowledge, this is the first attempt to apply a process mining methodology and adapt the techniques to support media industry with the recommender strategies.},
	Author = {E. V. {Epure} and J. {Espen Ingvaldsen} and R. {Deneckere} and C. {Salinesi}},
	Booktitle = {2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/RCIS.2016.7549356},
	Issn = {2151-1357},
	Keywords = {data analysis;data mining;recommender systems;strategic transition;media organizations;personalized information delivery;recommender strategies;data mining;recommender technologies;news organization;consumer behavior;content dynamics;data analysis;Norway;process mining;media industry;Data mining;Media;Recommender systems;Organizations;Biological system modeling;Hidden Markov models;Knowledge based systems;process mining;inferred intentional process models;behavioral analysis;recommender strategies;process mining methodology;news media case study},
	Month = {June},
	Pages = {1-12},
	Title = {Process mining for recommender strategies support in news media},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/RCIS.2016.7549356}}

@inproceedings{8253111,
	Abstract = {Prediction of the traffic flow in particular systems will expedite discovering of an optimal path for packet transmitting in dynamic wireless networks. The main goal is to predict traffic overload while changing a network topology. Machine learning techniques and process mining enables prediction of the traffic produced by several moving nodes. Several related approaches are observed. The idea of process mining approach is proposed.},
	Author = {K. {Krinkin} and E. {Kalishenko}},
	Booktitle = {2012 11th Conference of Open Innovations Association (FRUCT)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.23919/FRUCT.2012.8253111},
	Issn = {2305-7254},
	Keywords = {data mining;learning (artificial intelligence);prediction theory;telecommunication computing;telecommunication network topology;telecommunication traffic;wireless mesh networks;wireless mesh networks;process mining algorithms;dynamic wireless networks;network topology;packet transmission;traffic prediction flow;traffic overload prediction;machine learning techniques;Algorithm design and analysis;Prediction algorithms;Heuristic algorithms;Network topology;Data mining;Object oriented modeling;Time series analysis;Wireless mesh networks;routing;process mining;traffic overload},
	Month = {April},
	Pages = {88-94},
	Title = {Traffic prediction in wireless mesh networks using process mining algorithms},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.23919/FRUCT.2012.8253111}}

@inproceedings{7557586,
	Abstract = {Process mining techniques are able to improve processes by extracting knowledge from event logs commonly available in today's information systems. In the area, it is important to verify whether business goals can be satisfied. LTL (Linear Temporal Logic) verification is an important means for checking the goals automatically and exhaustively. However, writing formal language like LTL is difficult, and the properties by which the user's intentions are not reflected sufficiently have bad influence on the verification results. Therefore, it is needed to help writing correct LTL formula for users who do not have sufficient domain knowledge and knowledge of mathematical logic. We propose an approach for goal achievement prediction based on decision tree learning. It is conducted focusing on partial structures represented as event order relations of each trace. The proposed technique is evaluated on a phone repair process log.},
	Author = {H. {Horita} and H. {Hirayama} and T. {Hayase} and Y. {Tahara} and A. {Ohsuga}},
	Booktitle = {2016 5th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/IIAI-AAI.2016.174},
	Keywords = {business process re-engineering;data mining;decision trees;formal languages;learning (artificial intelligence);temporal logic;process mining approach;event logs partial structures;decision tree learning;knowledge extraction;business goals;LTL verification;linear temporal logic verification;formal language writing;LTL formula;goal achievement prediction;partial structures;trace event order relations;phone repair process log;Decision trees;Business;Feature extraction;Data mining;Training data;Prediction algorithms;business process management;process mining;requirements engineering;process aware information system;business constraints;linear temporal logic},
	Month = {July},
	Pages = {113-118},
	Title = {Process Mining Approach Based on Partial Structures of Event Logs and Decision Tree Learning},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/IIAI-AAI.2016.174}}

@inproceedings{7849946,
	Abstract = {It is widely observed that the poor event logs quality poses a significant challenge to the process mining project both in terms of choice of process mining algorithms and in terms of the quality of the discovered process model. Therefore, it is important to control the quality of event logs prior to conducting a process mining analysis. In this paper, we propose a qualitative model which aims to assess the quality of event logs before applying process mining algorithms. Our ultimate goal is to give process mining practitioners an overview of the quality of event logs which can help to indicate whether the event log quality is good enough to proceed to process mining and in this case, to suggest both the needed preprocessing steps and the process mining algorithm that is most tailored under such a circumstance. The qualitative model has been evaluated using both artificial and real-life case studies.},
	Author = {M. O. {Kherbouche} and N. {Laga} and P. {Masse}},
	Booktitle = {2016 IEEE Symposium Series on Computational Intelligence (SSCI)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SSCI.2016.7849946},
	Keywords = {data mining;event logs quality;process mining project;process mining algorithms;process mining analysis;Data mining;Complexity theory;Measurement;Algorithm design and analysis;Software algorithms;Heuristic algorithms;Finite element analysis;event logs;process mining;process mining algorithms;qualitative model},
	Month = {Dec},
	Pages = {1-8},
	Title = {Towards a better assessment of event logs quality},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/SSCI.2016.7849946}}

@inproceedings{7368480,
	Abstract = {In this research we applied process mining techniques in order to analyze work processes of a healthcare system. The main objective of the study was to investigate the performance of a private hospital treatment processes in Bangkok based on the event logs. Being aware of the fact that currently healthcare systems of majority of hospitals worldwide are equipped with information systems, provided us a great opportunity to access large amounts of the medical data with the intention of the research and knowledge discovery purposes. In this paper, we emphasized on the ``Time Performance'' of the process instances of the collected event logs from different wards/sections of a hospital in order to better visualize and study the behavior of patients referring to the following sections/wards (as well as the hospital's administrators/personnel attending to each case) during the entire treatment processes. . The results showed that the treatment process with respect to the waiting time was too long between the wards ``Irradiation cystitis'' and ``Osteoradionecrosi'' sections allocating 7.8 waiting time to themselves. Subsequently, the findings of the research can be used in order to help the hospital administrators and managers to better understand the amount of waiting time spent between different treatment processes in such a way that they can improve the performance of handling patients' demands and needs in a more efficient, effective and timely manner, eventually leading to increased customer satisfaction and better performance.},
	Author = {P. {Jaisook} and W. {Premchaiswadi}},
	Booktitle = {2015 13th International Conference on ICT and Knowledge Engineering (ICT Knowledge Engineering 2015)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ICTKE.2015.7368480},
	Issn = {2157-099X},
	Keywords = {customer satisfaction;data mining;health care;medical information systems;patient treatment;time performance analysis;medical treatment process;process mining techniques;healthcare system;private hospital treatment process;Bangkok;event logs;hospital information systems;knowledge discovery;patient behavior;Osteoradionecrosi sections;irradiation cystitis sections;waiting time allocation;hospital administrators;hospital managers;patient handling;customer satisfaction;Disco;Hospitals;Radiation effects;Data models;Diseases;Open systems;Process Mining;time performance analysis;Disco Fluxicon;medical event log;hospital information systems},
	Month = {Nov},
	Pages = {110-115},
	Title = {Time performance analysis of medical treatment processes by using disco},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICTKE.2015.7368480}}

@inproceedings{6661848,
	Abstract = {In recent years research on the integration of BPM and social software has tried to overcome the limitations of the traditional BPM approaches. The potential of Social BPM (SBPM) and social software towards the enhancement and advancement of the traditional BPM lifecycle have been argued. This paper aims to address gaps in social BPM research by working towards a goal-driven SBPM meta-model that seamlessly integrate the process design and enactment stages. We argue that this approach will lead to truly social driven process enactment environments.},
	Author = {M. E. {Rangiha} and B. {Karakostas}},
	Booktitle = {2013 Science and Information Conference},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Keywords = {business data processing;social sciences computing;goal-driven social business process management;social software;social BPM;BPM lifecycle;goal-driven SBPM meta-model;process design;social driven process enactment environments;Software;Collaboration;Process design;Adaptation models;Runtime;Organizations;BPM;Social Software;Goal-Based Modeling;Social BPM;Process Discovery;Process Enactment},
	Month = {Oct},
	Pages = {894-901},
	Title = {Goal-driven social business process management},
	Year = {2013}}

@inproceedings{5990012,
	Abstract = {Process mining serves a bridge between data mining and business process modeling. The goal is to extract process related knowledge from event data stored in information systems. One of the most challenging process mining tasks is process discovery, i.e., the automatic construction of process models from raw event logs. Today there are dozens of process discovery techniques generating process models using different notations (Petri nets, EPCs, BPMN, heuristic nets, etc.). This paper focuses on the representational bias used by these techniques. We will show that the choice of target model is very important for the discovery process itself. The representational bias should not be driven by the desired graphical representation but by the characteristics of the underlying processes and process discovery techniques. Therefore, we analyze the role of the representational bias in process mining.},
	Author = {W. M. P. {van der Aalst}},
	Booktitle = {2011 IEEE 20th International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/WETICE.2011.64},
	Issn = {1524-4547},
	Keywords = {business process re-engineering;data mining;information systems;process mining;data mining;business process modeling;information systems;process discovery technique;representational bias;Data mining;Registers;Noise;Data models;Noise measurement;Organizations},
	Month = {June},
	Pages = {2-7},
	Title = {On the Representational Bias in Process Mining},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/WETICE.2011.64}}

@inproceedings{7321401,
	Abstract = {Based on the state of the art of process mining, we can conclude that quality characteristics (failure rate metrics or loops) are poorly represented or absent in most predictive models that can be found in the literature. The main goal of this present research work is to analyze how to learn prediction model defining failure as response variable. A model of this type can be used for active real-time-controlling (e. g. through the reassignment of workflow activities based on prediction results) or for the automated support of redesign (i.e., prediction results are transformed in software requirements used to implement process improvements). The proposed methodology is based on the application of a data mining process because the objective of this work can be considered as a data mining goal.},
	Author = {M. S. {Camara} and I. {Fall} and G. {Mendy} and S. {Diaw}},
	Booktitle = {2015 19th International Conference on System Theory, Control and Computing (ICSTCC)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ICSTCC.2015.7321401},
	Keywords = {business data processing;data mining;system recovery;process mining;activity failure prediction;quality characteristics;failure rate metrics;predictive models;response variable;active real-time-controlling;workflow activities;automated support;software requirements;process improvements;data mining goal;business process management;BPM;Data mining;Business;Predictive models;Data models;Analytical models;Process control;Measurement;Process mining;Workflow management software;Business Process Management;Data mining;Supervised learning},
	Month = {Oct},
	Pages = {854-859},
	Title = {Activity failure prediction based on process mining},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSTCC.2015.7321401}}

@inproceedings{7293494,
	Abstract = {Given the ever changing needs of the job markets, education and training centers are increasingly held accountable for student success. Therefore, education and training centers have to focus on ways to streamline their offers and educational processes in order to achieve the highest level of quality in curriculum contents and managerial decisions. Educational process mining is an emerging field in the educational data mining (EPM) discipline, concerned with developing methods to discover, analyze and provide a visual representation of complete educational processes. In this paper, we present our distributed computation platform, under construction, which allows different education centers and institutions to load their data and access to advanced data mining and process mining services. To achieve this, we present also a comparative study of the different clustering techniques developed in the context of process mining to partition efficiently educational traces. Our goal is to find the best strategy for distributing heavy analysis computations on many processing nodes of our platform.},
	Author = {A. {Hicheur Cairns} and B. {Gueni} and H. {Hafdi} and C. {Joubert} and N. {Khelifa}},
	Booktitle = {2015 International Conference on Protocol Engineering (ICPE) and International Conference on New Technologies of Distributed Systems (NTDS)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/NOTERE.2015.7293494},
	Issn = {2162-190X},
	Keywords = {data mining;distributed processing;educational computing;educational institutions;pattern clustering;distributed computation platform;educational process discovery;educational process analysis;student success;education center;training center;curriculum contents;managerial decisions;educational process mining;educational data mining;EPM;visual representation;education institutions;clustering techniques;Analytical models;Context;PROM;Training;Portals},
	Month = {July},
	Pages = {1-8},
	Title = {Towards a distributed computation platform tailored for educational process discovery and analysis},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/NOTERE.2015.7293494}}

@inproceedings{7336189,
	Abstract = {Process mining algorithms use event logs to learn and reason about processes by technically coupling event history data and process models. During the execution of a learning process, several events occur which are of interest and/or necessary for completing and achieving a learning goal. The work in this paper describes a Semantic Process Mining approach directed towards automated learning. The proposed approach involves the extraction of process history data from learning execution environments, which is then followed by submitting the resulting eXtensible Event Streams (XES) and Mining eXtensible Markup Language (MXML) format to the process analytics environment for mining and further analysis. The XES and MXML data logs are enriched by using Semantic Annotations that references concepts in an Ontology specifically designed for representing learning processes. This involves the identification and modelling of data about different users. The approach focuses on augmenting information values of the resulting model based on individual learner profiles. A series of validation experiments were conducted in order to prove how Semantic Process Mining can be utilized to address the problem of analyzing concepts and relationships amongst learning objects, which also aid in discovering new and enhancement of existing learning processes. To this end, we demonstrate how data from learning processes can be extracted, semantically prepared, and transformed into mining executable formats for improved analysis.},
	Author = {K. {Okoye} and A. R. H. {Tawil} and U. {Naeem} and E. {Lamine}},
	Booktitle = {2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/HPCC-CSS-ICESS.2015.164},
	Keywords = {data mining;learning (artificial intelligence);ontologies (artificial intelligence);XML;semantic process mining algorithm;learning model analysis;automated learning;resulting extensible event streams;XES;mining extensible markup language;MXML;semantic annotations;ontology;Data mining;Semantics;Cognition;Ontologies;Data models;Context;History;process model;learning process;semantic annotation;ontology;process mining;event logs},
	Month = {Aug},
	Pages = {363-370},
	Title = {Semantic Process Mining Towards Discovery and Enhancement of Learning Model Analysis},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/HPCC-CSS-ICESS.2015.164}}

@inproceedings{6567291,
	Abstract = {Innovation processes can be defined as sets of goal-driven activities, deeply influenced by human experience and behavior. With respect to traditional, structured operational processes, they present an high degree of uncertainty and heterogeneity with little or no structure. As a consequence, traditional business intelligence tools are mostly not suitable for innovation processes. Although the innovation promotion has become one of the hottest topic in business economy in last decades and its importance in organization growth is widely recognized, currently there are not proposal in Literature for the automatic analysis of innovation activities performed by an organization. Our research is hence aimed to investigate such activities and their corresponding innovation processes, by taking into account the way in which they are really performed in organization daily job. In the present work we briefly sketch the main issues related to innovation activities and their automatic support, firstly considering the current state of the art in Literature and then describing the main ideas of our proposal.},
	Author = {L. {Genga}},
	Booktitle = {2013 International Conference on Collaboration Technologies and Systems (CTS)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/CTS.2013.6567291},
	Keywords = {competitive intelligence;data mining;economics;innovation management;process mining techniques;innovation analysis;innovation process;goal-driven activities;business intelligence;business economy;Technological innovation;Organizations;Patents;Collaboration;Data mining;Data analysis;open innovation;process mining;pattern discovery},
	Month = {May},
	Pages = {584-587},
	Title = {Application of process mining techniques for innovation analysis and support},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/CTS.2013.6567291}}

@inproceedings{6414158,
	Abstract = {The basic function of audit is finding risk and preventing fraud from occurring as well as maintaining healthy, safe operation of enterprise and even the whole economy. By combining process mining techniques and risk management theory and using the technique of obtaining evidence on fraud risk as a trial, this paper takes business process risk auditing of process-aware information systems as its research goal. The paper aims to find out faults in the business process and audit evidence. It also aims to propose process mining-based risk auditing models of information systems from the perspective of workflow and in allusion to complicated business process. This paper identifies risk and implements continuous audit and monitor as well as searches risk auditing mechanisms and risk control method by using consistency analysis between actual business process and pre-designed business.},
	Author = {Z. {Huang} and Q. {Cong} and J. {Hu}},
	Booktitle = {2012 International Conference on Management Science Engineering 19th Annual Conference Proceedings},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ICMSE.2012.6414158},
	Issn = {2155-1855},
	Keywords = {auditing;data mining;fraud;information systems;risk management;information system risk auditing model;fraud prevention;safe operation;healthy operation;risk management theory;process mining techniques;fraud risk;process-aware information systems;audit evidence;business process;risk control method;risk auditing mechanisms;consistency analysis;Information systems;Monitoring;Process control;Data mining;Immune system;Organizations;process mining;risk auditing mechanism;continuous auditing;continuous monitoring},
	Month = {Sep.},
	Pages = {39-45},
	Title = {Information system risk auditing model based on process mining},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICMSE.2012.6414158}}

@inproceedings{6570621,
	Abstract = {This paper presents a research in progress that aims to design and develop a web-based shared environment for stakeholders involved in disaster management. The goal of this environment is two-fold. Firstly it will provide a reliable disaster information source to facilitate the exchange and the analysis of previous crisis information. Secondly, it will assimilate best practices and provide recommendations based on experiences from previous disasters. One of the first steps towards such an environment is to elaborate a common and generic disaster model. This model is also a reference to define a template for the case base of previous disasters. In order for our system to provide recommendations based on previous practices, we combine case based reasoning with process mining. This article presents the first step towards a disaster management decision support system, specifically providing guidance on how to integrate process mining in the case based reasoning cycle.},
	Author = {S. {Triki} and N. B. B. {Saoud} and J. {Dugdale} and C. {Hanachi}},
	Booktitle = {2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/WETICE.2013.77},
	Issn = {1524-4547},
	Keywords = {case-based reasoning;data mining;decision support systems;emergency management;disaster management decision support system;generic disaster model;crisis information;reliable disaster information source;Web based shared environment;Web based crisis management decision support system;process mining;case based reasoning;Cognition;Unified modeling language;Disaster management;Data mining;Crisis management;Databases;Electronic mail;Generic Disaster Model; Process Mining; Case Based Reasoning; Previous practices; Crisis Management Decision Support.},
	Month = {June},
	Pages = {245-252},
	Title = {Coupling Case Based Reasoning and Process Mining for a Web Based Crisis Management Decision Support System},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/WETICE.2013.77}}

@inproceedings{4682496,
	Abstract = {The goal of this research is to provide an alternative for business processes evaluation and tracking, based on the analysis of non-structured information generated by such processes within the organization areas. In this article we introduce a method to determine the occurrence probability of a business process within the enterprisepsilas text documents. The proposed method introduces the use of Statistical language model (SLM), as a new technique in business processes mining area. In order to obtain this objective the following is considered: the probability that a sub process or a process part is in the text paragraph; the probability that this text belongs to a business process; the language model of the processes set; and the set of realized activities which is reconstructed according to the processes that gave origin to the analyzed documents.},
	Author = {D. R. {Pelayo} and R. A. T. {Ram{\'\i}rez}},
	Booktitle = {2008 Seventh Mexican International Conference on Artificial Intelligence},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/MICAI.2008.49},
	Keywords = {business data processing;data mining;text analysis;business process mining;statistical languages model;business processes evaluation;nonstructured information;business process occurrence probability;enterprise text documents;text paragraph;Information analysis;Probability;Artificial intelligence;Decision making;Information retrieval;Intelligent networks;Computer networks;Genetic algorithms;Data mining;Biological neural networks;business process management;text mining},
	Month = {Oct},
	Pages = {404-407},
	Title = {Business Process Mining by Means of Statistical Languages Model},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/MICAI.2008.49}}

@inproceedings{4276259,
	Abstract = {Today there are many process mining techniques that allow for the automatic construction of process models based on event logs. Unlike synthesis techniques (e.g., based on regions), process mining aims at the discovery of models (e.g., Petri nets) from incomplete information (i.e., only example behavior is given). The more mature process mining techniques perform well on structured processes. However, most of the existing techniques fail miserably when confronted with unstructured processes. This paper attempts to "bring structure to the unstructured" by using an integrated combination of abstraction and clustering techniques. The ultimate goal is to present process models that are understandable by analysts and that lead to improved system/process redesigns.},
	Author = {W. M. P. {van der Aalst} and C. W. {Gunther}},
	Booktitle = {Seventh International Conference on Application of Concurrency to System Design (ACSD 2007)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ACSD.2007.50},
	Issn = {1550-4808},
	Keywords = {data mining;enterprise resource planning;unstructured processes;process mining;event logs;abstraction;clustering techniques;Bismuth;Petri nets;Concurrent computing;Hospitals;Embedded system;Visualization;Roads;Humans;PROM;Design methodology},
	Month = {July},
	Pages = {3-12},
	Title = {Finding Structure in Unstructured Processes: The Case for Process Mining},
	Year = {2007},
	Bdsk-Url-1 = {https://doi.org/10.1109/ACSD.2007.50}}

@inproceedings{7804106,
	Abstract = {This research provides topological discovery process that took place and recorded in the log file history. The event log contains information of patient, date/time of the treatment, medical procedures, responsible staff, and case IDs. The goal of this research to verify the conformity of the desinated process workflow using Mine Transition Systems by comparing it with the actual activities recorded in the log file. By using a generated model (Petri Net), a guideline can be developed and used to improve the overall performance of the medical process. The inconsistency between the anticipated workflow and the extracted one from the log file history were found in this study. The main issue was an extensive delay in waiting time for treatment and for payment (prior to medication dispense). These results and findings can be presented to the people that are in charge and hence be used to solve this issue towards better service performance of the hospital.},
	Author = {T. {Mettiyaporn} and K. {Kungcharoen} and W. {Premchaiswadi}},
	Booktitle = {2016 14th International Conference on ICT and Knowledge Engineering (ICT KE)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ICTKE.2016.7804106},
	Issn = {2157-099X},
	Keywords = {hospitals;medical information systems;Petri nets;admission procedures;hospital;topological discovery process;service performance;Petri net;mine transition systems;process workflow;case ID;responsible staff;medical procedures;event log;log file history;Decision support systems;Process Mining;ProM;Mine Transition System},
	Month = {Nov},
	Pages = {105-108},
	Title = {Using transition systems and regions to analyze and monitor admission procedures of a hospital},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICTKE.2016.7804106}}

@inproceedings{7897255,
	Abstract = {Clinical pathway is a crucial tool for care quality improvement and expense control. Compared to expertdesigned clinical pathway, the topic-based clinical pathway mined from history data is more dynamic and adaptive for real-world application. In this approach, the latent topics discovered by topic modeling are treated as the clinical goals, so that each patient trace is converted to a topic-based sequence. Process mining is used to generate a concise process model on these sequences. However, there is a common problem about the topic modeling that the redundancy between different topics is considerable. It means that some clinical activities strongly correlate to many topics, which has a significant impact on the ability of representing clinical goals of the topics. This paper proposed a novel topic modeling method for clinical goal discovering. Domain knowledge is incorporated to limit the available topic size for each clinical activity. Experiments demonstrate the effectiveness of our approach in discovering quality topics as the clinical goals for clinical pathway mining.},
	Author = {X. {Xu} and T. {Jin} and Z. {Wei} and J. {Wang}},
	Booktitle = {2017 IEEE EMBS International Conference on Biomedical Health Informatics (BHI)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/BHI.2017.7897255},
	Keywords = {data mining;medical computing;domain knowledge;clinical goal discovering;clinical pathway mining;care quality improvement;expense control;topic modeling;process mining;Data mining;Hospitals;Blood;Electrocardiography;Integrated circuits},
	Month = {Feb},
	Pages = {261-264},
	Title = {Incorporating domain knowledge into clinical goal discovering for clinical pathway mining},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1109/BHI.2017.7897255}}

@inproceedings{6573118,
	Abstract = {This thesis proposes a hybrid log alert detection scheme, which incorporates anomaly detection and signature generation to accomplish its goal. Unlike previous work, minimum apriori knowledge of the system being analyzed is assumed. This assumption enhances the platform portability of the framework. The anomaly detection component works in a bottom-up manner on the contents of historical system log data to detect regions of the log, which contain anomalous (alert) behaviour. The identified anomalous regions (after inspection by a human administrator through a visualization system) are then passed to the signature generation component, which mines them for patterns. Consequently, future occurrences of the underlying alert in the anomalous log region, can be detected on a production system using the discovered patterns. The combination of anomaly detection and signature generation, which is novel when compared to previous work, ensures that a framework which is accurate while still being able to detect new and unknown alerts is attained.},
	Author = {A. {Makanju} and A. N. {Zincir-Heywood} and E. E. {Milios}},
	Booktitle = {2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Issn = {1573-0077},
	Keywords = {digital signatures;investigating event log analysis;minimum Apriori information;hybrid log alert detection scheme;platform portability;anomaly detection component;anomalous regions;visualization system;human administrator;signature generation component;Data visualization;Itemsets;Computers;Monitoring;Production systems;Data mining;Semantics;Algorithms;Networked Systems;System Management;Modeling and Assessment},
	Month = {May},
	Pages = {962-968},
	Title = {Investigating event log analysis with minimum apriori information},
	Year = {2013}}

@inproceedings{6051638,
	Abstract = {Software requirements documents (SRDs) are often authored in general-purpose rich-text editors, such as MS Word. SRDs contain instances of logical structures, such as use case, business rule, and functional requirement. Automated recognition and extraction of these instances enables advanced requirements management features, such as automated traceability, template conformance checking, guided editing, and interoperability with requirements management tools such as RequisitePro. The variability in content and physical representation of these instances poses challenges to their accurate recognition and extraction. To address these challenges, we present a framework allowing 1) the specification of logical structures in terms of their content, textual rendering, and variability and 2) the extraction of instances of such structures from rich-text documents. Our evaluation involves 36 different logical structures identified in 43 SRDs and shows that the intended content, style, and variability of these structures can be specified in the framework such that their instances can be extracted from the documents with high precision and recall, both close to 100%.},
	Author = {R. {Rauf} and M. {Antkiewicz} and K. {Czarnecki}},
	Booktitle = {2011 IEEE 19th International Requirements Engineering Conference},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/RE.2011.6051638},
	Issn = {2332-6441},
	Keywords = {document handling;formal specification;open systems;logical structure extraction;software requirements documents;SRD;automated recognition;automated traceability;template conformance checking;requirements management;textual rendering;rich-text documents;Unified modeling language;Feature extraction;Software;Organizations;Portable document format;Text analysis;Web pages;Logical Structures;SRS;Requirements Extraction;Software Requirements Documents},
	Month = {Aug},
	Pages = {101-110},
	Title = {Logical structure extraction from software requirements documents},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/RE.2011.6051638}}

@inproceedings{6681336,
	Abstract = {Context. Boilerplates have long been used in Requirements Engineering (RE) to increase the precision of natural language requirements and to avoid ambiguity problems caused by unrestricted natural language. When boilerplates are used, an important quality assurance task is to verify that the requirements indeed conform to the boilerplates. Objective. If done manually, checking conformance to boilerplates is laborious, presenting a particular challenge when the task has to be repeated multiple times in response to requirements changes. Our objective is to provide automation for checking conformance to boilerplates using a Natural Language Processing (NLP) technique, called Text Chunking, and to empirically validate the effectiveness of the automation. Method. We use an exploratory case study, conducted in an industrial setting, as the basis for our empirical investigation. Results. We present a generalizable and tool-supported approach for boilerplate conformance checking. We report on the application of our approach to the requirements document for a major software component in the satellite domain. We compare alternative text chunking solutions and argue about their effectiveness for boilerplate conformance checking. Conclusion. Our results indicate that: (1) text chunking provides a robust and accurate basis for checking conformance to boilerplates, and (2) the effectiveness of boilerplate conformance checking based on text chunking is not compromised even when the requirements glossary terms are unknown. This makes our work particularly relevant to practice, as many industrial requirements documents have incomplete glossaries.},
	Author = {C. {Arora} and M. {Sabetzadeh} and L. {Briand} and F. {Zimmer} and R. {Gnaga}},
	Booktitle = {2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ESEM.2013.13},
	Issn = {1949-3770},
	Keywords = {conformance testing;formal verification;natural language processing;quality assurance;text analysis;requirements engineering;natural language requirements;quality assurance task;natural language processing technique;NLP technique;tool-supported approach;boilerplate conformance checking;requirements document;software component;satellite domain;text chunking solutions;Terminology;Natural language processing;Syntactics;Pipelines;Software;Surveillance;Requirement Boilerplates;Natural Language Processing (NLP);Text Chunking;Case Study Research},
	Month = {Oct},
	Pages = {35-44},
	Title = {Automatic Checking of Conformance to Requirement Boilerplates via Text Chunking: An Industrial Case Study},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/ESEM.2013.13}}

@inproceedings{4782559,
	Abstract = {Service oriented architecture (SOA) includes several building blocks among which orchestration engine demands special attention. Although, there are a number of centralized orchestration engines to execute business processes described by BPEL language in SOA, you may find several decentralized orchestration engines and their purpose is to decompose a BPEL process to several software agents to improve some quality factors. On one hand, choosing a suitable method of process distribution may result in better adaptability of process with run-time environment. On the other hand, in the new generation of service oriented architecture (SOA), service level agreements (SLAs) are of paramount important. We need to have a run-time environment to improve the adaptability of run-time environment with SLAs. Thus, this paper we are going to to combine process distribution methods and SLAs. To reach this goal, firstly, we introduce an intelligent method of using process mining for business process distribution (IPD). Secondly, we compare different methods of processes distribution including fully, semi and intelligent process distribution methods. We also show the comparison of these methods considering quality factors such as total execution time, band width usage, agent granularity, resource adaptability, memory usage of agents, number of produced agents and total system adaptability. Thirdly, we consider all of the distribution methods from an SLA view through which users can determine their requirements of executing business processes to be mapped to run-time environment.},
	Author = {F. S. {Esfahani} and M. A. A. {Murad} and M. N. {Sulaiman} and N. I. {Udzir}},
	Booktitle = {2009 International Conference on Information, Process, and Knowledge Management},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/eKNOW.2009.14},
	Keywords = {software agents;software architecture;software quality;SLA-driven business process distribution;service oriented architecture;orchestration engine;centralized orchestration engines;BPEL language;software agents;quality factors;service level agreements;process mining;total execution time;band width usage;agent granularity;resource adaptability;Engines;Runtime environment;Service oriented architecture;Intelligent agent;Q factor;Computer science;Semiconductor optical amplifiers;Knowledge management;Computer architecture;Software agents;Service Level Agreement (SLA);Adaptive Systems;Business Process Mining;BPEL;Service Oriented Architecture;Mobile Agents;Workflow;Distributed Orchestrate Engine},
	Month = {Feb},
	Pages = {14-21},
	Title = {SLA-Driven Business Process Distribution},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/eKNOW.2009.14}}

@article{6906268,
	Abstract = {Many studies were devoted to the analysis and the detection of electromagnetic attacks against critical electronic systems at the system or the component levels. Some attempts have been made to correlate effects scenarios with events logged by the kernel of the operating system (OS) of commercial-off-the-shelf computer running Windows. Due to the closed principle of the last OS, we decided to perform such an analysis on a computer running a Linux distribution in which a complete access to logs is available. It will be demonstrated that a computer running such an open OS allows detecting the perturbations induced by intentional electromagnetic interferences at different levels of the targeted computer.},
	Author = {C. {Kasmi} and J. {Lopes-Esteves} and N. {Picard} and M. {Renard} and B. {Beillard} and E. {Martinod} and J. {Andrieu} and M. {Lalande}},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/TEMC.2014.2357060},
	Issn = {0018-9375},
	Journal = {IEEE Transactions on Electromagnetic Compatibility},
	Keywords = {computational electromagnetics;electromagnetic interference;Linux;operating system kernels;security;intentional electromagnetic interferences;Linux distribution;Windows;commercial-off-the-shelf computer;OS kernel;operating system kernel;critical electronic systems;electromagnetic attack detection;electromagnetic attack analysis;IEMI exposure;COTS computer;event log generation;Universal Serial Bus;Hardware;Sensors;Electromagnetic interference;Monitoring;Electromagnetic compatibility (EMC);electromagnetic interference;software engineering;system analysis and design;Electromagnetic compatibility (EMC);electromagnetic interference;software engineering;system analysis and design},
	Month = {Dec},
	Number = {6},
	Pages = {1723-1726},
	Title = {Event Logs Generated by an Operating System Running on a COTS Computer During IEMI Exposure},
	Volume = {56},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/TEMC.2014.2357060}}

@inproceedings{4125665,
	Abstract = {The decision of planning and scheduling for phosphor-chemical enterprise is very complex and difficult, which is a research area that needs to be studied thoroughly. In this paper, a model optimizing planning and scheduling for a phosphor-chemical enterprise is presented in consideration of the operation processes, including mining, mineral processing and chemical fertilizer producing. Also, some numerical examples for the model are computed and analyzed for the purpose of illustration. The results show that it is very useful for an enterprise to optimize production planning and scheduling},
	Author = {T. {Fan} and H. {Li} and D. {Xi}},
	Booktitle = {2006 IEEE International Conference on Service Operations and Logistics, and Informatics},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SOLI.2006.329023},
	Keywords = {chemical industry;decision making;fertilisers;mineral processing industry;mining;optimisation;production planning;scheduling;supply chains;optimization;planning;scheduling;phosphor-chemical enterprises;planning decision;scheduling decision;operation process;mining;mineral processing;chemical fertilizer production;production planning;supply chains;Ores;Job shop scheduling;Processor scheduling;Production planning;Supply chains;Minerals;Analytical models;Fertilizers;Chemical processes;Chemical industry;Goal Programming;Optimization;Planning and Scheduling;Supply Chain},
	Month = {June},
	Pages = {689-695},
	Title = {Optimization of planning and scheduling for phosphor-chemical enterprises},
	Year = {2006},
	Bdsk-Url-1 = {https://doi.org/10.1109/SOLI.2006.329023}}

@inproceedings{7382168,
	Abstract = {We propose a new approach to decision support system based on conceptualized and systemized domain knowledge for reducing semantic heterogeneity and cognitive biases among multiple data resource and achieving spatial data integration, exchange & share of mine enterprises. Firstly, an architecture and design ideas for data warehouse are established with formalized geo-semantic knowledge. Then the hybrid geoontology model and a goal-driven modeling methods covering static knowledge, dynamic events and humans actors of mine production process is presented based on mining terms, concepts, entities and characteristics. Finally, algorithms and examples to automatically build multidimensional model have been given according to mapping rules between OWL and XML. It is tested that the proposed method has superior usability, could implement standardized and sharing multidimensional models of data warehouses and provide a new way for data integration and decision-making application from different business domains.},
	Author = {X. {Liu} and G. {Lei} and F. {Ren} and H. {Ma}},
	Booktitle = {2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/FSKD.2015.7382168},
	Keywords = {data integration;data warehouses;decision making;decision support systems;knowledge representation languages;mining;ontologies (artificial intelligence);XML;data warehouse building;mine-production geoontology;decision support system;conceptualized domain knowledge;systemized domain knowledge;semantic heterogeneity reduction;cognitive bias;multiple data resource;spatial data integration;spatial data exchange;spatial data sharing;mine enterprise;formalized geo-semantic knowledge;goal-driven modeling method;static knowledge;dynamic events;mine production process;mining terms;mining concepts;mining entities;mining characteristics;multidimensional model;OWL;XML;decision making;Ontologies;Production;Geology;Semantics;Data mining;Data warehouses;Fuel processing industries;mine production;data warehouse;geo-ontology;data integration},
	Month = {Aug},
	Pages = {1508-1513},
	Title = {Building data warehouses based on mine-production geoontology},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/FSKD.2015.7382168}}

@inproceedings{1495958,
	Abstract = {Due to the prevalence of distributed and coordinated Internet attacks, many researchers and network administrators study the nature and strategies of attackers. To analyze event logs, using intrusion detection systems and active network monitoring, honeynets are being deployed to attract potential attackers in order to investigate their modus operandi. The goal is to use honeynet clusters as real-time warning systems in production networks. Towards satisfying this objective, we have built a honeynet cluster and have run experiments to determine its effectiveness. Majority of the honeynets function in isolation, not sharing information in real time. In order to rectify this deficiency, the authors built a federation of cooperating honeynets (referred to as knowledge sharing honeynets) that shares knowledge of malicious traffic. This paper describes the methods in building a hardware assisted honeynet cluster and testing its effectiveness.},
	Author = {S. {Sudaharan} and S. {Dhammalapathi} and S. {Rai} and D. {Wijesekera}},
	Booktitle = {Proceedings from the Sixth Annual IEEE SMC Information Assurance Workshop},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/IAW.2005.1495958},
	Keywords = {security of data;computer networks;real-time systems;distributed Internet attacks;coordinated Internet attacks;network administration;attack strategy;event log analysis;intrusion detection systems;active network monitoring;honeynet clusters;real-time warning systems;production networks;cooperating honeynets;knowledge sharing honeynets;malicious traffic;Network servers;Web server;Switches;Intrusion detection;Monitoring;Telecommunication traffic;Real time systems;Production systems;Hardware;Military computing},
	Month = {June},
	Pages = {240-243},
	Title = {Knowledge sharing honeynets},
	Year = {2005},
	Bdsk-Url-1 = {https://doi.org/10.1109/IAW.2005.1495958}}

@inproceedings{7749453,
	Abstract = {Clinical Pathway is ubiquitous and plays an essential role in clinical workflow management. The combination of topic modeling and process mining is an efficient approach to get a non-static and topic-based process model. Topic modeling is used to group the activities of each clinical day into the latent topics, and process mining is used to generate a concise workflow model based on these topics. However, because of the specificity of clinical data, it usually suffers from the performance of topic modeling. In this paper, we take an important clinical practice, all the same activities in one clinical day tend to represent the same clinical goal, into account to enhance the effectiveness of topic modeling. The experiments on real data show significant performance gains of our approach.},
	Author = {X. {Xu} and T. {Jin} and J. {Wang}},
	Booktitle = {2016 IEEE 18th International Conference on e-Health Networking, Applications and Services (Healthcom)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/HealthCom.2016.7749453},
	Keywords = {data mining;patient care;patient monitoring;ubiquitous computing;clinical pathway mining;patient daily activities;clinical workflow management;process mining;topic-based process model;nonstatic model;latent topics;Data mining;Blood;Neurology;Drugs;Business;Vocabulary;clinical activity clustering;topic modeling;clinical pathway},
	Month = {Sep.},
	Pages = {1-6},
	Title = {Summarizing patient daily activities for clinical pathway mining},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/HealthCom.2016.7749453}}

@inproceedings{5718492,
	Abstract = {Social network analysis is primarily based in the investigation of ties between nodes and the groups that those ties form. Computer-mediated interaction has introduced many unique forms of tie data to the field. The form of data used in this research are traces of activity left when people create and edit digital artifacts, and when navigating around hyperlinked environments. Using intentional and unintentional traces of activity to generate social graphs provides a unique window into collaboration and interaction. This paper elaborates a technique that uses event log data to trace contingencies in user activity and generate directed two-mode graphs, associograms, which can then be abstracted to sociogram representations. The social network ties generated represent connections between people based on actions contingent on one another. These ties can be used to represent potential social connections for collaboration, social collectives for coordination, and stigmergic self-organization.},
	Author = {D. {Rosen} and D. D. {Suthers}},
	Booktitle = {2011 44th Hawaii International Conference on System Sciences},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/HICSS.2011.385},
	Issn = {1530-1605},
	Keywords = {directed graphs;groupware;human computer interaction;interactive systems;social networking (online);social network analysis;computer mediated interaction;digital artifacts;social graphs;event log data;directed two-mode graphs;associograms;sociogram representations;stigmergic self-organization;contingency tracing;Social network services;Collaboration;Organizations;Media;Navigation;Context;Communities},
	Month = {Jan},
	Pages = {1-10},
	Title = {Stigmergy and Collaboration: Tracing the Contingencies of Mediated Interaction},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/HICSS.2011.385}}

@inproceedings{7546007,
	Abstract = {A modern diagnostic imaging system integrates several PACS (Picture Archiving and Communication System) through datacenters that allow a large community of users to access and share sensitive patient medical images. In such integration user access to the medical images that are stored in non-local PACS systems is based on a trust model, which makes data integrity and privacy vulnerable due to possible malicious user behaviors. Moreover, the limited scope and precision of the existing policy-based access control solutions prevent them from detecting suspicious behaviors of the authenticated users. In this paper, we propose an approach for analyzing the user behaviors that allows the administrators to identify the users whose behaviors may jeopardize the data privacy and system integrity. In this context, the system administrator can define an arbitrary pattern of a suspicious user behavior using our new behavior pattern language. A constraint-based pattern-matching engine will identify the instances of the suspicious behavior pattern in the system's audit-log repository. Finally, a decision support system will present the excerpt findings to the system administrator with the overall goal of refining the access control policy rules. We present a case study which indicates our proposed approach provides promising results.},
	Author = {H. {Sharghi} and K. {Sartipi}},
	Booktitle = {2016 IEEE 29th International Symposium on Computer-Based Medical Systems (CBMS)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/CBMS.2016.58},
	Issn = {2372-9198},
	Keywords = {authorisation;data integrity;data privacy;decision support systems;medical image processing;message authentication;PACS;pattern matching;trusted computing;user behavior-based approach;insider threat detection;distributed diagnostic imaging systems;PACS;picture archiving and communication system;datacenters;trust model;data integrity;data privacy;malicious user behaviors;policy-based access control solutions;suspicious behavior detection;user authentication;user behavior analysis;user identification;system integrity;behavior pattern language;constraint-based pattern-matching engine;audit-log repository;decision support system;access control policy rules;Pattern matching;Picture archiving and communication systems;Security;Monitoring;Power line communications;Biomedical imaging;User Behavior;Security;Pattern Matching;Insider Threat;Diagnostic Imaging System;Event-log Repository},
	Month = {June},
	Pages = {300-305},
	Title = {A User Behavior-Based Approach to Detect the Insider Threat in Distributed Diagnostic Imaging Systems},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/CBMS.2016.58}}

@inproceedings{5188792,
	Abstract = {The main goal of change management is to ensure that standardized methods and procedures are used for the efficient and prompt handling of changes in IT systems, in order to minimize change-related incidents and service-delivery disruption. To meet this goal, it is of paramount importance reusing the experience acquired from previous changes in the design of subsequent ones. Two distinct approaches may be usefully combined to this end. In a top-down approach, IT operators may manually design change templates based on the knowledge owned/acquired in the past. Considering a reverse, bottom-up perspective, these templates could be discovered from past execution traces gathered from IT provisioning tools. While the former has been satisfactorily explored in previous investigations, the latter - despite its undeniable potential to result in accurate templates in a reduced time scale - has not been subject of research, as far as the authors are aware of, by the service operations and management community. To fill in this gap, this paper proposes a solution, inspired on process mining techniques, to discover change templates from past changes. The solution is analyzed through a prototypical implementation of a change template miner subsystem called CHANGEMINER, and a set of experiments based on a real-life scenario.},
	Author = {W. L. d. C. {Cordeiro} and G. S. {Machado} and F. G. {Andreis} and J. A. {Wickboldt} and R. C. {Lunardi} and A. D. {dos Santos} and C. B. {Both} and L. P. {Gaspary} and L. Z. {Granville} and D. {Trastour} and C. {Bartolini}},
	Booktitle = {2009 IFIP/IEEE International Symposium on Integrated Network Management},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/INM.2009.5188792},
	Issn = {1573-0077},
	Keywords = {data mining;information management;information systems;management of change;CHANGEMINER;IT change template;change management;standardized method;IT system;change-related incident;service-delivery disruption;IT provisioning tool;process mining;change template miner subsystem;Laboratories;Informatics;Prototypes;Information technology;Information management;Technology management;Companies;Context-aware services;Libraries;Best practices},
	Month = {June},
	Pages = {97-104},
	Title = {CHANGEMINER: A solution for discovering IT change templates from past execution traces},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/INM.2009.5188792}}

@inproceedings{5934820,
	Abstract = {In this paper, we introduce a novel approach to reachability analysis of dynamically routed networks. The goal is to determine the network-wide reachability using static analysis of configuration files gathered from forwarding devices. We describe a method that can compute the reachability in networks with a mix of static routing configurations, distance vector routing protocols, filtering routing updates and redistributions. The method computes a network-wide approximation of distributed routing information using the standard graph algorithms. Thus, for any network state, we can determine a set of active paths used for packet delivery. The outcomes of the method can be, for instance, used during the conformance checking of distributed access control lists against network security policies.},
	Author = {M. {Sveda} and O. {Rysavy} and G. {de Silva} and P. {Matousek} and J. {Rab}},
	Booktitle = {2011 18th IEEE International Conference and Workshops on Engineering of Computer-Based Systems},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/ECBS.2011.24},
	Keywords = {authorisation;computer network security;formal verification;IP networks;program diagnostics;reachability analysis;routing protocols;telecommunication computing;reachability analysis;dynamically routed networks;network-wide reachability;static analysis;configuration files;forwarding devices;static routing configurations;distance vector routing protocols;filtering routing updates;redistributions;network-wide approximation;distributed routing information;standard graph algorithms;network state;active paths;packet delivery;conformance checking;distributed access control;network security policy;Routing;Routing protocols;Ribs;Computational modeling;Access control;IP-networks;network configuration;network design;network reachability;routing protocols},
	Month = {April},
	Pages = {197-205},
	Title = {Reachability Analysis in Dynamically Routed Networks},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/ECBS.2011.24}}

@inproceedings{6468487,
	Abstract = {A large percentage of computing capacity in today's large high-performance computing systems is wasted because of failures. Consequently current research is focusing on providing fault tolerance strategies that aim to minimize fault's effects on applications. By far the most popular technique is the checkpointrestart strategy. A complement to this classical approach is failure avoidance, by which the occurrence of a fault is predicted and preventive measures are taken. This requires a reliable prediction system to anticipate failures and their locations. Thus far, research in this field has used ideal predictors that were not implemented in real HPC systems. In this paper, we merge signal analysis concepts with data mining techniques to extend the ELSA (Event Log Signal Analyzer) toolkit and offer an adaptive and more efficient prediction module. Our goal is to provide models that characterize the normal behavior of a system and the way faults affect it. Being able to detect deviations from normality quickly is the foundation of accurate fault prediction. However, this is challenging because component failure dynamics are heterogeneous in space and time. To this end, a large part of the paper is focused on a detailed analysis of the prediction method, by applying it to two large-scale systems and by investigating the characteristics and bottlenecks of each step of the prediction process. Furthermore, we analyze the prediction's precision and recall impact on current checkpointing strategies and highlight future improvements and directions for research in this field.},
	Author = {A. {Gainaru} and F. {Cappello} and M. {Snir} and W. {Kramer}},
	Booktitle = {SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SC.2012.57},
	Issn = {2167-4337},
	Keywords = {data mining;fault tolerant computing;microscopes;parallel processing;signal processing;large-scale systems;component failure dynamics;event log signal analyzer toolkit;ELSA toolkit;data mining techniques;checkpoint-restart strategy;fault tolerance strategies;high-performance computing systems;HPC systems;microscope;fault prediction;Correlation;Itemsets;Signal analysis;Data mining;Checkpointing;Algorithm design and analysis;Prediction algorithms;fault tolerance;large-scale HPC systems;signal analysis;fault detection},
	Month = {Nov},
	Pages = {1-11},
	Title = {Fault prediction under the microscope: A closer look into HPC systems},
	Year = {2012},
	Bdsk-Url-1 = {https://doi.org/10.1109/SC.2012.57}}

@inproceedings{7849948,
	Abstract = {Local Process Model (LPM) discovery is focused on the mining of a set of process models where each model describes the behavior represented in the event log only partially, i.e. subsets of possible events are taken into account to create so-called local process models. Often such smaller models provide valuable insights into the behavior of the process, especially when no adequate and comprehensible single overall process model exists that is able to describe the traces of the process from start to end. The practical application of LPM discovery is however hindered by computational issues in the case of logs with many activities (problems may already occur when there are more than 17 unique activities). In this paper, we explore three heuristics to discover subsets of activities that lead to useful log projections with the goal of speeding up LPM discovery considerably while still finding high-quality LPMs. We found that a Markov clustering approach to create projection sets results in the largest improvement of execution time, with discovered LPMs still being better than with the use of randomly generated activity sets of the same size. Another heuristic, based on log entropy, yields a more moderate speedup, but enables the discovery of higher quality LPMs. The third heuristic, based on the relative information gain, shows unstable performance: for some data sets the speedup and LPM quality are higher than with the log entropy based method, while for other data sets there is no speedup at all.},
	Author = {N. {Tax} and N. {Sidorova} and W. M. P. {van der Aalst} and R. {Haakma}},
	Booktitle = {2016 IEEE Symposium Series on Computational Intelligence (SSCI)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SSCI.2016.7849948},
	Keywords = {data mining;entropy;Markov processes;pattern clustering;heuristic approach;local process model generation;log projections;LPM discovery;process model mining;event log;Markov clustering approach;execution time improvement;log entropy;relative information gain;Petri nets;Unified modeling language;Computational modeling;Ventilation;Data mining;Mathematical model;Process modeling},
	Month = {Dec},
	Pages = {1-8},
	Title = {Heuristic approaches for generating Local Process Models through log projections},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/SSCI.2016.7849948}}

@inproceedings{6889360,
	Abstract = {Accurate prediction of the completion time of a business process instance would constitute a valuable tool when managing processes under service level agreement constraints. Such prediction, however, is a very challenging task. A wide variety of factors could influence the trend of a process instance, and hence just using time statistics of historical cases cannot be sufficient to get accurate predictions. Here we propose a new approach where, in order to improve the prediction quality, both the control and the data flow perspectives are jointly used. To achieve this goal, our approach builds a process model which is augmented by time and data information in order to enable remaining time prediction. The remaining time prediction of a running case is calculated combining two factors: (a) the likelihood of all the following activities, given the data collected so far; and (b) the remaining time estimation given by a regression model built upon the data.},
	Author = {M. {Polato} and A. {Sperduti} and A. {Burattin} and M. {de Leoni}},
	Booktitle = {2014 International Joint Conference on Neural Networks (IJCNN)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/IJCNN.2014.6889360},
	Issn = {2161-4407},
	Keywords = {business data processing;contracts;regression analysis;business process instance;service level agreement constraints;time statistics;data flow perspectives;time estimation;regression model;data-aware remaining time prediction;Vectors;Predictive models;Data mining;Business;Gold;Data models;Indexes;Data-aware Prediction;Process mining;Naive Bayes;Support Vector Regression},
	Month = {July},
	Pages = {816-823},
	Title = {Data-aware remaining time prediction of business process instances},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/IJCNN.2014.6889360}}

@inproceedings{7849923,
	Abstract = {This paper focuses on analysis of machine performance in a manufacturing company. Machine behavior can be complex, because it usually consists of many tasks. Performance of these tasks depends on product attributes, worker's speed, and therefore, analysis is not simple. Performance analysis results can be used for different purposes. Prediction and description are typical products of data mining. Prediction should be used for online monitoring of the manufactory process and as an input for a scheduler. Description can serve as information for managers to know which attributes of products cause problems more frequently. However manufacturing processes are complex, every process is quite unique. Our long term goal is to generalize the most common patterns to build general analyzer. This task is not simple because the lack of real word data and information. Therefore this work may contribute to the other researchers in their understanding of real world manufacturing problems.},
	Author = {M. {Posp{\'\i}{\v s}il} and V. {Bart{\'\i}k} and T. {Hru{\v s}ka}},
	Booktitle = {2016 IEEE Symposium Series on Computational Intelligence (SSCI)},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/SSCI.2016.7849923},
	Keywords = {data mining;production engineering computing;production equipment;manufactory process online monitoring;manufacturing company;data mining;machine performance analysis;Data mining;Employment;Manufacturing;Time measurement;Job shop scheduling;Algorithm design and analysis;Process mining;data mining;manufacturing;performance analysis;simulation;prediction;association rules;scheduling},
	Month = {Dec},
	Pages = {1-7},
	Title = {Analyzing Machine Performance Using Data Mining},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/SSCI.2016.7849923}}

@article{7089232,
	Abstract = {Edu-AREA is a Web 2.0 application whose main goal is to contribute to teaching innovation. It provides descriptions of educational resources and guidelines that can be used by teachers to create their lesson plans and later to document their teaching experiences. At the current stage of the Edu-AREA development, a main issue is related to the management and classification of information provided by users. This paper introduces a folksonomy approach, the architecture of the system, and the results of an experiment about keyword recommendations.},
	Author = {M. {Caeiro-Rodr{\'\i}guez} and J. M. {Santos-Gago} and M. {Lama} and M. {Llamas-Nistal}},
	Date-Added = {2019-09-02 09:27:03 +0100},
	Date-Modified = {2019-09-02 09:27:03 +0100},
	Doi = {10.1109/RITA.2015.2417953},
	Issn = {1932-8540},
	Journal = {IEEE Revista Iberoamericana de Tecnologias del Aprendizaje},
	Keywords = {pattern classification;recommender systems;social networking (online);teaching;keyword recommendation;information organization;Edu-AREA;Web 2.0 application;teaching innovation;educational resources;educational guidelines;lesson plan creation;teaching experience documentation;information classification;information management;folksonomy approach;system architecture;Education;Web 2.0;Ontologies;Organizations;Data models;Vocabulary;Semantics;Social network services;Educational activities;Information sharing;Tagging;Social network services;educational activities;information sharing;tagging},
	Month = {May},
	Number = {2},
	Pages = {60-68},
	Title = {A Keyword Recommendation Experiment to Support Information Organization and Folksonomies in Edu-AREA},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/RITA.2015.2417953}}
