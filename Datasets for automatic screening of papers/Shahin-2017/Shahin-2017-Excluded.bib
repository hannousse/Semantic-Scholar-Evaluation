%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Salima Yahiouche at 2020-10-24 20:45:24 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{Ding:2015:VAW:2802130.2802135,
	Abstract = {Offloading mobile traffic to WiFi networks (WiFi Offloading) is a cost-effective technique to alleviate the pressure on mobile networks for meeting the surge of data capacity demand. However, most existing proposals from standards developing organizations (SDOs) and research communities are facing a deployment dilemma, either due to overlooking device limitations, lack user incentives, or missing operator supports. In this position paper, we introduce an open-source platform for WiFi offloading to tackle the deployment challenge. Our solution leverages the programmable feature of software-defined networking (SDN) to enhance extensibility and deployability in a collaborative manner. Inspired by our field measurements covering 4G/LTE and 802.11ac/n, we exploit context awareness as a use case to demonstrate the efficacy of our solution. We also discuss the potential usage by cloud service providers given the opportunities behind the growing popularity of mobile virtual network operators (MVNO). We have released our platform under open-source licenses to encourage future collaboration and development with SDOs and research communities.},
	Acmid = {2802135},
	Address = {New York, NY, USA},
	Author = {Ding, Aaron Yi and Liu, Yanhe and Tarkoma, Sasu and Flinck, Hannu and Schulzrinne, Henning and Crowcroft, Jon},
	Booktitle = {Proceedings of the 6th International Workshop on Mobile Cloud Computing and Services},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:31:29 +0100},
	Doi = {10.1145/2802130.2802135},
	Isbn = {978-1-4503-3545-4},
	Keywords = {SDN, WiFi offloading, cloud service provider, mobile virtual network operator},
	Location = {Paris, France},
	Numpages = {5},
	Pages = {44--48},
	Publisher = {ACM},
	Series = {MCS '15},
	Title = {Vision: Augmenting WiFi Offloading with An Open-source Collaborative Platform},
	Url = {http://doi.acm.org/10.1145/2802130.2802135},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2802130.2802135},
	Bdsk-Url-2 = {https://doi.org/10.1145/2802130.2802135}}

@inproceedings{Agarwal:2014:VCC:2609908.2609946,
	Abstract = {Today's cellular networks are built with``macro cell'' basestations connected to the Internet via a rigid, complicated backhaul. Even with state-of-art technologies like LTE, users get limited throughput and high latency, with high variance. Performance enhancing IP boxes are deployed in the cellular operator's datacenters, far from the user. As a result, the most compelling cloudlet applications are difficult to realize on such networks and cloudlet researchers have thus far focused on Wi-Fi networks only.
We argue that the cloudlet community should consider small cell networks in addition to Wi-Fi networks. Small cells, such as femtocells and picocells, are relatively new additions to the cellular standards. By reducing the cell size compared to the traditional macro cells, they increase spatial reuse of precious licensed frequencies. Users get higher bandwidth and lower latency, with relatively less variance. This architecture, where small cells are deployed simply with power and Ethernet connectivity, lends itself well to cloudlet augmentation. In this position paper, we describe why deployed macro cell basestations are unsuitable for cloudlet deployment. In contrast, we describe why a small cell architecture is amenable for cloudlet deployments. Our experience from operating a small cell testbed in licensed frequencies matches that reported by equipment vendors. The applications we care about require high throughput and low latency. In a cellular network this can be achieved today by augmenting small cells with powerful cloudlets.},
	Acmid = {2609946},
	Address = {New York, NY, USA},
	Author = {Agarwal, Sharad and Philipose, Matthai and Bahl, Paramvir},
	Booktitle = {Proceedings of the Fifth International Workshop on Mobile Cloud Computing \&\#38; Services},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:31:47 +0100},
	Doi = {10.1145/2609908.2609946},
	Isbn = {978-1-4503-2824-1},
	Keywords = {cloudlets, lte, small cells},
	Location = {Bretton Woods, New Hampshire, USA},
	Numpages = {5},
	Pages = {1--5},
	Publisher = {ACM},
	Series = {MCS '14},
	Title = {Vision: The Case for Cellular Small Cells for Cloudlets},
	Url = {http://doi.acm.org/10.1145/2609908.2609946},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2609908.2609946},
	Bdsk-Url-2 = {https://doi.org/10.1145/2609908.2609946}}

@inproceedings{Fard:2013:VNS:2492517.2492587,
	Abstract = {The verification of Multiagent Systems (MAS) and Distributed Software Systems (DSS) has taken a special attention due to the growing demand of having DSS in recent years. The distributed functionality and lack of having a central control in MAS and DSS may cause to emerge new behaviors in the execution time. This unexpected behavior which was not seen in the requirements is known as emergent behavior and may cause irreparable damages. Detection of these emergent behaviors is more valuable and cost effective in the early phases compared to detecting them after the deployment. In this paper we propose a new technique for the detection of a specific type of emergent behavior in the design phase. We take the advantage of social network visualization in this method. The novelty and direct advantage of this technique is presenting the exact point and cause of emergent behavior.},
	Acmid = {2492587},
	Address = {New York, NY, USA},
	Author = {Fard, Fatemeh Hendijani and Far, Behrouz H.},
	Booktitle = {Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:32:05 +0100},
	Doi = {10.1145/2492517.2492587},
	Isbn = {978-1-4503-2240-9},
	Keywords = {distributed software systems, emergent behavior, multiagent systems, scenario-based software engineering, social network visualization},
	Location = {Niagara, Ontario, Canada},
	Numpages = {2},
	Pages = {1280--1281},
	Publisher = {ACM},
	Series = {ASONAM '13},
	Title = {Visualizing the Network of Software Agents for Verification of Multiagent Systems},
	Url = {http://doi.acm.org/10.1145/2492517.2492587},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2492517.2492587},
	Bdsk-Url-2 = {https://doi.org/10.1145/2492517.2492587}}

@inproceedings{Strazzarino:2005:VNI:1099435.1099519,
	Abstract = {The world-wide-web provides a great means for delivering information services to a university community[1,2]. An unexpected side-effect of using the web to deliver information services is that a community member can develop new services independent of the computing services department. As an example, one of the authors (a computer science student at California State University Chico) has independently developed a tool to help students create class schedules. This tool reads course information from the university's public web pages and uses it to automatically create custom class schedules for individual students. Over five thousand students (nearly half of all registering students) used this tool to create their Fall 2005 schedules.This paper presents the development and deployment of the scheduling tool as an example of information services volunteerism. It includes a description of the genesis of the tool, a discussion of how it has been integrated into the university supported information services, and an overview of the scheduling tool.},
	Acmid = {1099519},
	Address = {New York, NY, USA},
	Author = {Strazzarino, Robert and Henry, Tyson R.},
	Booktitle = {Proceedings of the 33rd Annual ACM SIGUCCS Conference on User Services},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:32:22 +0100},
	Doi = {10.1145/1099435.1099519},
	Isbn = {1-59593-200-3},
	Keywords = {class schedule, university services, volunteerism, world-wide-web},
	Location = {Monterey, CA, USA},
	Numpages = {4},
	Pages = {372--375},
	Publisher = {ACM},
	Series = {SIGUCCS '05},
	Title = {Volunteerism: A New Idea for Filling University Information Technology Needs},
	Url = {http://doi.acm.org/10.1145/1099435.1099519},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1099435.1099519},
	Bdsk-Url-2 = {https://doi.org/10.1145/1099435.1099519}}

@inproceedings{Allen:2008:VIR:1371607.1372748,
	Abstract = {Distributed acoustic sensing underlies an increasingly important class of sensor network applications, from habitat monitoring and bioacoustic census to security applications and virtual fences. VoxNet is a complete hardware and software platform for distributed acoustic monitoring applications that focuses on three key goals: (1) rapid deployment in realistic environments; (2) a high level programming language that abstracts the user from platform and network details and compiles into a high performance distributed application; and (3) an interactive usage model based on run-time installable programs, with the ability to run the same high level program seamlessly over live or stored data. The VoxNet hardware is self-contained and weather-resistant, and supports a four-channel microphone array with automated time synchronization, localization, and network coordination. Using VoxNet, an investigator can visualize phenomena in real-time, develop and tune online analysis, and record raw data for off-line analysis and archival. This paper describes both the hardware and software elements of the platform, as well as the architecture required to support distributed programs running over a heterogeneous network. We characterize the performance of the platform, using both microbenchmarks that evaluate specific aspects of the platform and a real application running in the field.},
	Acmid = {1372748},
	Address = {Washington, DC, USA},
	Author = {Allen, Michael and Girod, Lewis and Newton, Ryan and Madden, Samuel and Blumstein, Daniel T. and Estrin, Deborah},
	Booktitle = {Proceedings of the 7th International Conference on Information Processing in Sensor Networks},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:32:44 +0100},
	Doi = {10.1109/IPSN.2008.45},
	Isbn = {978-0-7695-3157-1},
	Keywords = {acoustic source localization, bioacoustics, distributed signal processing, platforms, wireless sensor networks},
	Numpages = {12},
	Pages = {371--382},
	Publisher = {IEEE Computer Society},
	Series = {IPSN '08},
	Title = {VoxNet: An Interactive, Rapidly-Deployable Acoustic Monitoring Platform},
	Url = {https://doi.org/10.1109/IPSN.2008.45},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1109/IPSN.2008.45}}

@inproceedings{Lee:2012:VAS:2361999.2362017,
	Abstract = {Recently, software developers are faced with a fierce market competition with: diverse market needs, ever increasing number of features, and shortening product life cycle. To survive in this fierce competition, software developers are searching for methods and tools to develop various products with reduced time-to-market and improved quality.
In response to these needs, we present a new CASE called VULCAN. VULCAN is a software development workbench comprising various tools for supporting the entire phases of feature-oriented product line software development from feature modeling to asset and product development. Especially, it provides several tools for supporting architecture-model-based software development where: (1) product line architectures can be specified using various architecture patterns, (2) application-specific architectures can be derived from the product line architecture specifications, (3) application-specific control components can be generated from the application architecture specifications, and (4) different deployment architectures can be configured with various component communication mechanisms. Of various tools included in VULCAN, we focus on this tool set for supporting architecture-model-based software development in this paper and demonstration.},
	Acmid = {2362017},
	Address = {New York, NY, USA},
	Author = {Lee, Hyesun and Yang, Jin-seok and Kang, Kyo C.},
	Booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:33:05 +0100},
	Doi = {10.1145/2361999.2362017},
	Isbn = {978-1-4503-1568-5},
	Keywords = {architecture-model-based, component connection mechanism, deployment architecture, feature-oriented, software product line},
	Location = {Helsinki, Finland},
	Numpages = {4},
	Pages = {86--89},
	Publisher = {ACM},
	Series = {WICSA/ECSA '12},
	Title = {VULCAN: Architecture-model-based Software Development Workbench},
	Url = {http://doi.acm.org/10.1145/2361999.2362017},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2361999.2362017},
	Bdsk-Url-2 = {https://doi.org/10.1145/2361999.2362017}}

@inproceedings{Yu:2015:WDP:2820518.2820564,
	Abstract = {The pull-based development model, enabled by git and popularised by collaborative coding platforms like BitBucket, Gitorius, and GitHub, is widely used in distributed software teams. While this model lowers the barrier to entry for potential contributors (since anyone can submit pull requests to any repository), it also increases the burden on integrators (i.e., members of a project's core team, responsible for evaluating the proposed changes and integrating them into the main development line), who struggle to keep up with the volume of incoming pull requests. In this paper we report on a quantitative study that tries to resolve which factors affect pull request evaluation latency in GitHub. Using regression modeling on data extracted from a sample of GitHub projects using the Travis-CI continuous integration service, we find that latency is a complex issue, requiring many independent variables to explain adequately.},
	Acmid = {2820564},
	Address = {Piscataway, NJ, USA},
	Author = {Yu, Yue and Wang, Huaimin and Filkov, Vladimir and Devanbu, Premkumar and Vasilescu, Bogdan},
	Booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 00:14:28 +0100},
	Isbn = {978-0-7695-5594-2},
	Location = {Florence, Italy},
	Numpages = {5},
	Pages = {367--371},
	Publisher = {IEEE Press},
	Series = {MSR '15},
	Title = {Wait for It: Determinants of Pull Request Evaluation Latency on GitHub},
	Url = {http://dl.acm.org/citation.cfm?id=2820518.2820564},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2820518.2820564}}

@inproceedings{Thakur:2014:WDE:2674061.2674069,
	Abstract = {Increasing energy consumption of commercial buildings has motivated numerous energy tracking and monitoring systems in the recent years. A particular area that is less explored in this domain is that of energy apportionment whereby total energy usage of a shared space such as a building is disaggregated to attribute it to an individual occupant. This particular scenario of individual apportionment is important for increased transparency in the actual energy consumption of shared living spaces in commercial buildings e.g. hotels, student dormitories and hospitals amongst others. Accurate energy accounting is a difficult problem to solve using only a single smart meter. In this paper, we present a novel, scalable and a low cost energy apportionment system called WattShare that builds upon our EnergyLens architecture, where data from a common electricity meter and smartphones (carried by the occupants) is fused, and then used for detailed energy disaggregation. This information is then used to measure the room-level energy consumption. We evaluate WattShare using a week long deployment conducted in a student dormitory in a campus in India. We show that WattShare is able to disaggregate the total energy usage from a single smart meter to individual rooms with an average precision of 96.42% and average recall of 94.96%. WattShare achieves 86.42% energy apportionment accuracy which increases to 94.57% when an outlier room is removed.},
	Acmid = {2674069},
	Address = {New York, NY, USA},
	Author = {Thakur, Shailja and Saha, Manaswi and Singh, Amarjeet and Agarwal, Yuvraj},
	Booktitle = {Proceedings of the 1st ACM Conference on Embedded Systems for Energy-Efficient Buildings},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:33:28 +0100},
	Doi = {10.1145/2674061.2674069},
	Isbn = {978-1-4503-3144-9},
	Keywords = {energy disaggregation, personal energy apportionment, smart meters, smartphones},
	Location = {Memphis, Tennessee},
	Numpages = {10},
	Pages = {30--39},
	Publisher = {ACM},
	Series = {BuildSys '14},
	Title = {WattShare: Detailed Energy Apportionment in Shared Living Spaces Within Commercial Buildings},
	Url = {http://doi.acm.org/10.1145/2674061.2674069},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2674061.2674069},
	Bdsk-Url-2 = {https://doi.org/10.1145/2674061.2674069}}

@inproceedings{Frey:2015:WPI:2855321.2855350,
	Abstract = {Architectural patterns are a helpful means for designing IT architectures, as they facilitate re-using proven knowledge (good practices) from previous exercises. Furthermore referencing a pattern in an architecture model helps improving the understandability of the model, as it directs to a comprehensive description of the pattern, but does not require to include the full description into the model. In this paper we describe how patterns can be woven into architecture models, focusing on deployment views of the IT infrastructure. Two different modeling approaches, Fundamental Modeling Concepts (FMC) and ArchiMate, are compared based on a real-world case concerning the infrastructure architecture of a large data center. This paper provides practical insights for IT architects from the industry by discussing the practical case and comparing both modeling approaches. Furthermore, it is supposed to intensify the exchange between industry experts and scientific researchers and it should motivate pursuing further research concerning patterns and IT infrastructure models.},
	Acmid = {2855350},
	Address = {New York, NY, USA},
	Articleno = {28},
	Author = {Frey, Frank J. and Bijvank, Roland and P\"{o}ttker, Michael},
	Booktitle = {Proceedings of the 20th European Conference on Pattern Languages of Programs},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:33:48 +0100},
	Doi = {10.1145/2855321.2855350},
	Isbn = {978-1-4503-3847-9},
	Keywords = {it infrastructure, software architecture},
	Location = {Kaufbeuren, Germany},
	Numpages = {25},
	Pages = {28:1--28:25},
	Publisher = {ACM},
	Series = {EuroPLoP '15},
	Title = {Weaving in Patterns into It Infrastructure Models: Industry Case and Exemplary Approaches},
	Url = {http://doi.acm.org/10.1145/2855321.2855350},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2855321.2855350},
	Bdsk-Url-2 = {https://doi.org/10.1145/2855321.2855350}}

@inproceedings{Murad:2009:WBP:1838002.1838010,
	Abstract = {In this paper, we have proposed and developed a Poultry Farm Monitoring System based on Wireless Sensor Network (WSN) using Crossbow's TelosB motes integrated with commercial sensors capable of measuring temperature and humidity values. The data collected from the sensors is uploaded to an online database using an agent program and afterward accessed via the internet using web analysis applications. The feasibility of the developed system was tested by deploying the proposed system at N-W.F.P. Agricultural University's research poultry farm in Peshawar in the North-Western Frontier Province of Pakistan. To emulate the proposed idea, the data collected during a daylong experiment was put to test, evaluating the WSN's reliability and its ability to detect and report anomalies in the environment. This paper is the first step towards WSN based poultry farm monitoring systems. We have provided an online monitoring solution for poultry farms and tested its feasibility and reliability by presenting a thorough data analysis of our pilot deployment.},
	Acmid = {1838010},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Murad, Mohsin and Yahya, Khawaja Mohammad and Hassan, Ghulam Mubashar},
	Booktitle = {Proceedings of the 7th International Conference on Frontiers of Information Technology},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:24:42 +0100},
	Doi = {10.1145/1838002.1838010},
	Isbn = {978-1-60558-642-7},
	Keywords = {WSN, poultry farm monitoring system},
	Location = {Abbottabad, Pakistan},
	Numpages = {5},
	Pages = {7:1--7:5},
	Publisher = {ACM},
	Series = {FIT '09},
	Title = {Web Based Poultry Farm Monitoring System Using Wireless Sensor Network},
	Url = {http://doi.acm.org/10.1145/1838002.1838010},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1838002.1838010},
	Bdsk-Url-2 = {https://doi.org/10.1145/1838002.1838010}}

@inproceedings{Gutierrez:2005:WSE:1103022.1103025,
	Abstract = {Web Services (WS hereafter) Security is a crucial aspect for technologies based on this paradigm to be completely adopted by the industry. As a consequence, a lot of initiativesof initiatives have arisen during the last years setting as their main purpose the standardization of the security factors related to this paradigm. In fact, over the past years, the most important consortiums ofof Internet Internet, like IETF, W3C or OASIS, are producing a huge number of WS-based security standards. Despite of this growing, there's not exist yet a process that guides developers in the critical task of integrating security within all the stages of the development's life cycle of WS-based software. Such a process should facilitate developers in the activities of web service-specific security requirents specification, web services-based security architecture design and web services security standards selection, integration and deployment. In this article we briefly present the PWSSec (Process for Web Services Security) process that is composed of three stages, WSSecReq (Web Services Security Requirents), WSSecArch (Web Services Security Architecture) and WSSecTech (Web Services Security Technologies) that accomplishes the mentioned activities, respectively. In this article wWe also provide an thorough explanation of the WSSecArch (Web Services Security Stage) stage intended to design the web services-based security architecture. In addition, a real case study where this stage in being applied is also included.},
	Acmid = {1103025},
	Address = {New York, NY, USA},
	Author = {Guti{\'e}rrez, Carlos and Fern\'{a}ndez-Medina, Eduardo and Piattini, Mario},
	Booktitle = {Proceedings of the 2005 Workshop on Secure Web Services},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:19:57 +0100},
	Doi = {10.1145/1103022.1103025},
	Isbn = {1-59593-234-8},
	Keywords = {security, software architecture, software development process, web services},
	Location = {Fairfax, VA, USA},
	Numpages = {10},
	Pages = {10--19},
	Publisher = {ACM},
	Series = {SWS '05},
	Title = {Web Services Enterprise Security Architecture: A Case Study},
	Url = {http://doi.acm.org/10.1145/1103022.1103025},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1103022.1103025},
	Bdsk-Url-2 = {https://doi.org/10.1145/1103022.1103025}}

@inproceedings{Leong:2009:WSI:1639809.1639851,
	Abstract = {In a net-centric environment, data, tools and people operate in a distributed network. We explore a generic, flexible, scalable, usable and intelligent web services architecture framework that enables sharing and integration of web services on the fly. As an example application, the envisioned Web Service Architecture Intelligent Framework (WSAIF) is applied to the Modeling, Virtual Environments and Simulation (MOVES) domain. This paper elaborates on the design and implementation of web services for the Scenario Authoring and Visualization for Advanced Graphical Environments (SAVAGE) web-based graphics models and discusses the deployment and test results of those services. The study and comparison of various modeling techniques that enable integration, orchestration and adaptation of composable web services is mentioned. The modeling techniques are essential to and will eventually be used in WSAIF Orchestrator and Adaptor components. The paper further explains how WSAIF software agents and modeling data enable web services integration on the fly. The paper concludes with recommendations for future work.},
	Acmid = {1639851},
	Address = {San Diego, CA, USA},
	Articleno = {40},
	Author = {Leong, Hoe Wai and Brutzman, Don and McGregor, Don and Blais, Curtis},
	Booktitle = {Proceedings of the 2009 Spring Simulation Multiconference},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:24:23 +0100},
	Keywords = {semantic web services, service oriented architecture, software agents, web services, web services architecture},
	Location = {San Diego, California},
	Numpages = {8},
	Pages = {40:1--40:8},
	Publisher = {Society for Computer Simulation International},
	Series = {SpringSim '09},
	Title = {Web Services Integration on the Fly for Service-oriented Computing and Simulation},
	Url = {http://dl.acm.org/citation.cfm?id=1639809.1639851},
	Year = {2009},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1639809.1639851}}

@inproceedings{Leger:2008:WSS:1416729.1416733,
	Abstract = {Recent advances in networks, information and computation grids, and the Web have resulted in the proliferation of a multitude of physically distributed and autonomously developed component Web services. The W3C Web Services Architecture defines "Web service as a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface described in a machine processible format (specifically WSDL) and other systems can interact with it in a manner prescribed by its description using SOAP messages, typically conveyed using HTTP with an XML serialization in conjunction with other Web-related standards". So, web-services constitute a distributed computing infrastructure made up of many different systems trying to communicate over the Internet to virtually form a single logical system. Web-services are an effective means for linking loosely coupled systems together using a technology that does not bind to a particular component model, programming language or platform. Used according to the semantic web principles, semantic web services offer key integration capabilities in businesses as well as in scientific research over the Internet and corporate intranets, and so are applicable to a broad variety of applications including e-Enterprise, e-Business, e-Government, and e-Science. Correspondingly, the construction and deployment of composite services by combining and reusing independently developed component services is an important capability in the emerging Web-based computing infrastructure. In this presentation we will focus on a detailed analysis of existing discovery reasoning mechanisms and the current key solutions for composition of component services. We will conclude with current roadblocks and pilot applications.},
	Acmid = {1416733},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Leger, Alain},
	Booktitle = {Proceedings of the 8th International Conference on New Technologies in Distributed Systems},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 00:12:05 +0100},
	Doi = {10.1145/1416729.1416733},
	Isbn = {978-1-59593-937-1},
	Location = {Lyon, France},
	Numpages = {1},
	Pages = {3:1--3:1},
	Publisher = {ACM},
	Series = {NOTERE '08},
	Title = {Web Services: The Story So Far - an Academic and Industrial Account},
	Url = {http://doi.acm.org/10.1145/1416729.1416733},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1416729.1416733},
	Bdsk-Url-2 = {https://doi.org/10.1145/1416729.1416733}}

@inproceedings{Berlin:2009:WSN:1644038.1644087,
	Abstract = {This paper illustrates both challenges and benefits found in expanding a traditional game concept to a situated environment with a distributed set of wireless sensing modules. Our pervasive game equivalent of the Whac-A-Mole game, Whac-A-Bee, retains the find-and-seek aspects of the original game while extending the location, the number of players, and the time-span in which it can be played. We discuss the obstacles met during this work, and specifically address challenges in making the game robust and flexible enough for large and long-term deployments in unknown territory.},
	Acmid = {1644087},
	Address = {New York, NY, USA},
	Author = {Berlin, Eugen and Van Laerhoven, Kristof and Schiele, Bernt and Guerrero, Pablo and Herzog, Arthur and Jacobi, Daniel and Buchmann, Alejandro},
	Booktitle = {Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:23:59 +0100},
	Doi = {10.1145/1644038.1644087},
	Isbn = {978-1-60558-519-2},
	Keywords = {pervasive game, sensor network, wireless},
	Location = {Berkeley, California},
	Numpages = {2},
	Pages = {333--334},
	Publisher = {ACM},
	Series = {SenSys '09},
	Title = {Whac-A-Bee: A Sensor Network Game},
	Url = {http://doi.acm.org/10.1145/1644038.1644087},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1644038.1644087},
	Bdsk-Url-2 = {https://doi.org/10.1145/1644038.1644087}}

@inproceedings{Gunawi:2014:BLC:2670979.2670986,
	Abstract = {We conduct a comprehensive study of development and deployment issues of six popular and important cloud systems (Hadoop MapReduce, HDFS, HBase, Cassandra, ZooKeeper and Flume). From the bug repositories, we review in total 21,399 submitted issues within a three-year period (2011-2014). Among these issues, we perform a deep analysis of 3655 "vital" issues (i.e., real issues affecting deployments) with a set of detailed classifications. We name the product of our one-year study Cloud Bug Study database (CbsDB) [9], with which we derive numerous interesting insights unique to cloud systems. To the best of our knowledge, our work is the largest bug study for cloud systems to date.},
	Acmid = {2670986},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Gunawi, Haryadi S. and Hao, Mingzhe and Leesatapornwongsa, Tanakorn and Patana-anake, Tiratat and Do, Thanh and Adityatama, Jeffry and Eliazar, Kurnia J. and Laksono, Agung and Lukman, Jeffrey F. and Martin, Vincentius and Satria, Anang D.},
	Booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 00:11:03 +0100},
	Doi = {10.1145/2670979.2670986},
	Isbn = {978-1-4503-3252-1},
	Location = {Seattle, WA, USA},
	Numpages = {14},
	Pages = {7:1--7:14},
	Publisher = {ACM},
	Series = {SOCC '14},
	Title = {What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems},
	Url = {http://doi.acm.org/10.1145/2670979.2670986},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2670979.2670986},
	Bdsk-Url-2 = {https://doi.org/10.1145/2670979.2670986}}

@inproceedings{Goeschl:2010:AMO:1890692.1890702,
	Abstract = {This paper describes the testing approach for an object-oriented rich client application based on an agile software development process. The paper starts with an overview of project and the testing strategy being used. It then goes on to describe the test tools used in the project and the results achieved. The paper ends with a discussion of the discovered defects, their distribution and improvements for the testing process.},
	Acmid = {1890702},
	Address = {New York, NY, USA},
	Articleno = {10},
	Author = {Goeschl, Siegfried and Herp, Matthias and Wais, Clemens},
	Booktitle = {Proceedings of the 1st Workshop on Testing Object-Oriented Systems},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:20:12 +0100},
	Doi = {10.1145/1890692.1890702},
	Isbn = {978-1-4503-0538-9},
	Keywords = {agile testing, continuous integration, defect rates, integration testing, object-oriented testing, technical debts, unit testing},
	Location = {Maribor, Slovenia},
	Numpages = {5},
	Pages = {10:1--10:5},
	Publisher = {ACM},
	Series = {ETOOS '10},
	Title = {When Agile Meets OO Testing: A Case Study},
	Url = {http://doi.acm.org/10.1145/1890692.1890702},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1890692.1890702},
	Bdsk-Url-2 = {https://doi.org/10.1145/1890692.1890702}}

@inproceedings{Ashworth:2008:BTL:1449956.1450031,
	Abstract = {At SIGUCCS 2007, one of the themes promoted by James Hilton of the University of Virginia was that in the world of IT at colleges and universities, we are likely to see more change happening more quickly than ever before, and that we need to learn ways to make the change happen as quickly and as painlessly as possible. A variety of circumstances led UVa to make a switch in its enterprise calendaring system between December 4, 2007 and January 15, 2008. Learn about the reasons behind this change, the obstacles we encountered, the user experience of the change and the lessons we learned in our first rapid change.},
	Acmid = {1450031},
	Address = {New York, NY, USA},
	Author = {Ashworth, Jayne},
	Booktitle = {Proceedings of the 36th Annual ACM SIGUCCS Fall Conference: Moving Mountains, Blazing Trails},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:23:38 +0100},
	Doi = {10.1145/1449956.1450031},
	Isbn = {978-1-60558-074-6},
	Keywords = {rapid deployment},
	Location = {Portland, OR, USA},
	Numpages = {6},
	Pages = {247--252},
	Publisher = {ACM},
	Series = {SIGUCCS '08},
	Title = {When Blazing a Trail Leads over the Mountainside Cliff: Lessons Learned from Our First Rapid Deployment},
	Url = {http://doi.acm.org/10.1145/1449956.1450031},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1449956.1450031},
	Bdsk-Url-2 = {https://doi.org/10.1145/1449956.1450031}}

@inproceedings{Outtagarts:2015:MTR:3233397.3233477,
	Abstract = {Network Virtualization Functions (NFV) and Software-Defined Networking (SDN) are changing the landscape of the telecommunications industry, particularly infrastructure and network systems of Telco operators with the introduction of cloud computing, paradigms of virtualization and software approaches. In this paper, we describe a demonstrator which shows how IT technologies reduce the time of deployment of a wireless infrastructure. In less than 60s, a wireless LTE network is available for connecting Smartphone's. In this demo, the eNodeB, virtualized using docker containers, is orchestrated by OpenStack heat.},
	Acmid = {3233477},
	Address = {Piscataway, NJ, USA},
	Author = {Outtagarts, Abdelkader and Roullet, Laurent and Mongazon-Cazavet, Bruno and Aravinthan, Gopalasingham},
	Booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:19:25 +0100},
	Isbn = {978-0-7695-5697-0},
	Keywords = {LTE, docker, heat, openairinterface, openstack, orchestration, virtualization},
	Location = {Limassol, Cyprus},
	Numpages = {2},
	Pages = {422--423},
	Publisher = {IEEE Press},
	Series = {UCC '15},
	Title = {When IT Meets Telco: RAN As a Service},
	Url = {http://dl.acm.org/citation.cfm?id=3233397.3233477},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3233397.3233477}}

@inproceedings{Nevill-Manning:2016:CMC:2973750.2991038,
	Abstract = {Cities have benefited from the three greatest technological innovations of the past 200 years: the steam engine, electrification, and the automobile. But each advance has created its own challenges, including pollution, overcrowding, sprawl. As the digital revolution transforms cities once again, how can we make sure it improves quality of life while minimizing the downside? With population density comes the possibility of deploying network connectivity and wayfinding at lower cost, but density also increases complexity of deployment. How can digital technology reduce the bad friction of urban environments, such as congestion, cost, and complexity, while increasing good friction, such as all of the serendipitous interactions that cities encourage? Underlying all this change is ubiquitous connectivity and mobile technology, from the phones people carry with them to the supporting devices and network endpoints embedded in the urban infrastructure. Sidewalk Labs is an Alphabet company that works with cities to develop new technology that can improve urban life. We will discuss some of our discoveries and beliefs, and talk about our plans to use mobile technology to help cities take full advantage of the digital revolution.},
	Acmid = {2991038},
	Address = {New York, NY, USA},
	Author = {Nevill-Manning, Craig},
	Booktitle = {Proceedings of the 22Nd Annual International Conference on Mobile Computing and Networking},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:11:15 +0100},
	Doi = {10.1145/2973750.2991038},
	Isbn = {978-1-4503-4226-1},
	Keywords = {sidewalk labs, ubiquitous and mobile computing, urban technologies},
	Location = {New York City, New York},
	Numpages = {1},
	Pages = {1--1},
	Publisher = {ACM},
	Series = {MobiCom '16},
	Title = {Where the Curb Meets the Cloud: Urban Innovation in the Digital Age},
	Url = {http://doi.acm.org/10.1145/2973750.2991038},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2973750.2991038},
	Bdsk-Url-2 = {https://doi.org/10.1145/2973750.2991038}}

@inproceedings{Cushing:2012:VWP:2442576.2442579,
	Abstract = {A need for better ecology visualization tools is well documented, and development of these is underway, including our own NSF funded Visualization of Terrestrial and Aquatic Systems (VISTAS) project, now beginning its second of four years. VISTAS' goal is not only to devise visualizations that help ecologists in research and in communicating that research, but also to evaluate the visualizations and software. Thus, we ask "which visualizations work, for what purpose, and for which audiences," and our project involves equal participation of ecologists, computer scientists, and social scientists. We have begun to study visualization use by ecologists, assessed some existing software products, and implemented a prototype. This position paper reports how we apply social science methods in establishing context for VISTAS' evaluation and development. We describe our initial surveys of ecologists and ecology journals to determine current visualization use, outline our visualization evaluation strategies, and in conclusion pose questions critical to the evaluation, deployment, and adoption of VISTAS and VISTAS-like visualizations and software.},
	Acmid = {2442579},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Cushing, Judith Bayard and Hayduk, Evan and Walley, Jerilyn and Winters, Kirsten and Lach, Denise and Bailey, Michael and Thomas, Christoph and Stafford, Susan G.},
	Booktitle = {Proceedings of the 2012 BELIV Workshop: Beyond Time and Errors - Novel Evaluation Methods for Visualization},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:19:06 +0100},
	Doi = {10.1145/2442576.2442579},
	Isbn = {978-1-4503-1791-7},
	Keywords = {ecology informatics, scientific visualization, software evaluation, visualization development lifecycle},
	Location = {Seattle, Washington, USA},
	Numpages = {5},
	Pages = {3:1--3:5},
	Publisher = {ACM},
	Series = {BELIV '12},
	Title = {Which Visualizations Work, for What Purpose, for Whom?: Evaluating Visualizations of Terrestrial and Aquatic Systems},
	Url = {http://doi.acm.org/10.1145/2442576.2442579},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2442576.2442579},
	Bdsk-Url-2 = {https://doi.org/10.1145/2442576.2442579}}

@inproceedings{Wang:2014:WPM:2663716.2663742,
	Abstract = {Public infrastructure-as-a-service (IaaS) clouds such as Amazon EC2 and Microsoft Azure host an increasing number of web services. The dynamic, pay-as-you-go nature of modern IaaS systems enable web services to scale up or down with demand, and only pay for the resources they need. We are unaware, however, of any studies reporting on measurements of the patterns of usage over time in IaaS clouds as seen in practice. We fill this gap, offering a measurement platform that we call WhoWas. Using active, but lightweight, probing, it enables associating web content to public IP addresses on a day-by-day basis. We exercise WhoWas to provide the first measurement study of churn rates in EC2 and Azure, the efficacy of IP blacklists for malicious activity in clouds, the rate of adoption of new web software by public cloud customers, and more.},
	Acmid = {2663742},
	Address = {New York, NY, USA},
	Author = {Wang, Liang and Nappa, Antonio and Caballero, Juan and Ristenpart, Thomas and Akella, Aditya},
	Booktitle = {Proceedings of the 2014 Conference on Internet Measurement Conference},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:18:46 +0100},
	Doi = {10.1145/2663716.2663742},
	Isbn = {978-1-4503-3213-2},
	Keywords = {active measurement, azure, cloud computing, ec2, web service},
	Location = {Vancouver, BC, Canada},
	Numpages = {14},
	Pages = {101--114},
	Publisher = {ACM},
	Series = {IMC '14},
	Title = {WhoWas: A Platform for Measuring Web Deployments on IaaS Clouds},
	Url = {http://doi.acm.org/10.1145/2663716.2663742},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2663716.2663742},
	Bdsk-Url-2 = {https://doi.org/10.1145/2663716.2663742}}

@inproceedings{Serakiotou:2008::1449956.1450009,
	Abstract = {Most Information Technology Departments in academia have their historical roots in a culture best described by words such as "geeks," "beta-testers," "troubleshooters," "debuggers," "early adopters," and so on. This culture was partly created by the fact that the mission of academia is indeed to keep looking forward to new developments and cutting-edge technologies. It is the role of industry to adopt the outcome of academia's frenetic efforts to move forward and to then produce commodity-like products.
At the same time, in most academic institutions, Information Technology Departments have also evolved into providers of services that use mature but still evolving technology. The consumers (customers) have come to treat these technologies as commodity products or utilities, and they expect to be able to easily learn how to use them, and to be able to easily figure out what are the services offered and how they can be configured.
In such an environment, "deployment cycles" and "personnel training" are often seen as a complete waste of time by the academics, as they themselves are pushing for the next generation of technology to be adopted by IT as fast as possible. For these reasons, the creation of detailed documentation by technical writers is usually not deemed worth funding, and with the advent of internet searches and FAQ blogging is seen often as completely unnecessary. This is a true statement for many areas of technology, where there exists ambient literature and FAQs (such as any MS Office product, for example). It is disastrous for customer satisfaction though, if it is applied on issues of configuration that are specific to the institution.
Nevertheless, this type of documentation, the kind that describes "how do we do things here" is also passed up as a secondary, non-glamorous task. The fact that usually "the way we do things" also changes very rapidly, adds another layer of discouragement to webmasters and FAQ owners to try to keep up and maintain relevant content.
The final issue that IT departments also have to grapple with is the thin staffing and the fact that even if technical writers were at hand, the staff that "owns" the information to be documented does not have time to explain it or relay it to others.
This was the kind of impossible situation that Rice University Information Technology had reached: stale documentation, orphaned ownership of the content, a customer base that was very conducive to using documentation and very resentful of the fact that we were not providing any. We experimented with "wiki" technology and we experimented with involving different groups within IT as potential technical writers. We were able to overcome the impasse and discovered some added benefits that have affected the training that we offer to our customer base.},
	Acmid = {1450009},
	Address = {New York, NY, USA},
	Author = {Serakiotou, Niki and Diaw, Aime and Bui, Thomas and Roberts, Richard and White, Carolynne},
	Booktitle = {Proceedings of the 36th Annual ACM SIGUCCS Fall Conference: Moving Mountains, Blazing Trails},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:18:23 +0100},
	Doi = {10.1145/1449956.1450009},
	Isbn = {978-1-60558-074-6},
	Keywords = {Wiki technology, knowledge management},
	Location = {Portland, OR, USA},
	Numpages = {8},
	Pages = {173--180},
	Publisher = {ACM},
	Series = {SIGUCCS '08},
	Title = {(Wiki + ResTechs) = (Fresh Documentation + Organic Knowledge Management + Training Materials + Good, Cheap Technical Writers)},
	Url = {http://doi.acm.org/10.1145/1449956.1450009},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1449956.1450009},
	Bdsk-Url-2 = {https://doi.org/10.1145/1449956.1450009}}

@inproceedings{Eigebali:2009:WOE:1551950.1551959,
	Abstract = {WiMAX is a novel wireless broadband technology with lots of promises for Urban and rural areas. It has the potential of bringing true wireless broadband to the masses at a fraction of the cost of existing solutions. The technology comes with many benefits and a few challenges. This tutorial discusses the technology benefits and applications. It also addresses some of the hurdles towards deployment and what the industry is doing to overcome those hurdles. A special focus will be given to the Middle East and an overview of the opportunities that exist will be presented.},
	Acmid = {1551959},
	Address = {New York, NY, USA},
	Author = {Eigebali, Hani},
	Booktitle = {Proceedings of the 2009 Conference on Information Science, Technology and Applications},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-24 20:45:17 +0100},
	Doi = {10.1145/1551950.1551959},
	Isbn = {978-1-60558-478-2},
	Location = {Kuwait, Kuwait},
	Pages = {1},
	Publisher = {ACM},
	Series = {ISTA '09},
	Title = {WiMAX Opportunities for Emerging Markets},
	Url = {http://doi.acm.org/10.1145/1551950.1551959},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1551950.1551959},
	Bdsk-Url-2 = {https://doi.org/10.1145/1551950.1551959}}

@inproceedings{Dean:2011:WTT:2070364.2070408,
	Abstract = {Giving up on something you know in order to risk the unknown usually proves to be a challenge. It is customary to have created a comfortable atmosphere of using what's now considered the old, but there is a demand for the new. So now is the time to learn, explore and ultimately understand the new. This new version of Windows is radically different from the old version. It requires much time, patience and planning, which in the Information Technology world is something we do not have. This requires us to collectively pool our experiences together and collaborate to achieve our goals.
Windows XP has been around for a long time and most have become comfortable in developing and deploying it. Windows 7 offers a substantial upgrade from Windows XP in regards to features for users, but not necessarily for image developers. Image developers are discovering that creating a Windows 7 image is not an easy process compared to using Windows XP. It requires undergoing extensive test trials, researching new approaches to conduct old tasks, and giving up on features that can no longer be achieved using this new operating system.
At Lewis & Clark College, we have migrated all of our PC computers and dual-boot labs to Windows 7 64-bit from Windows XP. We overcame many obstacles while striving to meet our goals. This paper will highlight the in-depth process it took to develop and deploy a Windows 7 image for our environment, including the researched documentation and resources that assisted us through our endeavor.},
	Acmid = {2070408},
	Address = {New York, NY, USA},
	Author = {Dean, David Christopher},
	Booktitle = {Proceedings of the 39th Annual ACM SIGUCCS Conference on User Services},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:17:57 +0100},
	Doi = {10.1145/2070364.2070408},
	Isbn = {978-1-4503-1023-9},
	Keywords = {deployment, sysprep, windows 7},
	Location = {San Diego, California, USA},
	Numpages = {6},
	Pages = {165--170},
	Publisher = {ACM},
	Series = {SIGUCCS '11},
	Title = {Windows 7: Trials and Tribulations},
	Url = {http://doi.acm.org/10.1145/2070364.2070408},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2070364.2070408},
	Bdsk-Url-2 = {https://doi.org/10.1145/2070364.2070408}}

@inproceedings{Sikka:2006:WAS:1127777.1127852,
	Abstract = {Agriculture accounts for a significant portion of the GDP in most developed countries. However, managing farms, particularly large-scale extensive farming systems, is hindered by lack of data and increasing shortage of labour. We have deployed a large heterogeneous sensor network on a working farm to explore sensor network applications that can address some of the issues identified above. Our network is solar powered and has been running for over 6 months. The current deployment consists of over 40 moisture sensors that provide soil moisture profiles at varying depths, weight sensors to compute the amount of food and water consumed by animals, electronic tag readers, up to 40 sensors that can be used to track animal movement (consisting of GPS, compass and accelerometers), and 20 sensor/actuators that can be used to apply different stimuli (audio, vibration and mild electric shock) to the animal. The static part of the network is designed for 24/7 operation and is linked to the Internet via a dedicated high-gain radio link, also solar powered. The initial goals of the deployment are to provide a testbed for sensor network research in programmability and data handling while also being a vital tool for scientists to study animal behavior. Our longer term aim is to create a management system that completely transforms the way farms are managed.},
	Acmid = {1127852},
	Address = {New York, NY, USA},
	Author = {Sikka, Pavan and Corke, Peter and Valencia, Philip and Crossman, Christopher and Swain, Dave and Bishop-Hurley, Greg},
	Booktitle = {Proceedings of the 5th International Conference on Information Processing in Sensor Networks},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:11:47 +0100},
	Doi = {10.1145/1127777.1127852},
	Isbn = {1-59593-334-4},
	Keywords = {farm management, sensor network applications},
	Location = {Nashville, Tennessee, USA},
	Numpages = {8},
	Pages = {492--499},
	Publisher = {ACM},
	Series = {IPSN '06},
	Title = {Wireless Adhoc Sensor and Actuator Networks on the Farm},
	Url = {http://doi.acm.org/10.1145/1127777.1127852},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1127777.1127852},
	Bdsk-Url-2 = {https://doi.org/10.1145/1127777.1127852}}

@inproceedings{Hartpence:2005:WCI:1095714.1095734,
	Abstract = {As part of a complete wireless networking education, students must have an in-depth understanding of basic concepts such as signal propagation, environmental effects on RF signals, FCC regulations and limits, power levels, antenna construction and antenna operation. Lecture based curricula can only go so far in preparing a wireless professional to succeed in industry. To be complete, the student must have practical experience.Our wireless coursework is comprised of three courses, the first of which is a wireless concepts course. This course has a significant hands-on component that requires students to understand the tools while applying what they are learning about the physical layer and basic network operation. The students engage in two very large projects; a wireless building survey and signal propagation testing using specialized equipment.As part of the projects, students create a series of experiments with a variety of equipment and provide useable test data. Examples of the tests include interference, Fresnel zone effects, throughput, range and the effects of multi-path on signals. However, in the presence of an increasing number of wireless networks, obtaining real world, reliable data illustrating the various physical layer phenomena is difficult. Our solution was to build several wireless carts outfitted with various antennas, transmission equipment from different portions of the spectrum and that used different encoding or modulation techniques. In addition, a major requirement was that the carts be able to operate away from infrastructure support, including AC power.The carts have enabled students to isolate themselves from other wireless signals and have provided an extremely adaptive platform for experiments and projects. This paper will describe the coursework, projects, functions, costs, lessons learned and the data gathered as a result of their deployment.},
	Acmid = {1095734},
	Address = {New York, NY, USA},
	Author = {Hartpence, Bruce and Hill, Lawrence},
	Booktitle = {Proceedings of the 6th Conference on Information Technology Education},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:17:37 +0100},
	Doi = {10.1145/1095714.1095734},
	Isbn = {1-59593-252-6},
	Keywords = {carts, platform, teaching, wireless},
	Location = {Newark, NJ, USA},
	Numpages = {4},
	Pages = {79--82},
	Publisher = {ACM},
	Series = {SIGITE '05},
	Title = {Wireless Carts: An Inexpensive Education and Research Platform},
	Url = {http://doi.acm.org/10.1145/1095714.1095734},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1095714.1095734},
	Bdsk-Url-2 = {https://doi.org/10.1145/1095714.1095734}}

@inproceedings{Zhuang:2007:WSN:1366804.1366850,
	Abstract = {In recent years, wireless sensor networks have been used in applications of data gathering and target localization across large geographical areas. In this paper, we study the issues involved in applying wireless sensor networks to search and rescue of lost hikers in trails and focus on the optimal placement of sensors and access points such that the cost of search and rescue is minimized. Particularly, we address two problems: a) how to identify the lost hiker position as accurately as possible, i.e., obtain a small trail segments containing the lost hiker; and (b) how to search efficiently in trail segments for different trail topologies and search agent capabilities. For the optimal access point deployment problem, we propose theoretical models that consider both efficiency and accuracy criteria and present analytical results for simpler trail topologies. For complicated graph topologies, we develop efficient heuristic algorithms with various heuristics. After access point deployment is decided, the actual cost of search in individual trail segment can be computed. We analyze four different types of search and rescue agents, present algorithms to find the optimal search paths for each one of them, and compute their search costs. The algorithms are developed based on solving Chinese Postman problems. Finally, we present extensive experimental results to examine the accuracy of the mathematical models and compare the performances of different methods. A heuristic method, divide-merge, is shown to outperform all others and finds near-optimal solutions.},
	Acmid = {1366850},
	Address = {ICST, Brussels, Belgium, Belgium},
	Articleno = {36},
	Author = {Zhuang, Peng and Wang, Qingguo and Shang, Yi and Shi, Hongchi and Hua, Bei},
	Booktitle = {Proceedings of the 2Nd International Conference on Scalable Information Systems},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:17:11 +0100},
	Isbn = {978-1-59593-757-5},
	Keywords = {Chinese postman problem, graph partitioning, mobile agent, search and rescue, wireless sensor network},
	Location = {Suzhou, China},
	Numpages = {8},
	Pages = {36:1--36:8},
	Publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
	Series = {InfoScale '07},
	Title = {Wireless Sensor Network Aided Search and Rescue in Trails},
	Url = {http://dl.acm.org/citation.cfm?id=1366804.1366850},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1366804.1366850}}

@inproceedings{Will:2012:WSN:2248356.2248360,
	Abstract = {In the project FeuerWhere we researched the use of Wireless Sensor Networks (WSNs) in rescue scenarios by combining the monitoring of the environment and vital signs, as well as estimating the location of the nodes in a WSN.
The goal of this project was to monitor vital signs, envi- ronmental conditions and positions of firefighters in a large indoor emergency scenario using a meshed WSN. The WSN consists of one node for each firefighter which transports the data to the mesh network and which is also connected to a Body Area Network (BAN) [8]. The BAN itself consists of several nodes placed into protective suits. The main requirement to all parts of the system is robustness against all kinds of extreme environmental conditions, like extreme air temperatures up to 800$\,^{\circ}$C, extreme humidity up to 100% including condensation and wet walls in unknown buildings.
We report on an experimental evaluation of the deployment of a prototype that addresses the concern of monitoring firefighters in extreme environmental conditions. This will establish the general feasibility of WSN in extreme conditions and show that precise indoor localization using radio runtime measurements is not affected by these conditions.},
	Acmid = {2248360},
	Address = {New York, NY, USA},
	Author = {Will, Heiko and Hillebrandt, Thomas and Kyas, Marcel},
	Booktitle = {Proceedings of the 1st ACM International Workshop on Sensor-Enhanced Safety and Security in Public Spaces},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:10:34 +0100},
	Doi = {10.1145/2248356.2248360},
	Isbn = {978-1-4503-1288-2},
	Keywords = {extreme environmental conditions, real world deployment, wireless sensor networks},
	Location = {Hilton Head, South Carolina, USA},
	Numpages = {6},
	Pages = {9--14},
	Publisher = {ACM},
	Series = {SESP '12},
	Title = {Wireless Sensor Networks in Emergency Scenarios: The FeuerWhere Deployment},
	Url = {http://doi.acm.org/10.1145/2248356.2248360},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2248356.2248360},
	Bdsk-Url-2 = {https://doi.org/10.1145/2248356.2248360}}

@inproceedings{Frye:2007:WSN:1324302.1324360,
	Abstract = {There have been many response deficiencies cited regarding a fire department's ability to fight a structure fire. The attacks on the World Trade Center on 9/11 highlighted many of these problems and demanded the attention of the world. Wireless Sensor Networks (WSN) can benefit fire fighters as they bravely enter a structure fire. WSNs are networks consisting of many small sensors or nodes. The sensors can monitor a variety of data, such as the environment, movement and patient health readings. There has been much research completed in the area of WSNs but most of this research is proven via simulations with little actual experimentation or deployment of devices. Wireless sensor networks is an exciting and a new area of research; it has captured the interest of many researchers. The intrigue easily attracts the attention of students as well. With the help of various students, a wireless sensor network will be deployed in a fire training center to test a deployment in a structure. The primary measurement will be the performance of the sensors and the sensor network. Several students have been utilized to assist in writing the necessary programs, and more will continue to contribute to the project. This project will maintain research in utilizing WSNs in a fire fighting scenario and will continue to employ students to get them excited about learning and research.},
	Acmid = {1324360},
	Address = {New York, NY, USA},
	Author = {Frye, Lisa M.},
	Booktitle = {Proceedings of the 8th ACM SIGITE Conference on Information Technology Education},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:15:40 +0100},
	Doi = {10.1145/1324302.1324360},
	Isbn = {978-1-59593-920-3},
	Keywords = {implementation, nesC, network course, performance, teaching, wireless sensor networks},
	Location = {Destin, Florida, USA},
	Numpages = {2},
	Pages = {269--270},
	Publisher = {ACM},
	Series = {SIGITE '07},
	Title = {Wireless Sensor Networks: Learning and Teaching},
	Url = {http://doi.acm.org/10.1145/1324302.1324360},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1324302.1324360},
	Bdsk-Url-2 = {https://doi.org/10.1145/1324302.1324360}}

@inproceedings{Segura:2015:WBM:2820656.2820661,
	Abstract = {Weather conditions affect many cities and companies. The WISE (Weather InSights Environment) system serves as a central place to gather and present weather related information for decision makers. It was initially developed to fit a single tenant. Due to a multi-tenant opportunity, WISE is evolving to be deployed on a Cloud environment to support on-demand computing resources and multiple clients. Software product line techniques were applied to model common and variable features of tenants. WISE-SPL enables the derivation of products for each client and also the deployment on Cloud infrastructure. The contribution of this work is a demonstration and discussion of benefits and limitations in applying SPL techniques, following a extractive approach, to build a multi-tenant Cloud application.},
	Acmid = {2820661},
	Address = {Piscataway, NJ, USA},
	Author = {Segura, Vin\'{\i}cius C. V. B. and Tizzei, Leonardo P. and de F. Ramirez, Jo\~{a}o Paulo and dos Santos, Marcelo N. and Azevedo, Leonardo G. and de G. Cerqueira, Renato F.},
	Booktitle = {Proceedings of the Fifth International Workshop on Product LinE Approaches in Software Engineering},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 00:06:07 +0100},
	Location = {Florence, Italy},
	Numpages = {4},
	Pages = {7--10},
	Publisher = {IEEE Press},
	Series = {PLEASE '15},
	Title = {WISE-SPL: Bringing Multi-tenancy to the Weather InSights Environment System},
	Url = {http://dl.acm.org/citation.cfm?id=2820656.2820661},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2820656.2820661}}

@inproceedings{Jaradat:2014:WPD:2759989.2760040,
	Abstract = {Orchestrating service-oriented workflows is typically based on a design model that routes both data and control through a single point -- the centralised workflow engine. This causes scalability problems that include the unnecessary consumption of the network bandwidth, high latency in transmitting data between the services, and performance bottlenecks. These problems are highly prominent when orchestrating workflows that are composed from services dispersed across distant geographical locations. This paper presents a novel workflow partitioning approach, which attempts to improve the scalability of orchestrating large-scale workflows. It permits the workflow computation to be moved towards the services providing the data in order to garner optimal performance results. This is achieved by decomposing the workflow into smaller sub workflows for parallel execution, and determining the most appropriate network locations to which these sub workflows are transmitted and subsequently executed. This paper demonstrates the efficiency of our approach using a set of experimental workflows that are orchestrated over Amazon EC2 and across several geographic network regions.},
	Acmid = {2760040},
	Address = {Washington, DC, USA},
	Author = {Jaradat, Ward and Dearle, Alan and Barker, Adam},
	Booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:15:19 +0100},
	Doi = {10.1109/UCC.2014.34},
	Isbn = {978-1-4799-7881-6},
	Keywords = {Service-oriented workflows, computation placement analysis, deployment, orchestration, partitioning},
	Numpages = {10},
	Pages = {251--260},
	Publisher = {IEEE Computer Society},
	Series = {UCC '14},
	Title = {Workflow Partitioning and Deployment on the Cloud Using Orchestra},
	Url = {https://doi.org/10.1109/UCC.2014.34},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/UCC.2014.34}}

@inproceedings{Martin:2007:WC:1324892.1324945,
	Abstract = {This paper presents data and analysis from a long term ethnographic study of the design and development of an electronic patient records system in a UK hospital Trust. The project is a public private partnership (PPP) between the Trust and a US based software house (OurComp) contracted to supply, configure and support their customizable-off-the-shelf (COTS) healthcare information system in cooperation with an in-hospital project team. Given this contractual relationship for system delivery and support (increasingly common, and 'standard' in UK healthcare) we focus on the ways in which issues to do with the 'contract' enter into and impinge on everyday design and deployment work as part of the process of delivering dependable systems.},
	Acmid = {1324945},
	Address = {New York, NY, USA},
	Author = {Martin, Dave and Procter, Rob and Mariani, John and Rouncefield, Mark},
	Booktitle = {Proceedings of the 19th Australasian Conference on Computer-Human Interaction: Entertaining User Interfaces},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:10:53 +0100},
	Doi = {10.1145/1324892.1324945},
	Isbn = {978-1-59593-872-5},
	Keywords = {contracts, customizable-off-the-shelf (COTS) healthcare information systems, ethnography, project management},
	Location = {Adelaide, Australia},
	Numpages = {8},
	Pages = {241--248},
	Publisher = {ACM},
	Series = {OZCHI '07},
	Title = {Working the Contract},
	Url = {http://doi.acm.org/10.1145/1324892.1324945},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1324892.1324945},
	Bdsk-Url-2 = {https://doi.org/10.1145/1324892.1324945}}

@inproceedings{Panneerselvam:2014:WAS:2759989.2760086,
	Abstract = {Alongside the healthy development of the Cloud-based technologies across various application deployments, their associated energy consumptions incurred by the excess usage of Information and Communication Technology (ICT) resources, is one of the serious concerns demanding effective solutions with immediate effect. Effective auto scaling of the Cloud resources in accordance to the incoming user demand and thereby reducing the idle resources is one optimum solution which not only reduces the excess energy consumptions but also helps maintaining the Quality of Service (QoS). Whilst achieving such tasks, estimating the user demand in advance with reliable level of accuracy has become an integral and vital component. With this in mind, this research work is aimed at analyzing the Cloud workloads and further evaluating the performances of two widely used prediction techniques such as Markov modelling and Bayesian modelling with 7 hours of Google cluster data. An important outcome of this research work is the categorization and characterization of the Cloud workloads which will assist leading into the user demand prediction parameter modelling.},
	Acmid = {2760086},
	Address = {Washington, DC, USA},
	Author = {Panneerselvam, John and Liu, Lu and Antonopoulos, Nick and Bo, Yuan},
	Booktitle = {Proceedings of the 2014 IEEE/ACM 7th International Conference on Utility and Cloud Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:14:58 +0100},
	Doi = {10.1109/UCC.2014.144},
	Isbn = {978-1-4799-7881-6},
	Keywords = {modelling, pattern, prediction, workloads},
	Numpages = {7},
	Pages = {883--889},
	Publisher = {IEEE Computer Society},
	Series = {UCC '14},
	Title = {Workload Analysis for the Scope of User Demand Prediction Model Evaluations in Cloud Environments},
	Url = {https://doi.org/10.1109/UCC.2014.144},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/UCC.2014.144}}

@inproceedings{Zhang:2010:WCO:1827418.1827466,
	Abstract = {Operator-based programming languages provide an effective development model for large scale stream processing applications. A stream processing application consists of many runtime deployable software processing elements (PE) that work in flows to process incoming messages. Operators (OP) are logical building blocks hosted by PEs. One or more OPs can be fused into a PE at compile-time. Performance optimization for our streaming system includes compile-time fusion optimization and runtime PE-to-host deployment. One of the goals of an optimized stream application is to use minimal computing resource to sustain maximal message throughput.
Characterizing the resource usage of PEs is critical for performance optimization. During compile-time optimization, OP-level resource usage is used to predict the resource usage of fused PEs. When starting an application, PE-level resource usage is used as an initial estimation by the scheduler. In this paper, we propose an efficient workload characterization approach for data stream processing systems. Our method includes the procedures for obtaining reusable OP-level resource usage information from profiling data and recomposing OP-level profiles to predict PE-level resource usage. We present several techniques to overcome measurement errors from the OP data collection. The impact of hardware speed and multi-threading contention on hyper-threading and multi-core machines are also studied. We show that our method can be applied to several streaming applications and the prediction of the PE CPU resource usage is within 15% of the actual CPU usage.},
	Acmid = {1827466},
	Address = {New York, NY, USA},
	Author = {Zhang, Xiaolan J. and Parekh, Sujay and Gedik, Bugra and Andrade, Henrique and Wu, Kun-Lung},
	Booktitle = {Proceedings of the Fourth ACM International Conference on Distributed Event-Based Systems},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:09:17 +0100},
	Doi = {10.1145/1827418.1827466},
	Isbn = {978-1-60558-927-5},
	Keywords = {profiling, stream processing system, workload characterization},
	Location = {Cambridge, United Kingdom},
	Numpages = {13},
	Pages = {235--247},
	Publisher = {ACM},
	Series = {DEBS '10},
	Title = {Workload Characterization for Operator-based Distributed Stream Processing Applications},
	Url = {http://doi.acm.org/10.1145/1827418.1827466},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1827418.1827466},
	Bdsk-Url-2 = {https://doi.org/10.1145/1827418.1827466}}

@inproceedings{Fournel:2007:WLS:1236360.1236435,
	Abstract = {We present Worldsens,a prototyping and development work for wireless sensor protocols and applications.Our relies on two simulators,WSim and WSNet, and proposes a full simulation and performance estimation embedded platforms with instruction and radio byte curacy.During this demo,we propose to demonstrate the interest of the two simulators in stand-alone and in cooperative use.},
	Acmid = {1236435},
	Address = {New York, NY, USA},
	Author = {Fournel, Nicolas and Fraboulet, Antoine and Chelius, Guillaume and Fleury, Eric and Allard, Bruno and Brevet, Olivier},
	Booktitle = {Proceedings of the 6th International Conference on Information Processing in Sensor Networks},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:14:36 +0100},
	Doi = {10.1145/1236360.1236435},
	Isbn = {978-1-59593-638-7},
	Keywords = {design, sensor networks, simulation},
	Location = {Cambridge, Massachusetts, USA},
	Numpages = {2},
	Pages = {551--552},
	Publisher = {ACM},
	Series = {IPSN '07},
	Title = {Worldsens: From Lab to Sensor Network Application Development and Deployment},
	Url = {http://doi.acm.org/10.1145/1236360.1236435},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1236360.1236435},
	Bdsk-Url-2 = {https://doi.org/10.1145/1236360.1236435}}

@inproceedings{Sheppard:2012:WMF:2442952.2442964,
	Abstract = {We present "wq", an open-source framework for developing robust applications allowing volunteers to collect geographic information (VGI) in the field. Successful VGI applications been deployed in various contexts, but much of the effort that is put into common programming tasks cannot be re-used, often because the application code is too closely tied to the problem domain. User-friendly campaign authoring tools are being explored as ways to facilitate the rapid deployment of VGI applications, but many of these tools necessarily enforce a restricted vocabulary of interface elements and database models, limiting their usefulness for more complex VGI project workflows. In contrast, we propose a highly modular, open-source approach that enables reuse of general-purpose components created to facilitate common design patterns -- without enforcing any hard limitations on the database model or interface. The framework builds off of and extends numerous existing open-source projects and leverages open standards (e.g. HTML5), which means it will work across all popular mobile devices as well as desktop browsers. The ideas behind wq arose from our ongoing efforts to generalize an existing data collection application initially created for community-based stream quality monitoring. In this paper we justify the design decisions made in creating wq and suggest general principles that should be taken into consideration when designing systems for collecting, storing, and utilizing VGI.},
	Acmid = {2442964},
	Address = {New York, NY, USA},
	Author = {Sheppard, S. Andrew},
	Booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Crowdsourced and Volunteered Geographic Information},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:14:16 +0100},
	Doi = {10.1145/2442952.2442964},
	Isbn = {978-1-4503-1694-1},
	Keywords = {HTML5, VGI, citizen science, crowdsourcing, open source},
	Location = {Redondo Beach, California},
	Numpages = {8},
	Pages = {62--69},
	Publisher = {ACM},
	Series = {GEOCROWD '12},
	Title = {Wq: A Modular Framework for Collecting, Storing, and Utilizing Experiential VGI},
	Url = {http://doi.acm.org/10.1145/2442952.2442964},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2442952.2442964},
	Bdsk-Url-2 = {https://doi.org/10.1145/2442952.2442964}}

@inproceedings{Rajanna:2010:XEC:1851476.1851506,
	Abstract = {Large cluster-based cloud computing platforms increasingly use commodity Ethernet technologies, such as Gigabit Ethernet, 10GigE, and Fibre Channel over Ethernet (FCoE), for intra-cluster communication. Traffic congestion can become a performance concern in the Ethernet due to consolidation of data, storage, and control traffic over a common layer-2 fabric, as well as consolidation of multiple virtual machines (VMs) over less physical hardware. Even as networking vendors race to develop switch-level hardware support for congestion management, we make the case that virtualization has opened up a complementary set of opportunities to reduce or even eliminate network congestion in cloud computing clusters. We present the design, implementation, and evaluation of a system called XCo, that performs explicit coordination of network transmissions over a shared Ethernet fabric to proactively prevent network congestion. XCo is a software-only distributed solution executing only in the end-nodes. A central controller uses explicit permissions to temporally separate (at millisecond granularity) the transmissions from competing senders through congested links. XCo is fully transparent to applications, presently deployable, and independent of any switch-level hardware support. We present a detailed evaluation of our XCo prototype across a number of network congestion scenarios, and demonstrate that XCo significantly improves network performance during periods of congestion.},
	Acmid = {1851506},
	Address = {New York, NY, USA},
	Author = {Rajanna, Vijay Shankar and Shah, Smit and Jahagirdar, Anand and Lemoine, Christopher and Gopalan, Kartik},
	Booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:09:39 +0100},
	Doi = {10.1145/1851476.1851506},
	Isbn = {978-1-60558-942-8},
	Keywords = {congestion, ethernet, virtualization},
	Location = {Chicago, Illinois},
	Numpages = {12},
	Pages = {252--263},
	Publisher = {ACM},
	Series = {HPDC '10},
	Title = {XCo: Explicit Coordination to Prevent Network Fabric Congestion in Cloud Computing Cluster Platforms},
	Url = {http://doi.acm.org/10.1145/1851476.1851506},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1851476.1851506},
	Bdsk-Url-2 = {https://doi.org/10.1145/1851476.1851506}}

@inproceedings{Neubauer:2015:XXS:2814251.2814267,
	Abstract = {A multitude of Domain-Specific Languages (DSLs) have been implemented with XML Schemas. While such DSLs are well adopted and flexible, they miss modern DSL editor functionality. Moreover, since XML is primarily designed as a machine-processible format, artifacts defined with XML-based DSLs lack comprehensibility and, therefore, maintainability. In order to tackle these shortcomings, we propose a bridge between the XML Schema Definition (XSD) language and text-based metamodeling languages. This bridge exploits existing seams between the technical spaces XMLware, modelware, and grammarware as well as closes identified gaps. The resulting approach is able to generate Xtext-based editors from XSDs providing powerful editor functionality, customization options for the textual concrete syntax style, and round-trip transformations enabling the exchange of data between the involved technical spaces. We evaluate our approach by a case study on TOSCA, which is an XML-based standard for defining Cloud deployments. The results show that our approach enables bridging XMLware with modelware and grammarware in several ways going beyond existing approaches and allows the automated generation of editors that are at least equivalent to editors manually built for XML-based languages.},
	Acmid = {2814267},
	Address = {New York, NY, USA},
	Author = {Neubauer, Patrick and Bergmayr, Alexander and Mayerhofer, Tanja and Troya, Javier and Wimmer, Manuel},
	Booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Software Language Engineering},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:13:53 +0100},
	Doi = {10.1145/2814251.2814267},
	Isbn = {978-1-4503-3686-4},
	Keywords = {DSL, Language Engineering, Language Modernization, Markup Language, XSD, Xtext},
	Location = {Pittsburgh, PA, USA},
	Numpages = {6},
	Pages = {71--76},
	Publisher = {ACM},
	Series = {SLE 2015},
	Title = {XMLText: From XML Schema to Xtext},
	Url = {http://doi.acm.org/10.1145/2814251.2814267},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2814251.2814267},
	Bdsk-Url-2 = {https://doi.org/10.1145/2814251.2814267}}

@inproceedings{Sylwestrzak:2012:YAY:2232817.2232920,
	Abstract = {YADDA2 is an open software platform which facilitates creation of digital library applications. It consists of versatile building blocks providing, among others: storage, relational and full-text indexing, process management, and asynchronous communication. Its loosely-coupled service-oriented architecture enables deployment of highly-scalable, distributed systems.},
	Acmid = {2232920},
	Address = {New York, NY, USA},
	Author = {Sylwestrzak, Wojtek and Rosiek, Tomasz and Bolikowski, Lukasz},
	Booktitle = {Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:13:34 +0100},
	Doi = {10.1145/2232817.2232920},
	Isbn = {978-1-4503-1154-0},
	Keywords = {scalability, service-oriented architecture, software platform},
	Location = {Washington, DC, USA},
	Numpages = {2},
	Pages = {419--420},
	Publisher = {ACM},
	Series = {JCDL '12},
	Title = {YADDA2: Assemble Your Own Digital Library Application from LEGO Bricks},
	Url = {http://doi.acm.org/10.1145/2232817.2232920},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2232817.2232920},
	Bdsk-Url-2 = {https://doi.org/10.1145/2232817.2232920}}

@inproceedings{Carpenter:2009:YWU:1629501.1629556,
	Abstract = {In 2009, the Campus Information Technologies and Educational Services (CITES) Help Desk at the University of Illinois at Urbana-Champaign began taking a more active role in supporting many of the services provided by CITES. The introduction of new services is inevitable. Providing excellent support requires collaboration from the early planning stages through implementation and beyond.
The Help Desk experienced varied results during the integration of new services. Join us as we explore the importance of communication and collaboration between service managers and the Help Desk. We will discuss our successes and failures, past and present.
Our approach contains requirements that must be satisfied before the Help Desk takes on support responsibilities. We are the face of the organization. We must be an advocate for our customers.
We have a vision of how this organic process should work. Gaining trust and cooperation will be the foundation of a world-class institution.},
	Acmid = {1629556},
	Address = {New York, NY, USA},
	Author = {Carpenter, Nathan and Tucker, Ryan},
	Booktitle = {Proceedings of the 37th Annual ACM SIGUCCS Fall Conference: Communication and Collaboration},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:10:15 +0100},
	Doi = {10.1145/1629501.1629556},
	Isbn = {978-1-60558-477-5},
	Keywords = {SLA, applications, customer service, help desk, knowledgebase, lifecycle, management, project management, services},
	Location = {St. Louis, Missouri, USA},
	Numpages = {4},
	Pages = {293--296},
	Publisher = {ACM},
	Series = {SIGUCCS '09},
	Title = {You Want Us to Support WHAT?!? Negotiation, Delivery and Cultivation: The Gateway to Excellent Service Deployment},
	Url = {http://doi.acm.org/10.1145/1629501.1629556},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1629501.1629556},
	Bdsk-Url-2 = {https://doi.org/10.1145/1629501.1629556}}

@inproceedings{Demartini:2012:ZLP:2187836.2187900,
	Abstract = {We tackle the problem of entity linking for large collections of online pages; Our system, ZenCrowd, identifies entities from natural language text using state of the art techniques and automatically connects them to the Linked Open Data cloud. We show how one can take advantage of human intelligence to improve the quality of the links by dynamically generating micro-tasks on an online crowdsourcing platform. We develop a probabilistic framework to make sensible decisions about candidate links and to identify unreliable human workers. We evaluate ZenCrowd in a real deployment and show how a combination of both probabilistic reasoning and crowdsourcing techniques can significantly improve the quality of the links, while limiting the amount of work performed by the crowd.},
	Acmid = {2187900},
	Address = {New York, NY, USA},
	Author = {Demartini, Gianluca and Difallah, Djellel Eddine and Cudr{\'e}-Mauroux, Philippe},
	Booktitle = {Proceedings of the 21st International Conference on World Wide Web},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:13:15 +0100},
	Doi = {10.1145/2187836.2187900},
	Isbn = {978-1-4503-1229-5},
	Keywords = {crowdsourcing, entity linking, linked data, probabilistic reasoning},
	Location = {Lyon, France},
	Numpages = {10},
	Pages = {469--478},
	Publisher = {ACM},
	Series = {WWW '12},
	Title = {ZenCrowd: Leveraging Probabilistic Reasoning and Crowdsourcing Techniques for Large-scale Entity Linking},
	Url = {http://doi.acm.org/10.1145/2187836.2187900},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2187836.2187900},
	Bdsk-Url-2 = {https://doi.org/10.1145/2187836.2187900}}

@inproceedings{Vytiniotis:2014:ZWP:2636228.2661115,
	Abstract = {Software-defined radio (SDR) brings the flexibility of software to the domain of wireless protocol design, promising both an ideal platform for research and innovation and the rapid deployment of new protocols on existing hardware. Most existing SDR platforms require careful hand-tuning of low-level code to be useful in the real world. In this talk I will describe Ziria, an SDR platform that is both easily programmable and performant. Ziria introduces a programming model that builds on ideas from functional programming and that is tailored to wireless physical layer tasks. The model captures the inherent and important distinction between data and control paths in this domain. I will describe the programming model, give an overview of the execution model, compiler optimizations, and current work. We have used Ziria to produce an implementation of 802.11a/g and a partial implementation of LTE.},
	Acmid = {2661115},
	Address = {New York, NY, USA},
	Author = {Vytiniotis, Dimitrios},
	Booktitle = {Proceedings of the 3rd ACM SIGPLAN Workshop on Functional High-performance Computing},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:12:54 +0100},
	Doi = {10.1145/2636228.2661115},
	Isbn = {978-1-4503-3040-4},
	Keywords = {data and control paths, high performance, programming model, software-defined radio},
	Location = {Gothenburg, Sweden},
	Numpages = {1},
	Pages = {1--1},
	Publisher = {ACM},
	Series = {FHPC '14},
	Title = {Ziria: Wireless Programming for Hardware Dummies},
	Url = {http://doi.acm.org/10.1145/2636228.2661115},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2636228.2661115},
	Bdsk-Url-2 = {https://doi.org/10.1145/2636228.2661115}}

@inproceedings{Balaji:2015:ZOL:2821650.2821674,
	Abstract = {Large scale deployment of sensors is essential to practical applications in cyber physical systems. For instance, instrumenting a commercial building for 'smart energy' management requires deployment and operation of thousands of measurement and metering sensors and actuators that direct operation of the HVAC system. Each of these sensors need to be named consistently and constantly calibrated. Doing this process manually is not only time consuming but also error prone given the scale, heterogeneity and complexity of buildings as well as lack of uniform naming schemas. To address this challenge, we propose Zodiac - a framework for automatically classifying, naming and managing sensors based on active learning from sensor metadata. In contrast to prior work, Zodiac requires minimal user input in terms of labelling examples while being more accurate. To evaluate Zodiac, we deploy it across four real buildings on our campus and label the ground truth metadata for all the sensors in these buildings manually. Using a combination of hierarchical clustering and random forest classifiers we show that Zodiac can successfully classify sensors with an average accuracy of 98% with 28% fewer training examples when compared to a regular expression based method.},
	Acmid = {2821674},
	Address = {New York, NY, USA},
	Author = {Balaji, Bharathan and Verma, Chetan and Narayanaswamy, Balakrishnan and Agarwal, Yuvraj},
	Booktitle = {Proceedings of the 2Nd ACM International Conference on Embedded Systems for Energy-Efficient Built Environments},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:12:31 +0100},
	Doi = {10.1145/2821650.2821674},
	Isbn = {978-1-4503-3981-0},
	Keywords = {active learning, automated naming, bacnet, building management systems, hvac, ontology, sensor metadata, smart buildings},
	Location = {Seoul, South Korea},
	Numpages = {10},
	Pages = {13--22},
	Publisher = {ACM},
	Series = {BuildSys '15},
	Title = {Zodiac: Organizing Large Deployment of Sensors to Create Reusable Applications for Buildings},
	Url = {http://doi.acm.org/10.1145/2821650.2821674},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2821650.2821674},
	Bdsk-Url-2 = {https://doi.org/10.1145/2821650.2821674}}

@inproceedings{Simpson:2014:ZOW:2567948.2579215,
	Abstract = {This paper introduces the Zooniverse citizen science project and software framework, outlining its structure from an observatory perspective: both as an observable web-based system in itself, and as an example of a platform iteratively developed according to real-world deployment and used at scale. We include details of the technical architecture of Zooniverse, including the mechanisms for data gathering across the Zooniverse operation, access, and analysis. We consider the lessons that can be drawn from the experience of designing and running Zooniverse, and how this might inform development of other web observatories.},
	Acmid = {2579215},
	Address = {New York, NY, USA},
	Author = {Simpson, Robert and Page, Kevin R. and De Roure, David},
	Booktitle = {Proceedings of the 23rd International Conference on World Wide Web},
	Date-Added = {2019-10-30 18:05:54 +0100},
	Date-Modified = {2020-10-22 01:12:09 +0100},
	Doi = {10.1145/2567948.2579215},
	Isbn = {978-1-4503-2745-9},
	Keywords = {citizen science, crowdsourcing, web observatories},
	Location = {Seoul, Korea},
	Numpages = {6},
	Pages = {1049--1054},
	Publisher = {ACM},
	Series = {WWW '14 Companion},
	Title = {Zooniverse: Observing the World's Largest Citizen Science Platform},
	Url = {http://doi.acm.org/10.1145/2567948.2579215},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2567948.2579215},
	Bdsk-Url-2 = {https://doi.org/10.1145/2567948.2579215}}

@inproceedings{Nakano:2013:BIN:2555319.2555429,
	Abstract = {n this position paper, we describe an architecture for intrabody nanonetworks, a new type of body area networks targeted to the molecular environment deep inside the human body. Our approach is to learn from biological systems (e.g., microbial organisms that establish a complex adaptive system in the body) to design an architecture for intrabody nanonetworks. Our initial thoughts on the architectural design as well as testbed are first described. Future challenges are then discussed toward the practical deployment of intrabody nanonetworks.},
	Acmid = {2555429},
	Address = {ICST, Brussels, Belgium, Belgium},
	Author = {Nakano, Tadashi and Hosoda, Kazufumi and Nakamura, Yutaka and Ishii, Kojiro},
	Booktitle = {Proceedings of the 8th International Conference on Body Area Networks},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:34:58 +0100},
	Doi = {10.4108/icst.bodynets.2013.253511},
	Isbn = {978-1-936968-89-3},
	Keywords = {bio-inspired approach, intrabody nanonetwork, microbial ecosystem, system architecture},
	Location = {Boston, Massachusetts},
	Numpages = {4},
	Pages = {484--487},
	Publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
	Series = {BodyNets '13},
	Title = {A Biologically-inspired Intrabody Nanonetwork: Design Considerations},
	Url = {https://doi.org/10.4108/icst.bodynets.2013.253511},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.4108/icst.bodynets.2013.253511}}

@inproceedings{Stoykov:2015:BMA:3233397.3233425,
	Abstract = {Although there is an extensive amount of research covering in the area of Cloud computing, the field of bio-inspired cloud computing is underinvestigated when compared to the general research area. This study tries to find answers on how a biomorphic model can be implemented in the cloud in order to achieve adaptive cloud behaviour. The process of cellular differentiation where cells transform from one type to another, is chosen to be the foundation model for a developed technical model. We define analogies to the cloud where stem cells are blank servers and web servers are cells with a specific function. With a combination of configuration management, version control and cloud deployment systems, an imitation of this biological process is applied in the cloud. The use of automated cloud scaling as a case of adaptive behaviour is the main goal of the research. One approach has been developed for mapping the biological model to the cloud which consists of a prototype where the signal detection and node activation is being triggered by using the concept of random generated timers. The obtained performance results were varying, depending on the general timer distribution, providing new ideas for future improvements and different algorithm proposals.},
	Acmid = {3233425},
	Address = {Piscataway, NJ, USA},
	Author = {Stoykov, Gyorgi and Yazidi, Anis},
	Booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:35:21 +0100},
	Isbn = {978-0-7695-5697-0},
	Keywords = {biomorphic model, cloud comptuing, nature inspired computing, web scaling},
	Location = {Limassol, Cyprus},
	Numpages = {7},
	Pages = {179--185},
	Publisher = {IEEE Press},
	Series = {UCC '15},
	Title = {A Biomorphic Model for Automated Cloud Adaptation},
	Url = {http://dl.acm.org/citation.cfm?id=3233397.3233425},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3233397.3233425}}

@inproceedings{Jrad:2013:BFM:2462326.2462339,
	Abstract = {Computational science workflows have been successfully run on traditional HPC systems like clusters and Grids for many years. Today, users are interested to execute their workflow applications in the Cloud to exploit the economic and technical benefits of this new emerging technology. The deployment and management of workflows over the current existing heterogeneous and not yet interoperable Cloud providers, however, is still a challenging task for the workflow developers. In this paper, we present a broker-based framework for running workflows in a multi-Cloud environment. The framework allows an automatic selection of the target Clouds, a uniform access to the Clouds, and workflow data management with respect to user Service Level Agreement (SLA) requirements. Following a simulation approach, we evaluated the framework with a real scientific workflow application in different deployment scenarios. The results show that our framework offers benefits to users by executing workflows with the expected performance and service quality at lowest cost.},
	Acmid = {2462339},
	Address = {New York, NY, USA},
	Author = {Jrad, Foued and Tao, Jie and Streit, Achim},
	Booktitle = {Proceedings of the 2013 International Workshop on Multi-cloud Applications and Federated Clouds},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:35:44 +0100},
	Doi = {10.1145/2462326.2462339},
	Isbn = {978-1-4503-2050-4},
	Keywords = {cloud broker, cloud computing, cloud workflow, intercloud computing, multi-cloud},
	Location = {Prague, Czech Republic},
	Numpages = {8},
	Pages = {61--68},
	Publisher = {ACM},
	Series = {MultiCloud '13},
	Title = {A Broker-based Framework for Multi-cloud Workflows},
	Url = {http://doi.acm.org/10.1145/2462326.2462339},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2462326.2462339},
	Bdsk-Url-2 = {https://doi.org/10.1145/2462326.2462339}}

@inproceedings{Maes:2007:CCD:1378063.1378132,
	Abstract = {With the evolution of internet towards web 2.0 and real time communications and the adoption of IMS by telecommunications service providers, SIP becomes the next major protocol that application developer may want to rely on to develop their latest applications.
As SIP specifications stabilize, SIP application server become well understood entities. In fact most application servers, like J2EE, now support SIP servlet containers. As a result it become essential to understand how such SIP applications should interact with other web or IT applications.
In this paper, we describe a call control driven MVC programming model for converged applications, defined as application that mix Web and SIP. The programming model is then generalized to any call or media control situation.
As most telecommunications networks, in particular mobile network evolve towards the IMS, such programming model is expected to become essential to the development of future IMS mobile applications. It is in fact not limited to mobile, but extends to any application on network deploying IP (internet, broadband, wired and wireless with or without IMS as well as converged applications that provides a same experience across multiple such access networks).
The programming model presented in this paper can be extended to rely on the notion of enabler, thereby supporting the combination of call control with other mobile, voice and communication features and the deployment over any network (IMS, SIP and other SIP deployments as well as legacy networks (e.g. PSTN and IN)).},
	Acmid = {1378132},
	Address = {New York, NY, USA},
	Author = {Maes, St{\'e}phane H.},
	Booktitle = {Proceedings of the 4th International Conference on Mobile Technology, Applications, and Systems and the 1st International Symposium on Computer Human Interaction in Mobile Technology},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:36:04 +0100},
	Doi = {10.1145/1378063.1378132},
	Isbn = {978-1-59593-819-0},
	Keywords = {IMS (IP multimedia subsystems), Parlay, SIP, call control, converged applications, media server control, multimedia applications, multimodal applications, real time communications (RTC), voice application, web 2.0},
	Location = {Singapore},
	Numpages = {8},
	Pages = {439--446},
	Publisher = {ACM},
	Series = {Mobility '07},
	Title = {A Call Control Driven MVC Programming Model for Mixing Web and Call or Multimedia Applications},
	Url = {http://doi.acm.org/10.1145/1378063.1378132},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1378063.1378132},
	Bdsk-Url-2 = {https://doi.org/10.1145/1378063.1378132}}

@inproceedings{Taylor:2008:CSD:1385269.1385283,
	Abstract = {Data management lies at the core of most modern information technology deployments. Accordingly, the reliability of the database management system (DBMS) is critical to the reputation and success of both its vendors and their clients. However, there is a dearth of work in the literature focused on the reliability of the DBMS. More specifically, research is yet to be focused on the variables that influence DBMS reliability and the relationships between these variables.
We present an initial case study focused on the relationships between component type, usage profiles, component size, component changes, component usage, and defect yield. The system under study is a distributed enterprise relational DBMS.},
	Acmid = {1385283},
	Address = {New York, NY, USA},
	Articleno = {11},
	Author = {Taylor, C. A. and Gittens, M. S. and Miranskyy, A. V.},
	Booktitle = {Proceedings of the 1st International Workshop on Testing Database Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:36:22 +0100},
	Doi = {10.1145/1385269.1385283},
	Isbn = {978-1-60558-233-7},
	Keywords = {case study, database management system, testing, usage profiles},
	Location = {Vancouver, British Columbia, Canada},
	Numpages = {6},
	Pages = {11:1--11:6},
	Publisher = {ACM},
	Series = {DBTest '08},
	Title = {A Case Study in Database Reliability: Component Types, Usage Profiles, and Testing},
	Url = {http://doi.acm.org/10.1145/1385269.1385283},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1385269.1385283},
	Bdsk-Url-2 = {https://doi.org/10.1145/1385269.1385283}}

@inproceedings{Peyton:2012:CSI:2667036.2667038,
	Abstract = {This paper describes a two year case study in the engineering and deployment of a Clinical Information System (CIS) called Palliative Care Information System (PAL-IS) for managing and monitoring team-based community care of palliative patients. The case study followed the methodology, architecture and ontology proposed in previous work to address workflow, behavioral and technology issues for CIS that support collaborative, mobile, and accessible healthcare. Both PAL-IS and the methodology used in its development are evaluated. The results give fresh insight into interoperability issues which complicate CIS design. They also highlight the importance of reporting requirements as a major driver for investment in CIS and a critical factor in CIS design.},
	Acmid = {2667038},
	Address = {Piscataway, NJ, USA},
	Author = {Peyton, Liam and Kuziemsky, Craig and Langayan, Dishant},
	Booktitle = {Proceedings of the 4th International Workshop on Software Engineering in Health Care},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:36:39 +0100},
	Isbn = {978-1-4673-1843-3},
	Keywords = {clinical information system, collaboration, community care, interoperability, ontology, systems design},
	Location = {Zurich, Switzerland},
	Numpages = {7},
	Pages = {8--14},
	Publisher = {IEEE Press},
	Series = {SEHC '12},
	Title = {A Case Study in Interoperable Support for Collaborative Community Healthcare},
	Url = {http://dl.acm.org/citation.cfm?id=2667036.2667038},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2667036.2667038}}

@inproceedings{Jaferian:2009:CSE:1641587.1641594,
	Abstract = {This case study describes the adoption of an enterprise identity management(IdM) system in an insurance organization. We describe the state of the organization before deploying the IdM system, and point out the challenges in its IdM practices. We describe the organization's requirements for an IdM system, why a particular solution was chosen, issues in the deployment and configuration of the solution, the expected benefits, and the new challenges that arose from using the solution. Throughout, we identify practical problems that can be the focus of future research and development efforts. Our results confirm and elaborate upon the findings of previous research, contributing to an as-yet immature body of cases about IdM. Furthermore, our findings serve as a validation of our previously identified guidelines for IT security tools in general.},
	Acmid = {1641594},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Jaferian, Pooya and Botta, David and Hawkey, Kirstie and Beznosov, Konstantin},
	Booktitle = {Proceedings of the Symposium on Computer Human Interaction for the Management of Information Technology},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:36:58 +0100},
	Doi = {10.1145/1641587.1641594},
	Isbn = {978-1-60558-572-7},
	Keywords = {case study, identity management, organizational factors, qualitative research, security tools},
	Location = {Baltimore, Maryland},
	Numpages = {10},
	Pages = {7:46--7:55},
	Publisher = {ACM},
	Series = {CHiMiT '09},
	Title = {A Case Study of Enterprise Identity Management System Adoption in an Insurance Organization},
	Url = {http://doi.acm.org/10.1145/1641587.1641594},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1641587.1641594},
	Bdsk-Url-2 = {https://doi.org/10.1145/1641587.1641594}}

@inproceedings{Ko:2011:CSP:1984642.1984644,
	Abstract = {Many software requirements are identified only after a product is deployed, once users have had a chance to try the software and provide feedback. Unfortunately, addressing such feedback is not always straightforward, even when a team is fully invested in user-centered design. To investigate what constrains a teams evolution decisions, we performed a 6-month field study of a team employing iterative user-centered design methods to the design, deployment and evolution of a web application for a university community. Across interviews with the team, analyses of their bug reports, and further interviews with both users and non-adopters of the application, we found most of the constraints on addressing user feedback emerged from conflicts between users heterogeneous use of information and inflexible assumptions in the team's software architecture derived from earlier user research. These findings highlight the need for new approaches to expressing and validating assumptions from user research as software evolves.},
	Acmid = {1984644},
	Address = {New York, NY, USA},
	Author = {Ko, Andrew J. and Lee, Michael J. and Ferrari, Valentina and Ip, Steven and Tran, Charlie},
	Booktitle = {Proceedings of the 4th International Workshop on Cooperative and Human Aspects of Software Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:37:19 +0100},
	Doi = {10.1145/1984642.1984644},
	Isbn = {978-1-4503-0576-1},
	Keywords = {bug reports, bug triage, software evolution, user feedback},
	Location = {Waikiki, Honolulu, HI, USA},
	Numpages = {8},
	Pages = {1--8},
	Publisher = {ACM},
	Series = {CHASE '11},
	Title = {A Case Study of Post-deployment User Feedback Triage},
	Url = {http://doi.acm.org/10.1145/1984642.1984644},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1984642.1984644},
	Bdsk-Url-2 = {https://doi.org/10.1145/1984642.1984644}}

@inproceedings{Beran:2011:CFQ:2095536.2095584,
	Abstract = {In distributed, service-oriented systems, in which several concrete service instances need to be composed in order to respond to a request, it is important to select service deployments in an optimal and efficient way. Quality of Service attributes of deployments and network links are taken into account to decide between workflows that are identical in terms of their functionality. Several heuristic approaches have been proposed to solve the resulting QoS-aware service selection problem, known to be NP-hard. In our previous work, motivated by two concrete application scenarios, we proposed a blackboard and a genetic algorithm and compared them in terms of solution quality, performance and scalability. In order to seamlessly run and evaluate further approaches and parallel versions of the current algorithms in a distributed environment, a general framework for service selection optimization has been implemented using Cloud Computing resources. A performance study on sequential and parallel blackboard and genetic algorithms for solving service selection problems has been carried out in the Cloud.},
	Acmid = {2095584},
	Address = {New York, NY, USA},
	Author = {Beran, Peter Paul and Vinek, Elisabeth and Schikuta, Erich},
	Booktitle = {Proceedings of the 13th International Conference on Information Integration and Web-based Applications and Services},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:37:38 +0100},
	Doi = {10.1145/2095536.2095584},
	Isbn = {978-1-4503-0784-0},
	Keywords = {blackboard, genetic algorithm, optimization, service selection},
	Location = {Ho Chi Minh City, Vietnam},
	Numpages = {4},
	Pages = {284--287},
	Publisher = {ACM},
	Series = {iiWAS '11},
	Title = {A Cloud-based Framework for QoS-aware Service Selection Optimization},
	Url = {http://doi.acm.org/10.1145/2095536.2095584},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2095536.2095584},
	Bdsk-Url-2 = {https://doi.org/10.1145/2095536.2095584}}

@inproceedings{An:2014:CCS:2611286.2611323,
	Abstract = {The OMG Data Distribution Service (DDS), which is a standard specification for data-centric publish/subscribe communications, has shown promise for use in internet of things (IoT) applications because of its loosely coupled and scalable nature, and support for multiple QoS properties, such as reliable and real-time message delivery in dynamic environments. However, the current OMG DDS specification does not define coordination and discovery services for DDS message brokers, which are used in wide area network deployments of DDS. This paper describes preliminary research on a cloud-enabled coordination service for DDS message brokers, PubSubCoord, to overcome these limitations. Our approach provides a novel solution that brings together (a) ZooKeeper, which is used for the distributed coordination logic between message brokers, (b) DDS Routing Service, which is used to bridge DDS endpoints connected to different networks, and (c) BlueDove, which is used to provide a single-hop message delivery between brokers. Our design can support publishers and subscribers that dynamically join and leave their subnetworks.},
	Acmid = {2611323},
	Address = {New York, NY, USA},
	Author = {An, Kyoungho and Gokhale, Aniruddha},
	Booktitle = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:38:00 +0100},
	Doi = {10.1145/2611286.2611323},
	Isbn = {978-1-4503-2737-4},
	Keywords = {cloud computing, coordination, data distribution service, discovery, middleware, publish/subscribe},
	Location = {Mumbai, India},
	Numpages = {4},
	Pages = {310--313},
	Publisher = {ACM},
	Series = {DEBS '14},
	Title = {A Cloud-enabled Coordination Service for Internet-scale OMG DDS Applications},
	Url = {http://doi.acm.org/10.1145/2611286.2611323},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2611286.2611323},
	Bdsk-Url-2 = {https://doi.org/10.1145/2611286.2611323}}

@inproceedings{Matzler:2015:CHF:2701319.2701334,
	Abstract = {Quality function deployment (QFD) is a method for quality assurance developed for application in production processes. One prominent tool for implementing QFD is the House of Quality (HoQ), whose basic design principles have been left unchanged for the last decades. Modern concepts for handling product variability, most notably feature models, represent intuitive means for a refurbished roof construction of the HoQ, and thus more expressiveness in the definition of functional requirements.},
	Acmid = {2701334},
	Address = {New York, NY, USA},
	Articleno = {75},
	Author = {M\"{a}tzler, Emanuel and Wally, Bernhard and Mazak, Alexandra},
	Booktitle = {Proceedings of the Ninth International Workshop on Variability Modelling of Software-intensive Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 11:32:55 +0100},
	Doi = {10.1145/2701319.2701334},
	Isbn = {978-1-4503-3273-6},
	Keywords = {House of Quality; Feature Models; Requirements Engineering; Variability Modeling; Quality Function Deployment},
	Location = {Hildesheim, Germany},
	Numpages = {5},
	Pages = {75:75--75:79},
	Publisher = {ACM},
	Series = {VaMoS '15},
	Title = {A Common Home for Features and Requirements: Retrofitting the House of Quality with Feature Models},
	Url = {http://doi.acm.org/10.1145/2701319.2701334},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2701319.2701334},
	Bdsk-Url-2 = {https://doi.org/10.1145/2701319.2701334}}

@inproceedings{Woensel:2014:CMR:2682647.2682782,
	Abstract = {Semantic Web technology is used extensively in the health domain, due to its ability to specify expressive, domain-specific data, as well as its capacity to facilitate data integration between heterogeneous, health-related sources. In the health domain, mobile devices are an essential part of patient self-management approaches, where local clinical decision support is applied to ensure that patients receive timely clinical findings. Currently, increases in mobile device capabilities have enabled the deployment of Semantic Web technologies on mobile platforms, enabling the consumption of rich, semantically described health data. To make this semantic health data available to local decision support as well, Semantic Web reasoning should be deployed on mobile platforms. However, there is currently a lack of software solutions and performance analysis of mobile, Semantic Web reasoning engines. This paper presents and compares the mobile benchmarks of 4 reasoning engines, applied on a dataset and rule set for patients with Atrial Fibrillation (AF). In particular, these benchmarks investigate the scalability of the mobile reasoning processes, and study reasoning performance for different process flows in decision support. For the purpose of these benchmarks, we extended a number of existing rule engines and RDF stores with Semantic Web reasoning capabilities.},
	Acmid = {2682782},
	Address = {Washington, DC, USA},
	Author = {Woensel, William Van and Haider, Newres Al and Roy, Patrice C. and Ahmad, Ahmad Marwan and Abidi, Syed S. R.},
	Booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:28:38 +0100},
	Doi = {10.1109/WI-IAT.2014.25},
	Isbn = {978-1-4799-4143-8},
	Numpages = {8},
	Pages = {126--133},
	Publisher = {IEEE Computer Society},
	Series = {WI-IAT '14},
	Title = {A Comparison of Mobile Rule Engines for Reasoning on Semantic Web Based Health Data},
	Url = {https://doi.org/10.1109/WI-IAT.2014.25},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1109/WI-IAT.2014.25}}

@inproceedings{Billingsley:2013:CTI:2462476.2465592,
	Abstract = {In previous work we introduced a software studio course in which seventy students used continuous integration practices to collaborate on a common legacy code base. This enabled students to experience the issues of realistically sized software projects, and learn and apply appropriate techniques to overcome them, in a course without significant extra staffing. Although the course was broadly successful in its goals, it received a mixed response from students, and our paper noted several issues to overcome. This paper considers experimental changes to the course in light of our previous findings, and additional data from the official student surveys. Two iterations of the course and their respective results are compared. Whereas our previous paper addressed the feasibility of such a course, this paper considers how the student experience can be improved. The paper also considers how such a course can be adapted for more heterogeneous cohorts, such as the introduction of an unknown number of design and database students, or the introduction of online students.},
	Acmid = {2465592},
	Address = {New York, NY, USA},
	Author = {Billingsley, William and Steel, Jim},
	Booktitle = {Proceedings of the 18th ACM Conference on Innovation and Technology in Computer Science Education},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:38:21 +0100},
	Doi = {10.1145/2462476.2465592},
	Isbn = {978-1-4503-2078-8},
	Keywords = {continuous integration, experience report, software engineering, studio course},
	Location = {Canterbury, England, UK},
	Numpages = {6},
	Pages = {213--218},
	Publisher = {ACM},
	Series = {ITiCSE '13},
	Title = {A Comparison of Two Iterations of a Software Studio Course Based on Continuous Integration},
	Url = {http://doi.acm.org/10.1145/2462476.2465592},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2462476.2465592},
	Bdsk-Url-2 = {https://doi.org/10.1145/2462476.2465592}}

@inproceedings{Ruppen:2015:CBA:2834791.2834792,
	Abstract = {Model Driven Architectures are the holy grail of software engineering. Instead of writing code, developers draw models from the client's specification, which are then compiled into executable code (skeletons). We have taken this principle and applied it to the WoT. With the help of a meta-model tailored for the WoT we are able to build models to simultaneously take care of the physical and virtual aspects of smart devices. These models can then automatically be turned into code skeletons. The emphasis in the meta-model and its associated tools is reusability. Following the software engineering principle of independent reusable and deployable components, the outcome of the meta-model compiler are WoT compliant components.},
	Acmid = {2834792},
	Address = {New York, NY, USA},
	Articleno = {2},
	Author = {Ruppen, Andreas and Pasquier, Jacques and Meyer, Sonja and R\"{u}edlinger, Alexander},
	Booktitle = {Proceedings of the 6th International Workshop on the Web of Things},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:38:39 +0100},
	Doi = {10.1145/2834791.2834792},
	Isbn = {978-1-4503-4045-8},
	Keywords = {Web of Things, component based approach, meta-model, model driven architecture, software architecture, software development approach},
	Location = {Seoul, Republic of Korea},
	Numpages = {6},
	Pages = {2:1--2:6},
	Publisher = {ACM},
	Series = {WoT '15},
	Title = {A Component Based Approach for the Web of Things},
	Url = {http://doi.acm.org/10.1145/2834791.2834792},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2834791.2834792},
	Bdsk-Url-2 = {https://doi.org/10.1145/2834791.2834792}}

@inproceedings{Yan:2008:CTM:1387329.1387330,
	Abstract = {The growing importance of component software introduces special requirements on trust due to the nature of applications they provide, in particular when the system supports dynamic component deployment. This paper presents a comprehensive trust model in order to specify, evaluate and manage various trust relationships that exist among entities in a component software system. It contains a sub-model to present trust relationships among system entities, a sub-model to specify the information related to trust management for a software component, a sub-model for trust evaluation and a sub-model for trust management. This trust model supports trust management based on trust evaluation both at component download time and runtime.},
	Acmid = {1387330},
	Address = {New York, NY, USA},
	Author = {Yan, Zheng},
	Booktitle = {Proceedings of the 4th International Workshop on Security, Privacy and Trust in Pervasive and Ubiquitous Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:39:00 +0100},
	Doi = {10.1145/1387329.1387330},
	Isbn = {978-1-60558-207-8},
	Keywords = {component software, security, trust, trust management, trust model},
	Location = {Sorrento, Italy},
	Numpages = {6},
	Pages = {1--6},
	Publisher = {ACM},
	Series = {SecPerU '08},
	Title = {A Comprehensive Trust Model for Component Software},
	Url = {http://doi.acm.org/10.1145/1387329.1387330},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1387329.1387330},
	Bdsk-Url-2 = {https://doi.org/10.1145/1387329.1387330}}

@inproceedings{Zander:2012:CMI:2362499.2362518,
	Abstract = {Linked Data, Augmented Reality (AR), and technical advancements in mobile information technology lead to an increasing desire to exploit Linked Data for the integration and visualization in mobile AR applications. However, current approaches are either bound to existing client-server-based infrastructures or use closed data sources and proprietary data formats. Moreover, a number of related approaches are built upon content-based recognition algorithms that are both memory and processing-intensive, require a permanent connection to a host, and thus are inappropriate for a direct deployment onto mobile devices. In this work, we present a computational model that builds on a sensor-based tracking approach and maps proactively replicated Linked Data sets to a virtual representation of the user's vicinity computed by a mathematical model. We demonstrate the applicability of our approach through a proof-of-concept AR application that retrieves and aggregates mountain-specific data from a set of different sources and displays such data in a live-view interface. In consequence, our approach is resource-efficient, does not require a permanent network connection, is independent from existing server-based infrastructures, and allows to process Linked Data directly on a mobile device.},
	Acmid = {2362518},
	Address = {New York, NY, USA},
	Author = {Zander, Stefan and Chiu, Chris and Sageder, Gerhard},
	Booktitle = {Proceedings of the 8th International Conference on Semantic Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:39:24 +0100},
	Doi = {10.1145/2362499.2362518},
	Isbn = {978-1-4503-1112-0},
	Keywords = {augmented reality, linked data, mobile information systems, semantic web, visualization},
	Location = {Graz, Austria},
	Numpages = {8},
	Pages = {133--140},
	Publisher = {ACM},
	Series = {I-SEMANTICS '12},
	Title = {A Computational Model for the Integration of Linked Data in Mobile Augmented Reality Applications},
	Url = {http://doi.acm.org/10.1145/2362499.2362518},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2362499.2362518},
	Bdsk-Url-2 = {https://doi.org/10.1145/2362499.2362518}}

@article{Yazbek:2010:CQA:1838687.1838711,
	Abstract = {The quality of software has become more important to software companies in the past years. Software measurement is one of many approaches that is used to check the quality of software [18]. This may involve measuring some attributes of a software product or a software process and comparing these measurements to each other or to some desirable level. A software metric is "any type of measurement that relates to a software system, process or related documentation" [7]. Software metrics can help to improve the quality of the produced software. However, metrics and metrics tools are still not used in most software companies -- for example in the 3 companies where we cooperated in last 5 years. One reason is that there is lack of knowledge about metrics and hence software metrics are still unknown or difficult to use for some developers, and software measurement is still time-consuming for managers. Another reason is that, good metrics tools are still expensive for small and middle companies. In our opinion the effectiveness of metrics can be improved by simple organizational expedients. In this paper we present new/adapted requirements on metrics in CASE tools to define flexible product quality models. For this quality model we tried to use some standard techniques, e.g. metrics suite, metrics visualization or metrics filtering to show how metrics in CASE tools can be defined and how it can benefit different people who are involved in a software deployment.},
	Acmid = {1838711},
	Address = {New York, NY, USA},
	Author = {Yazbek, Hashem},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:39:47 +0100},
	Doi = {10.1145/1838687.1838711},
	Issn = {0163-5948},
	Issue_Date = {September 2010},
	Journal = {SIGSOFT Softw. Eng. Notes},
	Keywords = {coding tools and techniques, design tools and techniques, management - software quality assurance, metrics},
	Number = {5},
	Numpages = {8},
	Pages = {1--8},
	Publisher = {ACM},
	Title = {A Concept of Quality Assurance for Metrics in CASE-tools},
	Url = {http://doi.acm.org/10.1145/1838687.1838711},
	Volume = {35},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1838687.1838711},
	Bdsk-Url-2 = {https://doi.org/10.1145/1838687.1838711}}

@inproceedings{Muriithi:2013:CFD:2513456.2513502,
	Abstract = {Smart use of business intelligence (BI) can allow organizations to leverage the huge amounts of transactional data at their disposal and turn it into a powerful decision support mechanism that gives them competitive advantage. Despite the potential benefits of an effective BI system, the adoption and use of BI systems within the enterprise remains low, especially among smaller companies with resource constraints. This can partly be explained by the predominant deployment approach available today in which a firm needs to procure, install, configure and operate a BI system in-house. Barriers of high cost, complexity and lack of in-house expertise discourage many firms from adopting BI systems. This paper argues that adopting a cloud computing model, where BI is offered as a service over the Internet can lower these barriers and accelerate the pace of BI adoption. However, migrating BI systems from traditional on-premise environments to the cloud presents huge challenges. There are technical, economic, organizational and regulatory hurdles to overcome. Further, BI systems are multi-component (ETL, Data warehouse, data marts, OLAP, reporting, data mining etc.) and deciding which component(s) to move to the cloud, and which ones to leave on-premise needs careful consideration. In addition, the fact that cloud computing is still in its infancy means there is a general lack of conceptual and architectural frameworks to guide companies considering migrating enterprise systems to the cloud. This paper takes a closer look into traditional BI and proposes a conceptual framework that companies can use to chart an adoption path for cloud BI. The framework combines attributes of IT outsourcing, traditional BI, cloud computing as well as decision theory to present a consolidated view of cloud BI. The domain of South African Higher Education was chosen as the target in which the framework will be tested.},
	Acmid = {2513502},
	Address = {New York, NY, USA},
	Author = {Muriithi, G. M. and Kotz{\'e}, J. E.},
	Booktitle = {Proceedings of the South African Institute for Computer Scientists and Information Technologists Conference},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:40:06 +0100},
	Doi = {10.1145/2513456.2513502},
	Isbn = {978-1-4503-2112-9},
	Keywords = {BI as a service, SaaS, business intelligence, cloud BI, cloud computing, data warehouses},
	Location = {East London, South Africa},
	Numpages = {5},
	Pages = {96--100},
	Publisher = {ACM},
	Series = {SAICSIT '13},
	Title = {A Conceptual Framework for Delivering Cost Effective Business Intelligence Solutions As a Service},
	Url = {http://doi.acm.org/10.1145/2513456.2513502},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2513456.2513502},
	Bdsk-Url-2 = {https://doi.org/10.1145/2513456.2513502}}

@inproceedings{Gaudin:2011:CTB:1998582.1998633,
	Abstract = {This work presents an approach to self-healing that deals with un-handled exceptions within an executing program. More precisely, we propose an approach based on control theory that automatically disables system functionalities that have led to runtime exceptions. This approach requires the system to be instrumented prior to deployment so that it can later interact with a supervisor. This supervisor encodes the only sequences of actions (method calls) of the system that are permitted. We describe an implementation that automatically generates instrumentation for Java systems and demonstrate the efficacy of this approach through a comprehensive example.},
	Acmid = {1998633},
	Address = {New York, NY, USA},
	Author = {Gaudin, Benoit and Vassev, Emil Iordanov and Nixon, Patrick and Hinchey, Michael},
	Booktitle = {Proceedings of the 8th ACM International Conference on Autonomic Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:40:35 +0100},
	Doi = {10.1145/1998582.1998633},
	Isbn = {978-1-4503-0607-2},
	Keywords = {self-healing, software control theory, softwaremaintenance.},
	Location = {Karlsruhe, Germany},
	Numpages = {4},
	Pages = {217--220},
	Publisher = {ACM},
	Series = {ICAC '11},
	Title = {A Control Theory Based Approach for Self-healing of Un-handled Runtime Exceptions},
	Url = {http://doi.acm.org/10.1145/1998582.1998633},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1998582.1998633},
	Bdsk-Url-2 = {https://doi.org/10.1145/1998582.1998633}}

@inproceedings{SrirangamNarashiman:2014:CAD:2633661.2633665,
	Abstract = {Information and communication technology (ICT) infrastructure plays an important role to realize the full potential of Smart Grid applications. Smart grids utilize ICT entities to enhance efficiency, reliability and sustainability of power generation and distribution network. Majority of the architectures proposed hitherto focus only on a specific architectural aspect, like communication, storage, processing requirement, etc. Recent studies have shown that lack of knowledge on which architecture best satisfies certain information management requirements has hindered large scale smart grid deployments. In this paper, we investigate the cost-benefit analysis of four data processing architectures for various applications in smart grid. We introduce several key cost indicators to analyze hierarchical data processing architectures for the smart grid. In our evaluation, we consider realistic deployments for both dense and sparse environments. Results reported here are significant for smart grid designers, who can use them to discern the architecture that best fits the system requirements.},
	Acmid = {2633665},
	Address = {New York, NY, USA},
	Author = {Srirangam Narashiman, Akshay Uttama Nambi and Vasirani, Matteo and Prasad, R. Venkatesha Prasad and Aberer, Karl},
	Booktitle = {Proceedings of the 2014 ACM International Workshop on Wireless and Mobile Technologies for Smart Cities},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:41:02 +0100},
	Doi = {10.1145/2633661.2633665},
	Isbn = {978-1-4503-3036-7},
	Keywords = {data processing, distributed information systems, smart grid},
	Location = {Philadelphia, Pennsylvania, USA},
	Numpages = {10},
	Pages = {91--100},
	Publisher = {ACM},
	Series = {WiMobCity '14},
	Title = {A Cost-benefit Analysis of Data Processing Architectures for the Smart Grid},
	Url = {http://doi.acm.org/10.1145/2633661.2633665},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2633661.2633665},
	Bdsk-Url-2 = {https://doi.org/10.1145/2633661.2633665}}

@inproceedings{Bennett:2015:DST:2756755.2756763,
	Abstract = {In this paper, we present a method to synchronize data from multiple sensors in a cyber-physical system without any software or hardware modifications to the sensors. This method allows for synchronization of low-power embedded systems in heterogeneous sensor networks, regardless of accuracy of individual sensor clocks by using the events in the physical world to drive the synchronization in the cyber world. We propose two methods to select portions of sensor data streams to drive the synchronization: one leveraging the notion of known templates and the other using an information theoretic approach. Using the events as well as cues from the delay models, we determine alignment points between the data streams. These alignment points are used to synchronize the data. This novel approach is based solely on the sensor data for synchronization, and it can be applied post-deployment on systems of heterogeneous sensors that are not well designed and lack effective synchronization. Experiments show an average accuracy improvement from ~12000ppm to ~2400ppm for a template based-method and from ~12000 to ~277ppm and ~445ppm for information theoretic methods when comparing the synchronized (corrected) clock data to an ideal clock source.},
	Acmid = {2756763},
	Address = {New York, NY, USA},
	Author = {Bennett, Terrell R. and Gans, Nicholas and Jafari, Roozbeh},
	Booktitle = {Proceedings of the Second International Workshop on the Swarm at the Edge of the Cloud},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:41:22 +0100},
	Doi = {10.1145/2756755.2756763},
	Isbn = {978-1-4503-3595-9},
	Keywords = {alignment, cyber-physical systems, data-driven, sensor networks, synchronization},
	Location = {Seattle, Washington},
	Numpages = {6},
	Pages = {49--54},
	Publisher = {ACM},
	Series = {SWEC '15},
	Title = {A Data-driven Synchronization Technique for Cyber-physical Systems},
	Url = {http://doi.acm.org/10.1145/2756755.2756763},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2756755.2756763},
	Bdsk-Url-2 = {https://doi.org/10.1145/2756755.2756763}}

@article{Wu:2016:DOS:3001754.2983642,
	Abstract = {With the prosperity of media streaming applications over the Internet in the past decades, multimedia data has sharply increased (categorized as multimedia big data), which exerts more pressure on the infrastructure, such as networking of the application provider. In order to move this hurdle, an increasing number of traditional media streaming applications have migrated from a private server cluster onto the cloud. With the elastic resource provisioning and centralized management of the cloud, the operational costs of media streaming application providers can decrease dramatically. However, to the best of our knowledge, existing migration solutions do not fully take viewer information such as hardware condition into consideration. In this article, we consider the deployment optimization problem named ODP by leveraging local memories at each viewer. Considering the NP-hardness of calculating the optimal solution, we turn to propose computationally tractable algorithms. Specifically, we unfold the original problem into two interactive subproblems: coarse-grained migration subproblem and fine-grained scheduling subproblem. Then, the corresponding offline approximation algorithms with performance guarantee and computational efficiency are given. The results of extensive evaluation show that compared with the baseline algorithm without leveraging local memories at viewers, our proposed algorithms and their online versions can decrease total bandwidth reservation and enhance the utilization of bandwidth reservation dramatically.},
	Acmid = {2983642},
	Address = {New York, NY, USA},
	Articleno = {73},
	Author = {Wu, Taotao and Dou, Wanchun and Wu, Fan and Tang, Shaojie and Hu, Chunhua and Chen, Jinjun},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:41:43 +0100},
	Doi = {10.1145/2983642},
	Issn = {1551-6857},
	Issue_Date = {December 2016},
	Journal = {ACM Trans. Multimedia Comput. Commun. Appl.},
	Keywords = {Large-scale media streaming application, cloud, deployment optimization, local memory, multimedia big data},
	Number = {5s},
	Numpages = {23},
	Pages = {73:1--73:23},
	Publisher = {ACM},
	Title = {A Deployment Optimization Scheme Over Multimedia Big Data for Large-Scale Media Streaming Application},
	Url = {http://doi.acm.org/10.1145/2983642},
	Volume = {12},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2983642},
	Bdsk-Url-2 = {https://doi.org/10.1145/2983642}}

@inproceedings{Stillwell:2015:DAI:2804371.2804372,
	Abstract = {We present a description of the development and deployment infrastructure being created to support the integration effort of HARNESS, an EU FP7 project. HARNESS is a multi-partner research project intended to bring the power of heterogeneous resources to the cloud. It consists of a number of different services and technologies that interact with the OpenStack cloud computing platform at various levels. Many of these components are being developed independently by different teams at different locations across Europe, and keeping the work fully integrated is a challenge. We use a combination of Vagrant based virtual machines, Docker containers, and Ansible playbooks to provide a consistent and up-to-date environment to each developer. The same playbooks used to configure local virtual machines are also used to manage a static testbed with heterogeneous compute and storage devices, and to automate ephemeral larger-scale deployments to Grid'5000. Access to internal projects is managed by GitLab, and automated testing of services within Docker-based environments and integrated deployments within virtual-machines is provided by Buildbot.},
	Acmid = {2804372},
	Address = {New York, NY, USA},
	Author = {Stillwell, Mark and Coutinho, Jose G. F.},
	Booktitle = {Proceedings of the 1st International Workshop on Quality-Aware DevOps},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:42:03 +0100},
	Doi = {10.1145/2804371.2804372},
	Isbn = {978-1-4503-3817-2},
	Keywords = {Ansible, Automated Testing, BuildBot, Configuration Management, DevOps, Docker, GitLab, OpenStack, Vagrant},
	Location = {Bergamo, Italy},
	Numpages = {6},
	Pages = {1--6},
	Publisher = {ACM},
	Series = {QUDOS 2015},
	Title = {A DevOps Approach to Integration of Software Components in an EU Research Project},
	Url = {http://doi.acm.org/10.1145/2804371.2804372},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2804371.2804372},
	Bdsk-Url-2 = {https://doi.org/10.1145/2804371.2804372}}

@inproceedings{Kelley:2014:DAI:2608029.2608037,
	Abstract = {When envisioning "The Cloud," one is often presented with an idyllic black box of functionality that seamlessly stores arbitrary amounts of data while providing endless CPU cycles.
However, Cloud deployments are often partitioned along institutional and middleware boundaries that link computation and storage infrastructures. Decoupling storage from computation resource providers allows for greater flexibility in resource provisioning and new data storage paradigms to emerge.
This paper proposes a decentralized data management architecture that facilitates interoperability between Clouds and other heterogeneous systems. The goals of this research are to augment the latent storage capacity within provisioned Cloud VMs with existing institutional resources to build low-cost Storage Clouds for scientific computing.},
	Acmid = {2608037},
	Address = {New York, NY, USA},
	Author = {Kelley, Ian},
	Booktitle = {Proceedings of the 5th ACM Workshop on Scientific Cloud Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:42:34 +0100},
	Doi = {10.1145/2608029.2608037},
	Isbn = {978-1-4503-2911-8},
	Keywords = {attic, data management, p2p, peer-to-peer, science clouds},
	Location = {Vancouver, BC, Canada},
	Numpages = {8},
	Pages = {53--60},
	Publisher = {ACM},
	Series = {ScienceCloud '14},
	Title = {A Distributed Architecture for Intra- and Inter-cloud Data Management},
	Url = {http://doi.acm.org/10.1145/2608029.2608037},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2608029.2608037},
	Bdsk-Url-2 = {https://doi.org/10.1145/2608029.2608037}}

@inproceedings{Rosenkranz:2015:DTS:2753476.2753481,
	Abstract = {In this paper, we discuss challenges that are specific to testing of open IoT software systems. The analysis reveals gaps compared to wireless sensor networks as well as embedded software. We propose a testing framework which (a) supports continuous integration techniques, (b) allows for the integration of project contributors to volunteer hardware and software resources to the test system, and (c) can function as a permanent distributed plugtest for network interoperability testing. The focus of this paper lies in open-source IoT development but many aspects are also applicable to closed-source projects.},
	Acmid = {2753481},
	Address = {New York, NY, USA},
	Author = {Rosenkranz, Philipp and W\"{a}hlisch, Matthias and Baccelli, Emmanuel and Ortmann, Ludwig},
	Booktitle = {Proceedings of the 2015 Workshop on IoT Challenges in Mobile and Industrial Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:42:55 +0100},
	Doi = {10.1145/2753476.2753481},
	Isbn = {978-1-4503-3502-7},
	Keywords = {interoperability, open-source iot, test system architecture},
	Location = {Florence, Italy},
	Numpages = {6},
	Pages = {43--48},
	Publisher = {ACM},
	Series = {IoT-Sys '15},
	Title = {A Distributed Test System Architecture for Open-source IoT Software},
	Url = {http://doi.acm.org/10.1145/2753476.2753481},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2753476.2753481},
	Bdsk-Url-2 = {https://doi.org/10.1145/2753476.2753481}}

@inproceedings{Nittel:2007:DMD:1254850.1254860,
	Abstract = {Traditional means of observing the ocean, like fixed mooring stations and radar systems, are difficult and expensive to deploy and provide coarse-grained and data measurements of currents and waves. In this paper, we explore the use of inexpensive wireless drifters as an alternative flexible infrastructure for fine-grained ocean monitoring. Surface drifters are designed specifically to move passively with the flow of water on the ocean surface and they are able to acquire sensor readings and GPS-generated positions at regular intervals. We view the fleet of drifters as a wireless ad-hoc sensor network with two types of nodes:i) a few powerful drifters with satellite connectivity, acting as mobile base-stations, and ii)a large number of low-power drifters with short-range acoustic or radio connectivity. Using real datasets from the Gulf of Maine (US) and the Liverpool Bay (UK), we study connectivity and uniformity properties of the ad-hoc mobile sensor network. We investigate the effect of deployment strategy, weather conditions as well as seasonal changes on the ability of drifters to relay readings to the end-users,and to provide sufficient sensing coverage of the monitored area. Our empirical study provides useful insights on how to design distributed routing and in-network processing algorithms tailored for ocean-monitoring sensor networks.},
	Acmid = {1254860},
	Address = {New York, NY, USA},
	Author = {Nittel, Silvia and Trigoni, Niki and Ferentinos, Konstantinos and Neville, Francois and Nural, Arda and Pettigrew, Neal},
	Booktitle = {Proceedings of the 6th ACM International Workshop on Data Engineering for Wireless and Mobile Access},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:43:14 +0100},
	Doi = {10.1145/1254850.1254860},
	Isbn = {978-1-59593-765-0},
	Keywords = {ad-hoc sensor networks, drifters, geosensor networks, mobility, oceanography},
	Location = {Beijing, China},
	Numpages = {10},
	Pages = {49--58},
	Publisher = {ACM},
	Series = {MobiDE '07},
	Title = {A Drift-tolerant Model for Data Management in Ocean Sensor Networks},
	Url = {http://doi.acm.org/10.1145/1254850.1254860},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1254850.1254860},
	Bdsk-Url-2 = {https://doi.org/10.1145/1254850.1254860}}

@inproceedings{Heinz:2016:DEM:2851613.2851957,
	Abstract = {'How to' manuals help potential users with deploying and understanding software technologies such as web applications or servers. In a domain analysis, we survey existing 'how to' manuals to assess the feasibility of making the manuals executable and to derive a suggestion for domain-specific language support. We realize a DSL for executable 'how to' manuals and refer to the approach as 'literate deployment scripting', as it is inspired by literate programming in that all the code for deployment and configuration is embedded into its documentation. This includes code (or commands) for an initial deployment or changes to a deployed software system.},
	Acmid = {2851957},
	Address = {New York, NY, USA},
	Author = {Heinz, Marcel and Helsper, Philipp and L\"{a}mmel, Ralf and Schmidt, Tobias M.},
	Booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:43:39 +0100},
	Doi = {10.1145/2851613.2851957},
	Isbn = {978-1-4503-3739-7},
	Keywords = {executable how to, executable manual, install scripts, literate deployment, literate programming, software deployment},
	Location = {Pisa, Italy},
	Numpages = {3},
	Pages = {2007--2009},
	Publisher = {ACM},
	Series = {SAC '16},
	Title = {A DSL for Executable 'How to' Manuals},
	Url = {http://doi.acm.org/10.1145/2851613.2851957},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2851613.2851957},
	Bdsk-Url-2 = {https://doi.org/10.1145/2851613.2851957}}

@inproceedings{Laouid:2016:DDK:2896387.2900322,
	Abstract = {Ensuring key management in Wireless Sensor Networks has a vital importance, especially when sensor nodes have to communicate in hostile environments. In this paper, we propose a new simple and resource-aware key management scheme. The scheme is based on the idea that the initial pre-distributed key is not pre-loaded in all nodes to improve its resilience to node compromising attacks. Each node has to store an initial key, a set of prime number groups and a pseudo-random function. This pre-distributed secret information is used to establish pairwise keys between adjacent nodes after their deployment. Moreover, the proposed scheme is dynamic since it allows a flexible key refresh.},
	Acmid = {2900322},
	Address = {New York, NY, USA},
	Articleno = {70},
	Author = {Laouid, Abdelkader and Messai, Mohamed-Lamine and Bounceur, Ahc\`{e}ne and Euler, Reinhardt and Dahmani, Abdelnasser and Tari, Abdelkamel},
	Booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:44:06 +0100},
	Doi = {10.1145/2896387.2900322},
	Isbn = {978-1-4503-4063-2},
	Keywords = {Distributed algorithms, Key management, Security, WSNs},
	Location = {Cambridge, United Kingdom},
	Numpages = {6},
	Pages = {70:1--70:6},
	Publisher = {ACM},
	Series = {ICC '16},
	Title = {A Dynamic and Distributed Key Management Scheme for Wireless Sensor Networks},
	Url = {http://doi.acm.org/10.1145/2896387.2900322},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2896387.2900322},
	Bdsk-Url-2 = {https://doi.org/10.1145/2896387.2900322}}

@inproceedings{Niepert:2007:DOD:1255175.1255230,
	Abstract = {The successful deployment of digital technologies by humanities scholars presents computer scientists with a number of unique scientific and technological challenges. The task seems particularly daunting because issues in the humanities are presented in abstract language demanding the kind of subtle interpretation often thought to be beyond the scope of artificial intelligence, and humanities scholars themselves often disagree about the structure of their disciplines. The future of humanities computing depends on having tools for automatically discovering complex semantic relationships among different parts of a corpus. Digital library tools for the humanities will need to be capable of dynamically tracking the introduction of new ideas and interpretations and applying them to older texts in ways that support the needs of scholars and students.
This paper describes the design of new algorithms and the adjustment of existing algorithms to support the automated and semi-automated management of domain-rich metadata for an established digital humanities project, the Stanford Encyclopedia of Philosophy. Our approach starts with a "hand-built" formal ontology that is modified and extended by a combination of automated and semi-automated methods, thus becoming a "dynamic ontology". We assess the suitability of current information retrieval and information extraction methods for the task of automatically maintaining the ontology. We describe a novel measure of term-relatedness that appears to be particularly helpful for predicting hierarchical relationships in the ontology. We believe that our project makes a further contribution to information science by being the first to harness the collaboration inherent in a expert-maintained dynamic reference work to the task of maintaining and verifying a formal ontology. We place special emphasis on the task of bringing domain expertise to bear on all phases of the development and deployment of the system, from the initial design of the software and ontology to its dynamic use in a fully operational digital reference work.},
	Acmid = {1255230},
	Address = {New York, NY, USA},
	Author = {Niepert, Mathias and Buckner, Cameron and Allen, Colin},
	Booktitle = {Proceedings of the 7th ACM/IEEE-CS Joint Conference on Digital Libraries},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:44:25 +0100},
	Doi = {10.1145/1255175.1255230},
	Isbn = {978-1-59593-644-8},
	Keywords = {digital humanities, dynamic ontology, formal ontology, information extraction, information retrieval, link mining, metadata},
	Location = {Vancouver, BC, Canada},
	Numpages = {10},
	Pages = {288--297},
	Publisher = {ACM},
	Series = {JCDL '07},
	Title = {A Dynamic Ontology for a Dynamic Reference Work},
	Url = {http://doi.acm.org/10.1145/1255175.1255230},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1255175.1255230},
	Bdsk-Url-2 = {https://doi.org/10.1145/1255175.1255230}}

@inproceedings{Moisan:2012:FAS:2664431.2664444,
	Abstract = {Building large scale systems involves many design decisions, both at specification and implementation levels. This is due to numerous variants in the description of the task to achieve and its execution context as well as in the assembly of software components. We have modeled variability for large scale systems using feature diagrams, a formalism well suited for modeling variablility. These models are built with a clear separation of concerns between specification and implementation aspects. They are used at design and deployment time as well as at execution time.
Our test application domain is video surveillance systems, from a software engineering perspective. These are good candidates to put model driven engineering to the test, because of the huge variability in both the surveillance tasks and the video analysis algorithms. They are also dynamically adaptive systems, thus suitable for models at run time approaches.
We propose techniques and tools to define the models, to operate on them, and to transform specification requirements into an effective implementation of a processing chain. We also define a run time architecture to integrate models into the adaptation loop.},
	Acmid = {2664444},
	Address = {Piscataway, NJ, USA},
	Author = {Moisan, Sabine and Rigault, Jean-Paul and Acher, Mathieu},
	Booktitle = {Proceedings of the 4th International Workshop on Modeling in Software Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:44:48 +0100},
	Isbn = {978-1-4673-1757-3},
	Keywords = {feature model, models at run time, software variability, video surveillance},
	Location = {Zurich, Switzerland},
	Numpages = {7},
	Pages = {84--90},
	Publisher = {IEEE Press},
	Series = {MiSE '12},
	Title = {A Feature-based Approach to System Deployment and Adaptation},
	Url = {http://dl.acm.org/citation.cfm?id=2664431.2664444},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2664431.2664444}}

@inproceedings{Junqueira:2008:FFV:1456536.1456576,
	Abstract = {Version control is an activity very important for high-quality software production. The structure used by version control systems is the same used by file systems, but in general the abstraction level made by software developers considers the file contents and its internal structure, including details as classes, methods, control blocks and others. Fine-grained version control tools can provide a more detailed version control. However traditional tools and models provide very low flexibility and present high cost and impact of deployment in software development environments. In this paper, there are presented a model and a tool which aim at providing support to fine-grained version control activities.},
	Acmid = {1456576},
	Address = {New York, NY, USA},
	Author = {Junqueira, Daniel C. and Bittar, Thiago J. and Fortes, Renata P. M.},
	Booktitle = {Proceedings of the 26th Annual ACM International Conference on Design of Communication},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:45:06 +0100},
	Doi = {10.1145/1456536.1456576},
	Isbn = {978-1-60558-083-8},
	Keywords = {software configuration management, version control},
	Location = {Lisbon, Portugal},
	Numpages = {8},
	Pages = {185--192},
	Publisher = {ACM},
	Series = {SIGDOC '08},
	Title = {A Fine-grained and Flexible Version Control for Software Artifacts},
	Url = {http://doi.acm.org/10.1145/1456536.1456576},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1456536.1456576},
	Bdsk-Url-2 = {https://doi.org/10.1145/1456536.1456576}}

@article{Challenger:2005:FAE:1064340.1064343,
	Abstract = {This article presents a publishing system for efficiently creating dynamic Web content. Complex Web pages are constructed from simpler fragments. Fragments may recursively embed other fragments. Relationships between Web pages and fragments are represented by object dependence graphs. We present algorithms for efficiently detecting and updating Web pages affected after one or more fragments change. We also present algorithms for publishing sets of Web pages consistently; different algorithms are used depending upon the consistency requirements.Our publishing system provides an easy method for Web site designers to specify and modify inclusion relationships among Web pages and fragments. Users can update content on multiple Web pages by modifying a template. The system then automatically updates all Web pages affected by the change. Our system accommodates both content that must be proofread before publication and is typically from humans as well as content that has to be published immediately and is typically from automated feeds.We discuss some of our experiences with real deployments of our system as well as its performance. We also quantitatively present characteristics of fragments used at a major deployment of our publishing system including fragment sizes, update frequencies, and inclusion relationships.},
	Acmid = {1064343},
	Address = {New York, NY, USA},
	Author = {Challenger, Jim and Dantzig, Paul and Iyengar, Arun and Witting, Karen},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:45:26 +0100},
	Doi = {10.1145/1064340.1064343},
	Issn = {1533-5399},
	Issue_Date = {May 2005},
	Journal = {ACM Trans. Internet Technol.},
	Keywords = {Caching, Web, Web performance, dynamic content, fragments, publishing},
	Number = {2},
	Numpages = {31},
	Pages = {359--389},
	Publisher = {ACM},
	Title = {A Fragment-based Approach for Efficiently Creating Dynamic Web Content},
	Url = {http://doi.acm.org/10.1145/1064340.1064343},
	Volume = {5},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1064340.1064343},
	Bdsk-Url-2 = {https://doi.org/10.1145/1064340.1064343}}

@inproceedings{Oyibo:2013:FIN:2536853.2536932,
	Abstract = {This paper proposes an extendable Native Mobile Multimedia Learning Application (NMMLA) Framework for instantiating mobile learning applications for various educational subjects and courses on the Android platform. The framework supports a scalable number of components, which include Learn, Evaluate, Simulate, Resources, Chat, e-Quiz etc. It is a one-page-setup and do-it-yourself library that will facilitate the development and deployment of NMMLAs. Thus, it supports four major types of multimedia learning content---images, Hypertext Markup Language (HTML), audio and video---aimed at meeting different learning preferences. Moreover, it supports active and face-toface collaborative learning, such as simulations, chatting, and application/content sharing via Bluetooth, email and on social networks. Above all, it offers key application features, such as theme, course and quiz menus; listview/tabview presentational modes; and Search and Help utilities. This work will benefit practitioners in the m-learning field by providing a content flow algorithm tree that will prevent reinventing the wheel.},
	Acmid = {2536932},
	Address = {New York, NY, USA},
	Articleno = {589},
	Author = {Oyibo, Kiemute and Hamada, Mohamed},
	Booktitle = {Proceedings of International Conference on Advances in Mobile Computing \&\#38; Multimedia},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:28:16 +0100},
	Doi = {10.1145/2536853.2536932},
	Isbn = {978-1-4503-2106-8},
	Keywords = {M-learning; learning framework; multimedia; Android platform},
	Location = {Vienna, Austria},
	Numpages = {5},
	Pages = {589:589--589:593},
	Publisher = {ACM},
	Series = {MoMM '13},
	Title = {A Framework for Instantiating Native Mobile Multimedia Learning Applications on Android Platform},
	Url = {http://doi.acm.org/10.1145/2536853.2536932},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2536853.2536932},
	Bdsk-Url-2 = {https://doi.org/10.1145/2536853.2536932}}

@inproceedings{Yu:2007:FRI:1242572.1242697,
	Abstract = {The development of user interfaces (UIs) is one of the most time-consuming aspects in software development. In this context, the lack of proper reuse mechanisms for UIs is increasingly becoming manifest, especially as software development is more and more moving toward composite applications. In this paper we propose a framework for the integration of stand-alone modules or applications, where integration occurs at the presentation layer. Hence, the final goal is to reduce the effort required for UI development by maximizing reus.
The design of the framework is inspired by lessons learned from application integration, appropriately modified to account for the specificity of the UI integration problem. We provide an abstract component model to specify characteristics and behaviors of presentation components and propose an event-based composition model to specify the composition logic. Components and composition are described by means of a simple XML-based language, which is interpreted by a runtime middleware for the execution of the resulting composite application. A proof-of-concept prototype allows us to show that the proposed component model can also easily be applied to existing presentation components, built with different languages and/or component technologies.},
	Acmid = {1242697},
	Address = {New York, NY, USA},
	Author = {Yu, Jin and Benatallah, Boualem and Saint-Paul, Regis and Casati, Fabio and Daniel, Florian and Matera, Maristella},
	Booktitle = {Proceedings of the 16th International Conference on World Wide Web},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:45:47 +0100},
	Doi = {10.1145/1242572.1242697},
	Isbn = {978-1-59593-654-7},
	Keywords = {XPIL, component model, presentation component, presentation composition, presentation integration, user interface (UI)},
	Location = {Banff, Alberta, Canada},
	Numpages = {10},
	Pages = {923--932},
	Publisher = {ACM},
	Series = {WWW '07},
	Title = {A Framework for Rapid Integration of Presentation Components},
	Url = {http://doi.acm.org/10.1145/1242572.1242697},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1242572.1242697},
	Bdsk-Url-2 = {https://doi.org/10.1145/1242572.1242697}}

@inproceedings{Matharu:2014:FLC:2677855.2677862,
	Abstract = {In India, Information and Communication technology (ICT) is being leveraged as a modernization tool in almost every sector of economy such as health, education, and transportation. But when we consider the agricultural scenario in the Indian context, we realise that the ICT remains to be exploited to accrue its invaluable benefits. In recent times, the Government of India has introduced several initiatives to promote the application of ICT in agriculture sector. But when we compare the scale of ICT application in Indian agriculture sector with other developing countries like China, Brazil, etc., we find that application of ICT in Indian agriculture is yet to be applied on a significant magnitude. In this paper, we propose a cloud deployment model "Agri-Bridge", which provides access to agricultural market related information to farmers facing market connectivity constraints and acute capital shortage. Also, this model will operate as a bridge between the farmers and consumers within the existing agricultural produce marketing chain. This model utilizes the existing Government services, Agricultural Produce Marketing Committee (APMC) databases, retail market sources besides leveraging cloud computing, mobile phone services and Internet services to provide a solution to the problem of lack of access to real-time market information to the farmers, hence modernising the Indian agricultural produce marketing system.},
	Acmid = {2677862},
	Address = {New York, NY, USA},
	Articleno = {7},
	Author = {Matharu, Gurpreet Singh and Mishra, Anju and Chhikara, Pallavi},
	Booktitle = {Proceedings of the 2014 International Conference on Information and Communication Technology for Competitive Strategies},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:46:12 +0100},
	Doi = {10.1145/2677855.2677862},
	Isbn = {978-1-4503-3216-3},
	Keywords = {AGRI-CLOUD, Agmarknet, Agri-Bridge, Cloud Deployment, ICT, e-Agriculture, e-Choupal},
	Location = {Udaipur, Rajasthan, India},
	Numpages = {7},
	Pages = {7:1--7:7},
	Publisher = {ACM},
	Series = {ICTCS '14},
	Title = {A Framework to Leverage Cloud for Modernization of Indian Agricultural Produce Marketing System},
	Url = {http://doi.acm.org/10.1145/2677855.2677862},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2677855.2677862},
	Bdsk-Url-2 = {https://doi.org/10.1145/2677855.2677862}}

@inproceedings{Nguyen:2006:SDP:1151736.1151740,
	Abstract = {Frequent itemsets mining is well explored for various data types, and its computational complexity is well understood. Based on our previous work by Nguyen and Orlowska (2005), this paper shows the extension of the data pre-processing approach to further improve the performance of frequent itemsets computation. The methods focus on potential reduction of the size of the input data required for deployment of the partitioning based algorithms.We have made a series of the data pre-processing methods such that the final step of the Partition algorithm, where a combination of all local candidate sets must be processed, is executed on substantially smaller input data. Moreover, we have made a comparison among these methods based on the experiments with particular data sets.},
	Acmid = {1151740},
	Address = {Darlinghurst, Australia, Australia},
	Author = {Nguyen, Son N. and Orlowska, Maria E.},
	Booktitle = {Proceedings of the 17th Australasian Database Conference - Volume 49},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:46:31 +0100},
	Isbn = {1-920682-31-7},
	Keywords = {algorithm, data mining, frequent itemset, partition, performance},
	Location = {Hobart, Australia},
	Numpages = {7},
	Pages = {31--37},
	Publisher = {Australian Computer Society, Inc.},
	Series = {ADC '06},
	Title = {A Further Study in the Data Partitioning Approach for Frequent Itemsets Mining},
	Url = {http://dl.acm.org/citation.cfm?id=1151736.1151740},
	Year = {2006},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1151736.1151740}}

@inproceedings{Sharma:2016:GAD:2896958.2896966,
	Abstract = {Large organizations need to be nimble in delivering software solutions for meeting rapidly changing business requirements and technology landscape. Following Agile principles of software development is a natural choice. However, to truly leverage the power of Agile, big organizations need to be able to utilize distributed teams effectively. Agile relies hugely on shared context and awareness among team members and this can become a stumbling block among such geographically dispersed teams. Moreover, in such large projects there is a need for incentivizing quick delivery of user stories so that the teams have a constructive sense of competition and are recognized in-process. Here, we describe a gamification based approach which promotes quicker completion and acceptance of user stories in such distributed Agile projects. Our approach captures important events from the development environment and then helps create project-wide awareness regarding the progress of different teams. A model of earning revenue for faster delivery of user stories is used to determine the leading team at the end of each sprint. This approach has been implemented in an Agile process guidance and awareness workbench that we are piloting within our organization.},
	Acmid = {2896966},
	Address = {New York, NY, USA},
	Author = {Sharma, Vibhu Saujanya and Kaulgud, Vikrant and Duraisamy, P.},
	Booktitle = {Proceedings of the 5th International Workshop on Games and Software Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:46:55 +0100},
	Doi = {10.1145/2896958.2896966},
	Isbn = {978-1-4503-4160-8},
	Keywords = {agile gamification, distributed agile, software delivery},
	Location = {Austin, Texas},
	Numpages = {4},
	Pages = {42--45},
	Publisher = {ACM},
	Series = {GAS '16},
	Title = {A Gamification Approach for Distributed Agile Delivery},
	Url = {http://doi.acm.org/10.1145/2896958.2896966},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2896958.2896966},
	Bdsk-Url-2 = {https://doi.org/10.1145/2896958.2896966}}

@inproceedings{Goettelmann:2013:GAT:2536146.2536164,
	Abstract = {It is recognized that the most important obstacle to the development of the cloud is the variety of new security threats which requests new methods and mechanisms. This is even truer for those who want to deploy business processes, because of the critical knowledge they encapsulate in terms of know-how and data. This paper proposes an approach combining modeling techniques and cloud selection for a trusted deployment of a security risk-aware business process in security constrained clouds.},
	Acmid = {2536164},
	Address = {New York, NY, USA},
	Author = {Goettelmann, Elio and Mayer, Nicolas and Godart, Claude},
	Booktitle = {Proceedings of the Fifth International Conference on Management of Emergent Digital EcoSystems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:56:28 +0100},
	Doi = {10.1145/2536146.2536164},
	Isbn = {978-1-4503-2004-7},
	Keywords = {business process, cloud, security},
	Location = {Luxembourg, Luxembourg},
	Numpages = {8},
	Pages = {92--99},
	Publisher = {ACM},
	Series = {MEDES '13},
	Title = {A General Approach for a Trusted Deployment of a Business Process in Clouds},
	Url = {http://doi.acm.org/10.1145/2536146.2536164},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2536146.2536164},
	Bdsk-Url-2 = {https://doi.org/10.1145/2536146.2536164}}

@article{Coulson:2008:GCM:1328671.1328672,
	Abstract = {Component-based software structuring principles are now commonplace at the application level; but componentization is far less established when it comes to building low-level systems software. Although there have been pioneering efforts in applying componentization to systems-building, these efforts have tended to target specific application domains (e.g., embedded systems, operating systems, communications systems, programmable networking environments, or middleware platforms). They also tend to be targeted at specific deployment environments (e.g., standard personal computer (PC) environments, network processors, or microcontrollers). The disadvantage of this narrow targeting is that it fails to maximize the genericity and abstraction potential of the component approach. In this article, we argue for the benefits and feasibility of a generic yet tailorable approach to component-based systems-building that offers a uniform programming model that is applicable in a wide range of systems-oriented target domains and deployment environments. The component model, called OpenCom, is supported by a reflective runtime architecture that is itself built from components. After describing OpenCom and evaluating its performance and overhead characteristics, we present and evaluate two case studies of systems we have built using OpenCom technology, thus illustrating its benefits and its general applicability.},
	Acmid = {1328672},
	Address = {New York, NY, USA},
	Articleno = {1},
	Author = {Coulson, Geoff and Blair, Gordon and Grace, Paul and Taiani, Francois and Joolia, Ackbar and Lee, Kevin and Ueyama, Jo and Sivaharan, Thirunavukkarasu},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:56:54 +0100},
	Doi = {10.1145/1328671.1328672},
	Issn = {0734-2071},
	Issue_Date = {February 2008},
	Journal = {ACM Trans. Comput. Syst.},
	Keywords = {Component-based software, computer systems implementation},
	Number = {1},
	Numpages = {42},
	Pages = {1:1-1:42},
	Publisher = {ACM},
	Title = {A Generic Component Model for Building Systems Software},
	Url = {http://doi.acm.org/10.1145/1328671.1328672},
	Volume = {26},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1328671.1328672},
	Bdsk-Url-2 = {https://doi.org/10.1145/1328671.1328672}}

@inproceedings{Veloudis:2015:GMC:2801948.2801988,
	Abstract = {With the pervasion of cloud computing the enterprise IT environment is progressively transformed into an ecosystem of highly distributed, task-oriented, modular, and collaborative cloud services. In order to deal effectively with the complexity inherent in such ecosystems, future enterprises are anticipated to increasingly rely on cloud service brokerage (CSB). This paper presents a CSB mechanism which offers capabilities with respect to the Quality Assurance dimension of CSB. The proposed mechanism evaluates the compliance of cloud services with pre-specified policies concerning the technical, and mainly the business aspects, of service deployment and delivery. By relying on a declarative representation of both services and policies, the proposed mechanism is kept generic and orthogonal to any underlying cloud delivery platform.},
	Acmid = {2801988},
	Address = {New York, NY, USA},
	Author = {Veloudis, Simeon and Paraskakis, Iraklis and Petsos, Christos},
	Booktitle = {Proceedings of the 19th Panhellenic Conference on Informatics},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:57:19 +0100},
	Doi = {10.1145/2801948.2801988},
	Isbn = {978-1-4503-3551-5},
	Keywords = {cloud computing, cloud service brokerage, linked USDL, policy-based governance, quality control, service description languages, service governance},
	Location = {Athens, Greece},
	Numpages = {6},
	Pages = {247--252},
	Publisher = {ACM},
	Series = {PCI '15},
	Title = {A Generic Mechanism for Cloud Service Governance and Quality Control},
	Url = {http://doi.acm.org/10.1145/2801948.2801988},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2801948.2801988},
	Bdsk-Url-2 = {https://doi.org/10.1145/2801948.2801988}}

@inproceedings{Zhong:2014:GJD:2676652.2683463,
	Abstract = {In Dynamic Adaptive Streaming over HTTP (DASH), the video bitrate is adapted to the network's condition based on client's feedback. Typically, a high bitrate is selected for next video chunk when the average of previously observed bandwidth samples is high and vice versa. An unexpected drop in the network throughput causes video freezing as the adaptation is done based on network's status in previous moment. In a vehicular environment, this problem is more likely to occur due to rapid movements of client to new locations where the network throughput can be very volatile. In this paper, we present a modified JavaScript DASH player that enables the streaming client to optimize the streaming performance by taking advantage of pre-collected bandwidth statistics in different locations. In this video player, Markov Decision Process (MDP) has been considered as the underlying optimization framework.},
	Acmid = {2683463},
	Address = {New York, NY, USA},
	Author = {Zhong, Garson and Bokani, Ayub},
	Booktitle = {Proceedings of the 2014 Workshop on Design, Quality and Deployment of Adaptive Video Streaming},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:57:37 +0100},
	Doi = {10.1145/2676652.2683463},
	Isbn = {978-1-4503-3281-1},
	Keywords = {adaptive video streaming, bandwidth map},
	Location = {Sydney, Australia},
	Numpages = {2},
	Pages = {39--40},
	Publisher = {ACM},
	Series = {VideoNext '14},
	Title = {A Geo-Adaptive JavaScript DASH Player},
	Url = {http://doi.acm.org/10.1145/2676652.2683463},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2676652.2683463},
	Bdsk-Url-2 = {https://doi.org/10.1145/2676652.2683463}}

@article{Morelli:2010:GCD:1859204.1859235,
	Abstract = {A firsthand account of an international team effort to install the Sahana disaster-management system in Chengdu, Sichuan after an earthquake.
On Monday May 12, 2008, an earthquake measuring 7.9 on the Richter scale struck in Sichuan Province in southwestern China, destroying homes, schools, hospitals, roads, and vital power and communication infrastructure. More than 45 million people were affected---tens of thousands were killed, hundreds of thousands injured, millions of people were evacuated and left homeless, and millions of buildings were destroyed.
When the earthquake hit, several members of what became an international, volunteer, disaster-management IT team were attending a workshop in Washington, D.C. The workshop was organized by the IBM Office of Corporate Citizenship and Corporate Affairs department to train IBM personnel and others in the use and deployment of Sahana, a free and open source software (FOSS) disaster management system.
Sahana, which means relief in Sinhalese, is a Web-based collaboration tool that helps manage information resources during a disaster recovery effort. It supports a wide range of relief efforts from finding missing persons, to managing volunteers, tracking resources, and coordinating refugee camps. Sahana enables government groups, non-governmental organizations (NGOs) and the victims themselves to work together during a disaster recovery effort.
Over the next several weeks, the team members, distributed among several cities (Beijing and Chengdu in China, Hartford and New York in the U.S., and Colombo in Sri Lanka), worked together over global communication channels to configure and deploy Sahana in Chengdu, in order to support the disaster recovery effort there.
The organizations involved in the collaboration included:
* The Lanka Software Foundation (LSF), developers of the Sahana system. Three LSF members, led by the second author, were conducting the training workshop.
* Various departments of IBM, including Business Continuity and Resiliency Services. Ten employees led by the fourth author, who organized the workshop, were receiving instruction in how to deploy and use Sahana.
* The Humanitarian FOSS Project (H-FOSS), an NSF-funded effort aimed at revitalizing undergraduate computing education. Four students and their mentors, the first and third authors, were attending the workshop as developers and undergraduate members of the Sahana community.
* IBM China. Initially, local teams in Beijing and Chengdu consisting of corporate citizenship, government relations, and technical professionals led in demonstrating Sahana to local officials, securing buy-in, and establishing channels to proceed. Then a large team of developers, language specialists, and others, including a team based in Chengdu, Sichuan, eventually took charge of the deployment effort in Chengdu. The fifth author was a member of the China development team.
Almost immediately after the earthquake, discussions were held between IBM, IBM China, China's Ministry of Civil Affairs, and the Chengdu city government in Sichuan province. Once the Chengdu government expressed real interest in deploying Sahana, a team was formed to begin the process of localizing Sahana---that is, translating its user interface into simplified Chinese. The team was led by executives and software developers from IBM-China and assisted by Sahana team members in Colombo and student H-FOSS volunteers in Hartford.
The team's organizational structure followed the normal procedure involved in previous Sahana deployments---a local group in close proximity to the incident supported by volunteers from the global Sahana community. In this case IBM-China, including some who were directly affected by the disaster, took the lead in deploying Sahana over an intensive three-week period.
The decision by the Chengdu government to proceed with the deployment was taken on May 21, 2008 and a revised and localized version of Sahana was deployed in Chengdu on May 25. On June 12 we learned that 42 families had been reunited with the help of Sahana.
This article provides an inside look at the deployment effort. It describes how a diverse, multidisciplinary team---professional programmers, software engineers, executives from a large global enterprise, students, faculty, and humanitarian IT specialists from a global FOSS community---worked together to assist the earthquake recovery effort. The success of the collaboration illustrates the power of virtual communities working across international boundaries using a variety of electronic communication software. It also demonstrates that the Internet has truly made us all neighbors and is constantly forcing us to redefine our concept of community.},
	Acmid = {1859235},
	Address = {New York, NY, USA},
	Author = {Morelli, Ralph and de Silva, Chamindra and de Lanerolle, Trishan and Curzon, Rebecca and Mao, Xin Sheng},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:25:22 +0100},
	Doi = {10.1145/1859204.1859235},
	Issn = {0001-0782},
	Issue_Date = {December 2010},
	Journal = {Commun. ACM},
	Number = {12},
	Numpages = {8},
	Pages = {142--149},
	Publisher = {ACM},
	Title = {A Global Collaboration to Deploy Help to China},
	Url = {http://doi.acm.org/10.1145/1859204.1859235},
	Volume = {53},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1859204.1859235},
	Bdsk-Url-2 = {https://doi.org/10.1145/1859204.1859235}}

@inproceedings{Baba:2013:GMF:2525314.2525461,
	Abstract = {The Radio Frequency Identification (RFID) emerges to be one of the key technologies to modernize object tracking and monitoring systems in indoor environments, e.g., airport baggage tracking. Although RFID has advantages over alternative identification technologies, the raw RFID data produced is inherently uncertain and contains errors. The dirty nature of raw RFID data hinders the progress of applying meaningful high-level applications that range from querying to analyzing. Therefore, cleansing RFID data is a high necessity. In this paper, we focus on handling one of the main aspects of raw RFID data, namely, false negatives, which occurs when a moving object passes the detection range of an RFID reader but the reader fails to produce any readings. We investigate the topology of indoor spaces as well as the deployment of RFID readers, and propose the transition probabilities that capture how likely objects move from one RFID reader to another. We organize such probabilities, together with the characteristics of indoor topology and RFID readers, into a probabilistic distance-aware graph model. Further, we evaluate the effectiveness and efficiency of devised graph model in recovering the false negatives using real dataset. The experimental results show that the devised graph model is effective and efficient in handling false negatives in indoor RFID tracking data.},
	Acmid = {2525461},
	Address = {New York, NY, USA},
	Author = {Baba, Asif Iqbal and Lu, Hua and Pedersen, Torben Bach and Xie, Xike},
	Booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:57:58 +0100},
	Doi = {10.1145/2525314.2525461},
	Isbn = {978-1-4503-2521-9},
	Keywords = {RFID, data cleansing, indoor spaces},
	Location = {Orlando, Florida},
	Numpages = {4},
	Pages = {464--467},
	Publisher = {ACM},
	Series = {SIGSPATIAL'13},
	Title = {A Graph Model for False Negative Handling in Indoor RFID Tracking Data},
	Url = {http://doi.acm.org/10.1145/2525314.2525461},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2525314.2525461},
	Bdsk-Url-2 = {https://doi.org/10.1145/2525314.2525461}}

@inproceedings{Loewen:2013:GIP:2498328.2500069,
	Abstract = {Management of cloud and cluster systems can be a daunting task, and becomes increasingly complicated as the organization grows and requires more computing resources. Organizations that have a need for a large amount of computing resources have two major deployment decisions: native cluster architecture or a local cloud based architecture. Regardless of the underlying architecture, the deployment process should remain universal. Many different deployment considerations are available. Automated deployment is desirable because typically these systems are headless, and thus difficult to manage individually. The tool, which we have developed allows users to deploy filesystem images on computer systems within the same local network. Additionally, the tool presents the user with node management tools for controlling the power state of the systems, as well as information such as IP addresses, MAC addresses, and power state.},
	Acmid = {2500069},
	Address = {New York, NY, USA},
	Articleno = {23},
	Author = {Loewen, Gabriel and Galloway, Michael and Vrbsky, Susan},
	Booktitle = {Proceedings of the 51st ACM Southeast Conference},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:58:25 +0100},
	Doi = {10.1145/2498328.2500069},
	Isbn = {978-1-4503-1901-0},
	Keywords = {ACM proceedings, latex, text tagging},
	Location = {Savannah, Georgia},
	Numpages = {6},
	Pages = {23:1--23:6},
	Publisher = {ACM},
	Series = {ACMSE '13},
	Title = {A Graphical Interface for Private Cloud and Cluster Management},
	Url = {http://doi.acm.org/10.1145/2498328.2500069},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2498328.2500069},
	Bdsk-Url-2 = {https://doi.org/10.1145/2498328.2500069}}

@inproceedings{Salvini:2011:HSC:2096123.2096139,
	Abstract = {The HiPerDNO project aims to develop new applications to enhance the operational capabilities of Distribution Network Operators (DNO). Their delivery requires an advanced computational strategy. This paper describes a High Performance Computing (HPC) platform developed for these applications whilst also being flexible enough to accommodate new ones emerging from the gradual introduction of smart metering in the Low Voltage (LV) networks (AMI: Advanced Metering Infrastructures). Security and reliability requirements for both data and computations are very stringent. Our proposed architecture would allow the deployment of computations and data access as services, thus achieving independence on the actual hardware and software technologies deployed, and hardening against malicious as well as accidental corruptions. Cost containment and reliance on proven technologies are also of paramount importance to DNOs. We suggest an architecture that fulfills these needs, which includes the following components for the HPC and Data Storage systems: Hadoop Distributed File System, a federation of loosely coupled computational clusters, the PELICAN computational application framework},
	Acmid = {2096139},
	Address = {New York, NY, USA},
	Author = {Salvini, Stefano and Lopatka, Piotr and Wallom, David},
	Booktitle = {Proceedings of the First International Workshop on High Performance Computing, Networking and Analytics for the Power Grid},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:58:44 +0100},
	Doi = {10.1145/2096123.2096139},
	Isbn = {978-1-4503-1061-1},
	Keywords = {high performance computing applications, smart grid, systems design},
	Location = {Seattle, Washington, USA},
	Numpages = {8},
	Pages = {75--82},
	Publisher = {ACM},
	Series = {HiPCNA-PG '11},
	Title = {A Hardware and Software Computational Platform for the HiPerDNO (High Performance Distribution Network Operation) Project},
	Url = {http://doi.acm.org/10.1145/2096123.2096139},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2096123.2096139},
	Bdsk-Url-2 = {https://doi.org/10.1145/2096123.2096139}}

@inproceedings{Tang:2015:HDF:2818869.2818898,
	Abstract = {The ubiquitous deployment of various kinds of sensors in smart cities requires a new computing paradigm to support Internet of Things (IoT) services and applications, and big data analysis. Fog Computing, which extends Cloud Computing to the edge of network, fits this need. In this paper, we present a hierarchical distributed Fog Computing architecture to support the integration of massive number of infrastructure components and services in future smart cities. To secure future communities, it is necessary to build large-scale, geospatial sensing networks, perform big data analysis, identify anomalous and hazardous events, and offer optimal responses in real-time. We analyze case studies using a smart pipeline monitoring system based on fiber optic sensors and sequential learning algorithms to detect events threatening pipeline safety. A working prototype was constructed to experimentally evaluate event detection performance of the recognition of 12 distinct events. These experimental results demonstrate the feasibility of the system's city-wide implementation in the future.},
	Acmid = {2818898},
	Address = {New York, NY, USA},
	Articleno = {28},
	Author = {Tang, Bo and Chen, Zhen and Hefferman, Gerald and Wei, Tao and He, Haibo and Yang, Qing},
	Booktitle = {Proceedings of the ASE BigData \&\#38; SocialInformatics 2015},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:59:02 +0100},
	Doi = {10.1145/2818869.2818898},
	Isbn = {978-1-4503-3735-9},
	Keywords = {Fog computing, big data analysis, distributed computing architecture, pipeline safety monitoring, smart city},
	Location = {Kaohsiung, Taiwan},
	Numpages = {6},
	Pages = {28:1--28:6},
	Publisher = {ACM},
	Series = {ASE BD\&\#38;SI '15},
	Title = {A Hierarchical Distributed Fog Computing Architecture for Big Data Analysis in Smart Cities},
	Url = {http://doi.acm.org/10.1145/2818869.2818898},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2818869.2818898},
	Bdsk-Url-2 = {https://doi.org/10.1145/2818869.2818898}}

@inproceedings{Duro:2013:HPS:2488551.2488598,
	Abstract = {This paper presents the design and implementation of a storage system for high performance systems based on a multiple level I/O caching architecture. The solution relies on Memcached as a parallel storage system, preserving its powerful capacities such as transparency, quick deployment, and scalability. The designed parallel storage system targets to reduce the I/O latency in data-intensive high performance applications. The proposed solution consists of a user-level library and extended Memcached servers. The solution aims to be hierarchical by deploying Memcached-based I/O servers across all the infrastructure data path. Our experiments demonstrate that our solution is up to 40% faster than PVFS2.},
	Acmid = {2488598},
	Address = {New York, NY, USA},
	Author = {Duro, Francisco Rodrigo and Blas, Javier Garcia and Carretero, Jesus},
	Booktitle = {Proceedings of the 20th European MPI Users' Group Meeting},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:59:20 +0100},
	Doi = {10.1145/2488551.2488598},
	Isbn = {978-1-4503-1903-4},
	Keywords = {distributed cache, memcached, parallel storage system},
	Location = {Madrid, Spain},
	Numpages = {2},
	Pages = {139--140},
	Publisher = {ACM},
	Series = {EuroMPI '13},
	Title = {A Hierarchical Parallel Storage System Based on Distributed Memory for Large Scale Systems},
	Url = {http://doi.acm.org/10.1145/2488551.2488598},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2488551.2488598},
	Bdsk-Url-2 = {https://doi.org/10.1145/2488551.2488598}}

@inproceedings{Antonic:2015:HTP:2675743.2772588,
	Abstract = {The ACM DEBS Grand Challenge 2015 focuses on real-time analytics over a high volume geospatial data stream composed of taxi trip reports from New York City. The goal of the challenge is to provide a solution which continuously identifies the most frequent routes (query 1) and most profitable areas (query 2) for taxis in New York City. The solution needs to process the incoming data stream in near real-time to provide valid information about taxi positions to end-users in a real-world deployment. We propose a modular processing engine design which is configured to offer efficient performance with a high data throughput and low processing latency. It consists of three main components: an input processor which pre-processes data objects to detect outliers, and two independent query processors tailored to the requirements of challenge queries. To efficiently compute query results, query processors use algorithms customized to the distribution of the taxi-generated data stream. Our experimental evaluation shows that the system can on average process 350,000 input events per second in a distributed mode, while achieving an average latency of less than 1 ms for both queries. Due to their excellent performance, the proposed algorithms are well suited for efficient tracking of a large number of vehicles that are present in modern urban areas.},
	Acmid = {2772588},
	Address = {New York, NY, USA},
	Author = {Antoni\'{c}, Aleksandar and Pripu\v{z}i\'{c}, Kre\v{s}imir and Marjanovi\'{c}, Martina and Sko\v{c}ir, Pavle and Je\v{z}i\'{c}, Gordan and \v{Z}arko, Ivana Podnar},
	Booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:59:44 +0100},
	Doi = {10.1145/2675743.2772588},
	Isbn = {978-1-4503-3286-6},
	Keywords = {ACM DEBS grand challenge, complex event processing, smart city, trafic monitoring},
	Location = {Oslo, Norway},
	Numpages = {7},
	Pages = {309--315},
	Publisher = {ACM},
	Series = {DEBS '15},
	Title = {A High Throughput Processing Engine for Taxi-generated Data Streams},
	Url = {http://doi.acm.org/10.1145/2675743.2772588},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2675743.2772588},
	Bdsk-Url-2 = {https://doi.org/10.1145/2675743.2772588}}

@inproceedings{Kertesz:2012:HSP:2666742.2666750,
	Abstract = {Cloud Computing builds on the latest achievements of diverse research areas, such as Grid Computing, Service-oriented computing, business process modeling and virtualization. As this new computing paradigm was mostly lead by companies, several proprietary systems arisen. Recently, alongside these commercial systems, several smaller-scale privately owned systems are maintained and developed. In this paper we present our research results performed within the S-Cube European FP7 NoE project to enable automated service provisioning for users on a highly dynamic infrastructure consisting of multiple Cloud providers. We developed a Federated Cloud Management architecture that provides unified access to a federated Cloud that aggregates multiple heterogeneous IaaS Cloud providers in a transparent manner. We have also incorporated an integrated monitoring approach that enables more reliable provider selection in these heterogeneous environments.},
	Acmid = {2666750},
	Address = {Piscataway, NJ, USA},
	Author = {Kertesz, Attila and Kecskemeti, Gabor and Nemeth, Zsolt and Oriol, Marc and Franch, Xavier},
	Booktitle = {Proceedings of the First International Workshop on European Software Services and Systems Research: Results and Challenges},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:00:04 +0100},
	Isbn = {978-1-4673-1807-5},
	Keywords = {cloud brokering, cloud computing, on-demand deployment, service monitoring},
	Location = {Zurich, Switzerland},
	Numpages = {2},
	Pages = {25--26},
	Publisher = {IEEE Press},
	Series = {S-Cube '12},
	Title = {A Holistic Service Provisioning Solution for Federated Cloud Infrastructures},
	Url = {http://dl.acm.org/citation.cfm?id=2666742.2666750},
	Year = {2012},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2666742.2666750}}

@inproceedings{Anish:2014:KFB:2593861.2593864,
	Abstract = {The disciplines of requirements engineering (RE) and software architecture (SA) are fundamental to the success of software projects. The synergistic relationship between these two disciplines has long been acknowledged by both academicians and practitioners alike. To build successful and cost-effective software systems, we must understand and leverage the linkages between functional and architectural requirements. We discuss a knowledge-assisted approach that establishes traceability between functional and architectural requirements. The approach classifies requirements into the problem context (functional) and a solution (architectural) context. The functional context is called Functional Requirement Viewpoint (FRV). The architectural context is further categorized into three sub-contexts namely the Functional Architecture Viewpoint (FAV), the Technical Architecture Viewpoint (TAV) and the Deployment Architecture Viewpoint (DAV). Though the approach separates the problem domain and the solution domain explicitly; it facilitates development of requirements and architectural specifications concurrently; appreciating the necessary interplay between the two.},
	Acmid = {2593864},
	Address = {New York, NY, USA},
	Author = {Anish, Preethu Rose and Balasubramaniam, Balaji},
	Booktitle = {Proceedings of the 4th International Workshop on Twin Peaks of Requirements and Architecture},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:00:29 +0100},
	Doi = {10.1145/2593861.2593864},
	Isbn = {978-1-4503-2848-7},
	Keywords = {Requirements Engineering, Software Engineering, Software and Systems Architecture, Twin Peaks model},
	Location = {Hyderabad, India},
	Numpages = {4},
	Pages = {14--17},
	Publisher = {ACM},
	Series = {TwinPeaks 2014},
	Title = {A Knowledge-assisted Framework to Bridge Functional and Architecturally Significant Requirements},
	Url = {http://doi.acm.org/10.1145/2593861.2593864},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2593861.2593864},
	Bdsk-Url-2 = {https://doi.org/10.1145/2593861.2593864}}

@inproceedings{Hall:2004:LRA:1067343.1067383,
	Abstract = {The growth of the mobile gaming market offers considerable potential for the deployment of engaging and compelling games constructed using AI components and techniques. This paper discusses a rule-based approach for constructing lightweight Game AI systems for deployment on mobile devices. The development environment and the mimosa programming language for constructing Game AI components are outlined. A prototype game of Texas Hold'em Poker implemented using this environment is described. Ideas for future work, including the development of games mentors for deployment on mobile devices are briefly presented.},
	Acmid = {1067383},
	Address = {New York, NY, USA},
	Author = {Hall, Lynne and Gordon, Adrian and James, Russell and Newall, Lynne},
	Booktitle = {Proceedings of the 2004 ACM SIGCHI International Conference on Advances in Computer Entertainment Technology},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:00:45 +0100},
	Doi = {10.1145/1067343.1067383},
	Isbn = {1-58113-882-2},
	Keywords = {game AI, mobile gaming, rule-based languages, tools},
	Location = {Singapore},
	Numpages = {6},
	Pages = {284--289},
	Publisher = {ACM},
	Series = {ACE '04},
	Title = {A Lightweight Rule-based Al Engine for Mobile Games},
	Url = {http://doi.acm.org/10.1145/1067343.1067383},
	Year = {2004},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1067343.1067383},
	Bdsk-Url-2 = {https://doi.org/10.1145/1067343.1067383}}

@inproceedings{Rice:2010:LMB:1878431.1878447,
	Abstract = {We present a model targeted at practical, wide-scale deployment which produces an ongoing breakdown of building energy consumption. We argue that wide-scale deployment is practical due to its reliance only on commonly available sensor information and crowd-sourced inventory data. The results for our own building over the previous 10 months show many of the trends seen in the building's true, me-tered energy consumption and we find our model predicts long term averages within 10% of the true value in some scenarios. We further use our model to estimate the potential impact of some energy saving scenarios.},
	Acmid = {1878447},
	Address = {New York, NY, USA},
	Author = {Rice, Andrew and Hay, Simon and Ryder-Cook, Dan},
	Booktitle = {Proceedings of the 2Nd ACM Workshop on Embedded Sensing Systems for Energy-Efficiency in Building},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:01:05 +0100},
	Doi = {10.1145/1878431.1878447},
	Isbn = {978-1-4503-0458-0},
	Keywords = {personal energy meter},
	Location = {Zurich, Switzerland},
	Numpages = {6},
	Pages = {67--72},
	Publisher = {ACM},
	Series = {BuildSys '10},
	Title = {A Limited-data Model of Building Energy Consumption},
	Url = {http://doi.acm.org/10.1145/1878431.1878447},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1878431.1878447},
	Bdsk-Url-2 = {https://doi.org/10.1145/1878431.1878447}}

@article{Kim:2012:LSV:2379799.2379807,
	Abstract = {We present a long-term and cross-sectional study of a vibration-based water flow rate monitoring system in practical environments and scenarios. In our earlier research, we proved that a water flow monitoring system with vibration sensors is feasible by deploying and evaluating it in a small-scale laboratory setting. To validate the proposed system, the system was deployed in existing environments---two houses and a public restroom---and in two different laboratory test settings. With the collected data, we first demonstrate various aspects of the system's performance, including sensing stability, sensor node lifetime, the stability of autonomous sensor calibration, time to adaptation, and deployment complexity. We then discuss the practical challenges and lessons from the full-scale deployments. The evaluation results show that our water monitoring solution is a practical, quick-to-deploy system with a less than 5% average flow estimation error.},
	Acmid = {2379807},
	Address = {New York, NY, USA},
	Articleno = {8},
	Author = {Kim, Younghun and Park, Heemin and Srivastava, Mani B.},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:01:26 +0100},
	Doi = {10.1145/2379799.2379807},
	Issn = {1550-4859},
	Issue_Date = {November 2012},
	Journal = {ACM Trans. Sen. Netw.},
	Keywords = {Application of sensor networks, adaptive sensor calibration, nonintrusive and spatially distributed sensing, parameter estimation via numerical optimization},
	Number = {1},
	Numpages = {28},
	Pages = {8:1--8:28},
	Publisher = {ACM},
	Title = {A Longitudinal Study of Vibration-based Water Flow Sensing},
	Url = {http://doi.acm.org/10.1145/2379799.2379807},
	Volume = {9},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2379799.2379807},
	Bdsk-Url-2 = {https://doi.org/10.1145/2379799.2379807}}

@inproceedings{McGregor:2010:MAS:1842752.1842773,
	Abstract = {The ecosystem for a software product line includes all of the entities with which the software product line organization interacts. Information, artifacts, customers, money and products move among these entities as a part of the planning, development, and deployment processes. In this paper we present an analysis technique that uses the economic notion of a transaction to examine the transfers between the entities. The result of the analysis is data that is used to evaluate and structure the organization. We illustrate with an example.},
	Acmid = {1842773},
	Address = {New York, NY, USA},
	Author = {McGregor, John D.},
	Booktitle = {Proceedings of the Fourth European Conference on Software Architecture: Companion Volume},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:01:47 +0100},
	Doi = {10.1145/1842752.1842773},
	Isbn = {978-1-4503-0179-4},
	Keywords = {software ecosystem, software product line},
	Location = {Copenhagen, Denmark},
	Numpages = {8},
	Pages = {73--80},
	Publisher = {ACM},
	Series = {ECSA '10},
	Title = {A Method for Analyzing Software Product Line Ecosystems},
	Url = {http://doi.acm.org/10.1145/1842752.1842773},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1842752.1842773},
	Bdsk-Url-2 = {https://doi.org/10.1145/1842752.1842773}}

@inproceedings{Kim:2009:MSM:1655925.1655942,
	Abstract = {In this paper, we propose a method to support handover between heterogeneous access technologies to multiple interface (MIF) mobile node (MN) in the Proxy Mobile IPv6 (PMIPv6) domain. PMIPv6 enables network-based mobility for a regular IPv6 mobile (MIPv6) with no mobility management protocol. But, there have been some issues identified with supporting a host with multiple interfaces attaching to the PMIPv6 domain. So we propose the method using NAT (Network Address Translation) in MAG (Mobile Access Gateway) which can continuously delivery packets although handover between heterogeneous access technologies occur. This paper analyzes the reduction of handover signaling cost in the proposed MAT (MAG Address Translation) method. It compares the handover signaling cost with PMIPv6 assisted by IEEE 802.21(MIH) and with the proposed method (i.e. PMIPv6-MAT) to show how much handover signaling cost reduction can be achieved.},
	Acmid = {1655942},
	Address = {New York, NY, USA},
	Author = {Kim, Hyun-Jong and Kim, Jeong Yun and Choi, Seong-Gon},
	Booktitle = {Proceedings of the 2Nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:02:23 +0100},
	Doi = {10.1145/1655925.1655942},
	Isbn = {978-1-60558-710-3},
	Keywords = {PMIPv6, handover, mobility, multiple interfaces, signaling cost},
	Location = {Seoul, Korea},
	Numpages = {6},
	Pages = {91--96},
	Publisher = {ACM},
	Series = {ICIS '09},
	Title = {A Method to Support Multiple Interfaces Mobile Nodes in PMIPv6 Domain},
	Url = {http://doi.acm.org/10.1145/1655925.1655942},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1655925.1655942},
	Bdsk-Url-2 = {https://doi.org/10.1145/1655925.1655942}}

@inproceedings{Aberer:2006:MFF:1182635.1164243,
	Abstract = {A key problem in current sensor network technology is the heterogeneity of the available software and hardware platforms which makes deployment and application development a tedious and time consuming task. To minimize the unnecessary and repetitive implementation of identical functionalities for different platforms, we present our Global Sensor Networks (GSN) middleware which supports the flexible integration and discovery of sensor networks and sensor data, enables fast deployment and addition of new platforms, provides distributed querying, filtering, and combination of sensor data, and supports the dynamic adaption of the system configuration during operation. GSN's central concept is the virtual sensor abstraction which enables the user to declaratively specify XML-based deployment descriptors in combination with the possibility to integrate sensor network data through plain SQL queries over local and remote sensor data sources. In this demonstration, we specifically focus on the deployment aspects and allow users to dynamically reconfigure the running system, to add new sensor networks on the fly, and to monitor the effects of the changes via a graphical interface. The GSN implementation is available from http://globalsn.sourceforge.net/.},
	Acmid = {1164243},
	Author = {Aberer, Karl and Hauswirth, Manfred and Salehi, Ali},
	Booktitle = {Proceedings of the 32Nd International Conference on Very Large Data Bases},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:23:43 +0100},
	Location = {Seoul, Korea},
	Numpages = {4},
	Pages = {1199--1202},
	Publisher = {VLDB Endowment},
	Series = {VLDB '06},
	Title = {A Middleware for Fast and Flexible Sensor Network Deployment},
	Url = {http://dl.acm.org/citation.cfm?id=1182635.1164243},
	Year = {2006},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1182635.1164243}}

@inproceedings{Furtado:2014:MRW:2677017.2677026,
	Abstract = {Web service composition is a commonly used solution to build distributed systems on the cloud. Choreographies are one specific kind of service composition in which the responsibilities for the execution of the system are shared by its service components without a central point of coordination. Due to the distributed nature of these systems, a manual approach to resource usage monitoring and allocation to maintain the expected Quality of Service (QoS) is not only inefficient but also does not scale. In this paper, we present an open source choreography enactment middleware that is capable of automatically deploying and executing a composition. Additionally, it also monitors the composition execution to perform automatic resource provisioning and dynamic service reconfiguration based on pre-defined Service Level Agreement (SLA) constraints. To achieve that, it keeps a meta-level representation of the compositions, which contains their specifications, deployment statuses, and QoS attributes. Application developers can write specific rules that take into account these meta-data to reason about the performance of the composition and change its behavior. Our middleware was evaluated on Amazon EC2 and our results demonstrate that, with little effort from the choreography developer or deployer, the middleware is able to maintain the established SLA using both horizontal and vertical scaling when faced with varying levels of load. Additionally, it also reduces operational costs by using as little computational resources as possible.},
	Acmid = {2677026},
	Address = {New York, NY, USA},
	Articleno = {9},
	Author = {Furtado, Thiago and Francesquini, Emilio and Lago, Nelson and Kon, Fabio},
	Booktitle = {Proceedings of the 13th Workshop on Adaptive and Reflective Middleware},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:03:00 +0100},
	Doi = {10.1145/2677017.2677026},
	Isbn = {978-1-4503-3232-3},
	Keywords = {QoS, SOA, middleware, reflection},
	Location = {Bordeaux, France},
	Numpages = {6},
	Pages = {9:1--9:6},
	Publisher = {ACM},
	Series = {ARM '14},
	Title = {A Middleware for Reflective Web Service Choreographies on the Cloud},
	Url = {http://doi.acm.org/10.1145/2677017.2677026},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2677017.2677026},
	Bdsk-Url-2 = {https://doi.org/10.1145/2677017.2677026}}

@inproceedings{Gomes:2014:MAQ:2684080.2684082,
	Abstract = {Non-functional requirements specification for an application and definition of required resources for its deployment are activities commonly performed at ad hoc, which can cause quality of service degradation due to inefficient use of resources. This problem is even greater with applications formed by the composition of multiple services. Cloud computing emerges as an alternative to provision of the needed resources, which is usually done using local infrastructure in conjunction with external providers. However, efficient resource management mechanisms and policies for offering quality services in these environments are necessary. This work is a PhD proposal, where is presented a middleware-based approach for cloud resources management and their provisioning according with non-funcional requirements and with greater usability by the deployer. Use of models at run-time and choreography of services for higher abstraction, adaptability and decentralization of the running system is proposed. Using this approach applications running in cloud environments may work based on their immediate needs.},
	Acmid = {2684082},
	Address = {New York, NY, USA},
	Articleno = {2},
	Author = {Gomes, Raphael and Costa, F\'{a}bio and da Rocha, Ricardo and Georgantas, Nikolaos},
	Booktitle = {Proceedings of the 11th Middleware Doctoral Symposium},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:03:23 +0100},
	Doi = {10.1145/2684080.2684082},
	Isbn = {978-1-4503-3221-7},
	Keywords = {choreography, cloud computing, models@runtime, non-functional requirement},
	Location = {Bordeaux, France},
	Numpages = {4},
	Pages = {2:1--2:4},
	Publisher = {ACM},
	Series = {MDS '14},
	Title = {A Middleware-based Approach for QoS-aware Deployment of Service Choreography in the Cloud},
	Url = {http://doi.acm.org/10.1145/2684080.2684082},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2684080.2684082},
	Bdsk-Url-2 = {https://doi.org/10.1145/2684080.2684082}}

@article{Bellavista:2006:MCM:1183463.1183465,
	Abstract = {The widespread diffusion of mobile computing calls for novel services capable of providing results that depend on both the current physical position of users (location) and the logical set of accessible resources, subscribed services, preferences, and requirements (context). Leaving the burden of location/context management to applications complicates service design and development. In addition, traditional middleware solutions tend to hide location/context visibility to the application level and are not suitable for supporting novel adaptive services for mobile computing scenarios. The article proposes a flexible middleware for the development and deployment of location/context-aware services for heterogeneous data access in the Internet. A primary design choice is to exploit a high-level policy framework to simplify the specification of services that the middleware dynamically adapts to the client location/context. In addition, the middleware adopts the mobile agent technology to effectively support autonomous, asynchronous, and local access to data resources, and is particularly suitable for temporarily disconnected clients. The article also presents the case study of a museum guide assistant service that provides visitors with location/context-dependent artistic data. The case study points out the flexibility and usability of the proposed middleware that permits automatic service reconfiguration with no impact on the implementation of the application logic.},
	Acmid = {1183465},
	Address = {New York, NY, USA},
	Author = {Bellavista, Paolo and Corradi, Antonio and Montanari, Rebecca and Stefanelli, Cesare},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:03:45 +0100},
	Doi = {10.1145/1183463.1183465},
	Issn = {1533-5399},
	Issue_Date = {November 2006},
	Journal = {ACM Trans. Internet Technol.},
	Keywords = {Mobile computing, adaptive services, context awareness, location awareness, middleware, mobile agents, policies},
	Number = {4},
	Numpages = {25},
	Pages = {356--380},
	Publisher = {ACM},
	Title = {A Mobile Computing Middleware for Location- and Context-aware Internet Data Services},
	Url = {http://doi.acm.org/10.1145/1183463.1183465},
	Volume = {6},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1183463.1183465},
	Bdsk-Url-2 = {https://doi.org/10.1145/1183463.1183465}}

@inproceedings{Hu:2013:MCS:2541603.2541604,
	Abstract = {This paper presents TripleS, a novel mobile crowdsensing system enhanced by social networking services, which enables mobile users to participate and perform mobile crowdsensing tasks in an efficient manner. TripleS provides a flexible and universal architecture across mobile devices and cloud computing platforms by integrating the service-oriented architecture with multi-agent frameworks for mobile crowdsensing, with extensive supports to application developers and end users. The customized platform of TripleS enables dynamic deployments and collaborations of services and tasks during run-time of mobile devices. Our practical experiments show that TripleS performs its tasks with a considerable computation efficiency, and low computation and communication overhead on mobile devices. Also, the mobile crowdsensing application developed on TripleS demonstrates the functionalities and practical usage of TripleS.},
	Acmid = {2541604},
	Address = {New York, NY, USA},
	Articleno = {3},
	Author = {Hu, Xiping and Liu, Qiang and Zhu, Chunsheng and Leung, Victor C. M. and Chu, Terry H. S. and Chan, Henry C. B.},
	Booktitle = {Proceedings of the First International Workshop on Middleware for Cloud-enabled Sensing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:04:09 +0100},
	Doi = {10.1145/2541603.2541604},
	Isbn = {978-1-4503-2554-7},
	Keywords = {cloud, mobile crowdsensing, social networks, system architecture},
	Location = {Beijing, China},
	Numpages = {6},
	Pages = {3:1--3:6},
	Publisher = {ACM},
	Series = {MCS '13},
	Title = {A Mobile Crowdsensing System Enhanced by Cloud-based Social Networking Services},
	Url = {http://doi.acm.org/10.1145/2541603.2541604},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2541603.2541604},
	Bdsk-Url-2 = {https://doi.org/10.1145/2541603.2541604}}

@inproceedings{Tang:2009:MVC:1518701.1519012,
	Abstract = {Hospital work coordination and collaboration often requires mobility for acquiring proper information and resources. In turn, the spatial distribution and the mobility of clinicians can curtail the opportunities for effective communications making collaboration difficult. In this situation, a mobile hands-free voice communication system, Vocera, was introduced to enhance communication. It supports quick and impromptu conversations among coworkers for work coordination and collaboration anytime and anywhere. We study this deployment and present our findings concerning the impact of this communication system on the information flow. Our information flow framework's communication strategies help contrast the information processes before and after the deployment of Vocera.},
	Acmid = {1519012},
	Address = {New York, NY, USA},
	Author = {Tang, Charlotte and Carpendale, Sheelagh},
	Booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:04:29 +0100},
	Doi = {10.1145/1518701.1519012},
	Isbn = {978-1-60558-246-7},
	Keywords = {communication strategy, healthcare, information flow, mobile, observational study, vocera, voice communication},
	Location = {Boston, MA, USA},
	Numpages = {10},
	Pages = {2041--2050},
	Publisher = {ACM},
	Series = {CHI '09},
	Title = {A Mobile Voice Communication System in Medical Setting: Love It or Hate It?},
	Url = {http://doi.acm.org/10.1145/1518701.1519012},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1518701.1519012},
	Bdsk-Url-2 = {https://doi.org/10.1145/1518701.1519012}}

@inproceedings{Alipour:2016:MDM:2993274.3011285,
	Abstract = {Vendor lock-in is the issues in auto-scaling configuration; scaling configuration of a service cannot automatically transfer when the service is migrated from one cloud to another cloud. To facilitate fast service deployment, there is a need to automate the operations of auto-scaling configuration and deployment.},
	Acmid = {3011285},
	Address = {New York, NY, USA},
	Author = {Alipour, Hanieh and Liu, Yan},
	Booktitle = {Proceedings of the 4th International Workshop on Release Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:20:41 +0100},
	Doi = {10.1145/2993274.3011285},
	Isbn = {978-1-4503-4399-2},
	Keywords = {Auto-scaling, Cloud Computing, DevOps, Model-driven},
	Location = {Seattle, WA, USA},
	Numpages = {1},
	Pages = {23--23},
	Publisher = {ACM},
	Series = {RELENG 2016},
	Title = {A Model Driven Method to Deploy Auto-scaling Configuration for Cloud Services},
	Url = {http://doi.acm.org/10.1145/2993274.3011285},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2993274.3011285},
	Bdsk-Url-2 = {https://doi.org/10.1145/2993274.3011285}}

@inproceedings{GoncalvesJunior:2015:MAA:2695664.2695923,
	Abstract = {Cloud computing is a recent computing paradigm that is changing software engineering. It offers scalable virtual compute resources at low prices, thus attracting many software developers interested in reducing their infrastructure and operational costs. Even though using cloud solutions is simple, with many providers and resource types available, a common difficulty developers face is how to best configure their applications using a myriad of cloud services, specially when considering different attributes such as cost, scalability, performance and others. A wrong architectural decision can lead to a significant cost increase or a deployment option that does not meet the minimum required performance. This work presents an approach that relies on non-functional requirements as key drivers for assessing and selecting, based on a multi-criteria optimization method, the best architectural options for deploying applications in the cloud. Results from a real application (WordPress) deployed in a popular cloud provider (Amazon) are discussed to illustrate the use and benefits of the approach.},
	Acmid = {2695923},
	Address = {New York, NY, USA},
	Author = {Gon\c{c}alves Junior, Ronaldo and Rolim, Tiago and Sampaio, Am{\'e}rico and Mendon\c{c}a, Nabor C.},
	Booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:21:00 +0100},
	Doi = {10.1145/2695664.2695923},
	Isbn = {978-1-4503-3196-8},
	Keywords = {AHP, architectural analysis, cloud deployment, multi-criteria},
	Location = {Salamanca, Spain},
	Numpages = {7},
	Pages = {1383--1389},
	Publisher = {ACM},
	Series = {SAC '15},
	Title = {A Multi-criteria Approach for Assessing Cloud Deployment Options Based on Non-functional Requirements},
	Url = {http://doi.acm.org/10.1145/2695664.2695923},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2695664.2695923},
	Bdsk-Url-2 = {https://doi.org/10.1145/2695664.2695923}}

@inproceedings{Ashraf:2015:MAA:3233397.3233452,
	Abstract = {In this paper, we present a novel Multi-Objective Ant Colony System algorithm to optimize Cost, Performance, and Reliability (MOACS-CoPeR) in the cloud. The proposed algorithm provides a metaheuristic-based approach for the multi-objective cloud-based software component deployment problem. MOACS-CoPeR explores the search-space of architecture design alternatives with respect to several architectural degrees of freedom and produces a set of Pareto-optimal deployment configurations. We also present a Java-based implementation of our proposed algorithm and compare its results with the Non-dominated Sorting Genetic Algorithm II (NSGA-II). We evaluate the two algorithms against a cloud-based storage service, which is loosely based on a real system.},
	Acmid = {3233452},
	Address = {Piscataway, NJ, USA},
	Author = {Ashraf, Adnan and Byholm, Benjamin and Porres, Ivan},
	Booktitle = {Proceedings of the 8th International Conference on Utility and Cloud Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:22:13 +0100},
	Isbn = {978-0-7695-5697-0},
	Location = {Limassol, Cyprus},
	Numpages = {7},
	Pages = {341--347},
	Publisher = {IEEE Press},
	Series = {UCC '15},
	Title = {A Multi-objective ACS Algorithm to Optimize Cost, Performance, and Reliability in the Cloud},
	Url = {http://dl.acm.org/citation.cfm?id=3233397.3233452},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3233397.3233452}}

@inproceedings{Chilana:2013:MFS:2470654.2470685,
	Abstract = {We present a multi-site field study to evaluate LemonAid, a crowdsourced contextual help approach that allows users to retrieve relevant questions and answers by making selections within the interface. We deployed LemonAid on 4 different web sites used by thousands of users and collected data over several weeks, gathering over 1,200 usage logs, 168 exit surveys, and 36 one-on-one interviews. Our results indicate that over 70% of users found LemonAid to be helpful, intuitive, and desirable for reuse. Software teams found LemonAid easy to integrate with their sites and found the analytics data aggregated by LemonAid a novel way of learning about users' popular questions. Our work provides the first holistic picture of the adoption and use of a crowdsourced contextual help system and offers several insights into the social and organizational dimensions of implementing such help systems for real-world applications.},
	Acmid = {2470685},
	Address = {New York, NY, USA},
	Author = {Chilana, Parmit K. and Ko, Andrew J. and Wobbrock, Jacob O. and Grossman, Tovi},
	Booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:21:27 +0100},
	Doi = {10.1145/2470654.2470685},
	Isbn = {978-1-4503-1899-0},
	Keywords = {contextual help, crowdsourced help, field deployments, field studies, help systems, software support=.},
	Location = {Paris, France},
	Numpages = {10},
	Pages = {217--226},
	Publisher = {ACM},
	Series = {CHI '13},
	Title = {A Multi-site Field Study of Crowdsourced Contextual Help: Usage and Perspectives of End Users and Software Teams},
	Url = {http://doi.acm.org/10.1145/2470654.2470685},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2470654.2470685},
	Bdsk-Url-2 = {https://doi.org/10.1145/2470654.2470685}}

@inproceedings{Sun:2014:NMS:2619239.2626298,
	Abstract = {We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants.},
	Acmid = {2626298},
	Address = {New York, NY, USA},
	Author = {Sun, Peng and Mahajan, Ratul and Rexford, Jennifer and Yuan, Lihua and Zhang, Ming and Arefin, Ahsan},
	Booktitle = {Proceedings of the 2014 ACM Conference on SIGCOMM},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:21:46 +0100},
	Doi = {10.1145/2619239.2626298},
	Isbn = {978-1-4503-2836-4},
	Keywords = {datacenter network, network state, software-defined networking},
	Location = {Chicago, Illinois, USA},
	Numpages = {12},
	Pages = {563--574},
	Publisher = {ACM},
	Series = {SIGCOMM '14},
	Title = {A Network-state Management Service},
	Url = {http://doi.acm.org/10.1145/2619239.2626298},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2619239.2626298},
	Bdsk-Url-2 = {https://doi.org/10.1145/2619239.2626298}}

@article{Sun:2014:NMS:2740070.2626298,
	Abstract = {We present Statesman, a network-state management service that allows multiple network management applications to operate independently, while maintaining network-wide safety and performance invariants. Network state captures various aspects of the network such as which links are alive and how switches are forwarding traffic. Statesman uses three views of the network state. In observed state, it maintains an up-to-date view of the actual network state. Applications read this state and propose state changes based on their individual goals. Using a model of dependencies among state variables, Statesman merges these proposed states into a target state that is guaranteed to maintain the safety and performance invariants. It then updates the network to the target state. Statesman has been deployed in ten Microsoft Azure datacenters for several months, and three distinct applications have been built on it. We use the experience from this deployment to demonstrate how Statesman enables each application to meet its goals, while maintaining network-wide invariants.},
	Acmid = {2626298},
	Address = {New York, NY, USA},
	Author = {Sun, Peng and Mahajan, Ratul and Rexford, Jennifer and Yuan, Lihua and Zhang, Ming and Arefin, Ahsan},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:22:04 +0100},
	Doi = {10.1145/2740070.2626298},
	Issn = {0146-4833},
	Issue_Date = {October 2014},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {datacenter network, network state, software-defined networking},
	Number = {4},
	Numpages = {12},
	Pages = {563--574},
	Publisher = {ACM},
	Title = {A Network-state Management Service},
	Url = {http://doi.acm.org/10.1145/2740070.2626298},
	Volume = {44},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2740070.2626298},
	Bdsk-Url-2 = {https://doi.org/10.1145/2740070.2626298}}

@article{Meixner:2012:NPD:2180796.2180812,
	Abstract = {In many areas of human life, computer-based Information Technology (IT) has prevailed and become essential for the coordinated and efficient organization of workflows. Especially in the field of health care, interaction between human beings and IT is a sensitive subject. Inevitably, people working in the field of health care will have to make use of the potentials of IT in order to meet the enormous demands on e.g. patient management. When we look at the situation of current medical software systems, we can find major advances in the performance capacities of modern software systems, but must also note their rapid penetration into almost every facet of the daily hospital routine. Today medical software systems can be characterized as a vast number of (networked) software which each fulfills specific functions (e.g., hospital information system, Picture Archiving and Communication System) and which is used by different user groups (e.g., physicians, nurses, medical secretaries) with diverse user tasks [4]. Yet, for almost two decades, graphical user interfaces have dominated the interaction of medical software systems in most cases. In the future, a broader range of paradigms will emerge, allowing for multi-modal interaction. But also the growing number of heterogeneous platforms (operating systems, graphical libraries) and devices utilized complementarily (e.g., PC, Toughbook, Smartphone, PDA) demand for the development of congeneric user interfaces for a plethora of target platforms. Especially "mobile healthcare computing devices (MHCDs) are rapidly becoming an integral part of hospital information systems. Deployment of these devices is becoming an important IT strategy designed to assist in improving quality of care, enhancing patient services, increasing productivity, lowering costs, improving cash flow, as well as facilitating other critical delivery processes" [3]. For MHCDs aspects like getting the right information at the right time in the right place (context sensitivity) is important.
To be able to cope with requirements like interoperability, context-sensitivity and device-independent usage a model-based approach for the development of user interfaces (MBUID) appears to be favourable [2].},
	Acmid = {2180812},
	Address = {New York, NY, USA},
	Author = {Meixner, Gerrit and Zuehlke, Detlef},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:20:57 +0100},
	Doi = {10.1145/2180796.2180812},
	Issn = {2158-8813},
	Issue_Date = {March 2012},
	Journal = {SIGHIT Rec.},
	Number = {1},
	Numpages = {1},
	Pages = {20--20},
	Publisher = {ACM},
	Title = {A New Paradigm for the Development of Future Medical Software Systems},
	Url = {http://doi.acm.org/10.1145/2180796.2180812},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2180796.2180812},
	Bdsk-Url-2 = {https://doi.org/10.1145/2180796.2180812}}

@inproceedings{Das:2009:NGS:1655925.1656146,
	Abstract = {This paper has focused on the integration of renewable energy, specifically the solar energy resources into conventional electric grid and deployment of smart architecture of hybrid energy system in the user-centric pervasive computing concept in the context of Kyoto Protocol for sustainable development of the rural and urban sector. A concept of next generation mobile smart-grid city for efficient real-time collaborative use of renewable and non-renewable energy sources at smart user-centric device for sustainable green environment in the context of climate change is proposed},
	Acmid = {1656146},
	Address = {New York, NY, USA},
	Author = {Das, Aurobi and Balakrishnan, V.},
	Booktitle = {Proceedings of the 2Nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:22:22 +0100},
	Doi = {10.1145/1655925.1656146},
	Isbn = {978-1-60558-710-3},
	Keywords = {Kyoto protocol, NASA surface meteorology and solar energy data on solar energy resources, SAP-SOA energy optimization model, acceptance index, energy service companies, enterprise SAP-SOA net weaver architecture, enterprise resource planning, green house gas, market potential mappings, next generation smart-grid through pervasive computing, service-oriented-architecture, systems applications products in data processing},
	Location = {Seoul, Korea},
	Numpages = {7},
	Pages = {1212--1218},
	Publisher = {ACM},
	Series = {ICIS '09},
	Title = {A Next Generation Smart Energy Technology},
	Url = {http://doi.acm.org/10.1145/1655925.1656146},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1655925.1656146},
	Bdsk-Url-2 = {https://doi.org/10.1145/1655925.1656146}}

@inproceedings{Evensen:2011:PCC:2002259.2002301,
	Abstract = {The current approach used to obtain official television channel statistics is based on polls combined with specialized reporting hardware. These are deployed only on a small scale and batch processed every 24~hours. With the enhanced capabilities of present-day IPTV set-top-boxes, network operators can track channel popularity and usage patterns with a degree of precision and sophistication not possible with existing methods. One such network operator, Altibox, is the largest provider of IPTV in Norway with a deployment of over 320,000 set top-boxes. By tapping into the high-volume stream of channel zap events sent from these set-top boxes, very accurate viewership can be obtained and presented in near real-time.
In this paper, we examine two programming paradigms for implementing applications to compute viewership based on channel zap events. One based on a general-purpose programming language (Java) and the other based on a highly specialized event stream processing language (EPL). An important characteristic of this application is stateful event processing. We are interested in exploring the trade-offs between these two implementations, to determine their suitability for such applications. Specifically, we are interested in the performance trade-off and the program complexity of each implementation.
Our results show that a pure Java implementation has a significant edge over EPL in terms of performance. Although, our numbers cannot be used to draw a general conclusion, it seems indicative that an event stream processing engine would suffer more than a general-purpose language as query complexity grows. We conjecture that this is because it is easier to construct custom data structures for the specific problem in a general-purpose language like Java. In terms of program complexity, EPL has a slight edge in all metrics, and a significant edge when event streams can be reused to perform more complex processing, indicating that less effort is necessary to extend functionality.},
	Acmid = {2002301},
	Address = {New York, NY, USA},
	Author = {Evensen, P{\aa}l and Meling, Hein},
	Booktitle = {Proceedings of the 5th ACM International Conference on Distributed Event-based System},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:04:54 +0100},
	Doi = {10.1145/2002259.2002301},
	Isbn = {978-1-4503-0423-8},
	Keywords = {stream processing, tv viewership statistics},
	Location = {New York, New York, USA},
	Numpages = {10},
	Pages = {317--326},
	Publisher = {ACM},
	Series = {DEBS '11},
	Title = {A Paradigm Comparison for Collecting TV Channel Statistics from High-volume Channel Zap Events},
	Url = {http://doi.acm.org/10.1145/2002259.2002301},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2002259.2002301},
	Bdsk-Url-2 = {https://doi.org/10.1145/2002259.2002301}}

@inproceedings{Fernandez:2015:PNF:2855321.2855369,
	Abstract = {Cloud computing has brought a large variety of services available to potential consumers. A recent type of services are the provision of network functions using virtualization. Network Functions Virtualization (NFV) is a network architecture where network node functions such as load balancers, firewalls, IDS, and accelerators are built in software and offered as services. This approach results in reduced complexity in network design, better scalability and agility, as well as faster deployment. We present here a pattern for the NFV architecture.},
	Acmid = {2855369},
	Address = {New York, NY, USA},
	Articleno = {47},
	Author = {Fernandez, Eduardo B. and Hamid, Brahim},
	Booktitle = {Proceedings of the 20th European Conference on Pattern Languages of Programs},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:22:44 +0100},
	Doi = {10.1145/2855321.2855369},
	Isbn = {978-1-4503-3847-9},
	Keywords = {architecture patterns, cloud computing, network functions, security patterns, telecommunications, virtualization},
	Location = {Kaufbeuren, Germany},
	Numpages = {9},
	Pages = {47:1--47:9},
	Publisher = {ACM},
	Series = {EuroPLoP '15},
	Title = {A Pattern for Network Functions Virtualization},
	Url = {http://doi.acm.org/10.1145/2855321.2855369},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2855321.2855369},
	Bdsk-Url-2 = {https://doi.org/10.1145/2855321.2855369}}

@inproceedings{Rodin:2011:PLR:2578903.2579147,
	Abstract = {A release is a collection of authorized software changes that include new functionality, changed functionality, or both, that are introduced into the production environment. Agile software development results in many small releases delivered as needed, as opposed to big-bang releases that deploy large amounts of functionality at regularly determined intervals. Agile software development without a detailed standard process model encompassing the overall release management activity can result in an emergent release schedule (which is almost as bad as not having one), potential bottlenecks for resources, and problems handling genuine emergency releases. Changes to software systems must occur in a disciplined and controlled way. Otherwise lack of control over the delivered changes will lead to deteriorated system quality. Change Management is often in conflict with Agile in that Change Management requires a big, heavy, schedule, dates, content, and deployment resources specified well in advance. This is a major attraction of big-bang releases; that their deployments seem to be easier to manage. In a fast changing Agile environment the survival of the software organization may be affected. There is no reason to abandon Agile development for the sake of a disciplined and controlled release process. Our proposed pattern language captures the expertise necessary to create an orderly, well thought out, end-to-end release management process that should help deliver software releases in a disciplined and controlled way within an Agile development environment.},
	Acmid = {2579147},
	Address = {New York, NY, USA},
	Articleno = {9},
	Author = {Rodin, Rick and Leet, Jon and Azua, Maria and Bygrave, Dwight},
	Booktitle = {Proceedings of the 18th Conference on Pattern Languages of Programs},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:23:04 +0100},
	Doi = {10.1145/2578903.2579147},
	Isbn = {978-1-4503-1283-7},
	Keywords = {compliance, intellectual property, patterns, release management, software maintenance, software risk},
	Location = {Portland, Oregon, USA},
	Numpages = {13},
	Pages = {9:1--9:13},
	Publisher = {ACM},
	Series = {PLoP '11},
	Title = {A Pattern Language for Release and Deployment Management},
	Url = {http://doi.acm.org/10.1145/2578903.2579147},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2578903.2579147},
	Bdsk-Url-2 = {https://doi.org/10.1145/2578903.2579147}}

@inproceedings{Lee:2010:PSC:1851476.1851542,
	Abstract = {Cloud computing has the potential for tremendous benefits, but wide scale adoption has a range of challenges that must be met. We review these challenges and how they relate to scientific computing. To achieve the portability, interoperability, and economies of scale that clouds offer, it is clear that common design principles must be widely adopted in both the user community and marketplace. To this end, we argue that a private-to-public cloud deployment trajectory will be very common, if not dominant. This trajectory can be used to define a progression of needed common practices and standards which, in turn, can be used to define deployment, development and fundamental research agendas. We then survey the cloud standards landscape and how the standards process could be driven by major stakeholders, e.g., large user groups, vendors, and governments, to achieve scientific and national objectives. We conclude with a call to action for stakeholders to actively engage in driving this process to a successful conclusion.},
	Acmid = {1851542},
	Address = {New York, NY, USA},
	Author = {Lee, Craig A.},
	Booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:05:13 +0100},
	Doi = {10.1145/1851476.1851542},
	Isbn = {978-1-60558-942-8},
	Keywords = {cloud computing, deployment trajectory, standardization},
	Location = {Chicago, Illinois},
	Numpages = {9},
	Pages = {451--459},
	Publisher = {ACM},
	Series = {HPDC '10},
	Title = {A Perspective on Scientific Cloud Computing},
	Url = {http://doi.acm.org/10.1145/1851476.1851542},
	Year = {2010},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1851476.1851542},
	Bdsk-Url-2 = {https://doi.org/10.1145/1851476.1851542}}

@inproceedings{Huang:2007:PNS:1244002.1244359,
	Abstract = {Many Web service standards of orchestration and choreography are designed to reduce their inherent complexity of composing Web services. Existing standards all remain at the descriptive level, without providing any formal semantics and method for verifying important properties. Web Service Choreography Interface (WSCI) describes the flow of messages exchanged by a Web service which participates in choreographed interactions with other services. Due to many advantages of Petri nets, an extended one is used to formalize WSCI in this paper. We show several nets to represent the activity, process and interface respectively. Our formal model remarkably focuses on the WSCI concept of message exchange and the context which describes the environment. This paper proposes some properties and introduces technique for checking them to ensure its correct deployment.},
	Acmid = {1244359},
	Address = {New York, NY, USA},
	Author = {Huang, Yu and Wang, Hanpin},
	Booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:25:10 +0100},
	Doi = {10.1145/1244002.1244359},
	Isbn = {1-59593-480-4},
	Keywords = {WSCI, formal semantics, petri net, web service},
	Location = {Seoul, Korea},
	Numpages = {2},
	Pages = {1689--1690},
	Publisher = {ACM},
	Series = {SAC '07},
	Title = {A Petri Net Semantics for Web Service Choreography},
	Url = {http://doi.acm.org/10.1145/1244002.1244359},
	Year = {2007},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1244002.1244359},
	Bdsk-Url-2 = {https://doi.org/10.1145/1244002.1244359}}

@inproceedings{Dawoud:2009:PSV:1655925.1656111,
	Abstract = {It is well established now within the vehicular Communication (VC) community the fact that security and protection of privacy are a prerequisite for the deployment of the technology. It is also understood that without the integration of strong and practical security and privacy enhancing mechanisms, VC systems could be disrupted or disabled, even by relatively unsophisticated attackers.
The paper introduces the widely accepted solution known as the "pseudonymous authentication" approach and discusses the limitations of some of the schemes proposed. The paper introduces a new proposal that helps to solve some of these limitations.},
	Acmid = {1656111},
	Address = {New York, NY, USA},
	Author = {Dawoud, Peter D. and Dawoud, D. S. and Peplow, R.},
	Booktitle = {Proceedings of the 2Nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:05:42 +0100},
	Doi = {10.1145/1655925.1656111},
	Isbn = {978-1-60558-710-3},
	Keywords = {authentication, digital signatures, privacy, pseudonyms, vehicular communication (VC)},
	Location = {Seoul, Korea},
	Numpages = {7},
	Pages = {1026--1032},
	Publisher = {ACM},
	Series = {ICIS '09},
	Title = {A Proposal for Secure Vehicular Communications},
	Url = {http://doi.acm.org/10.1145/1655925.1656111},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1655925.1656111},
	Bdsk-Url-2 = {https://doi.org/10.1145/1655925.1656111}}

@inproceedings{C.:2012:PDL:2261605.2261632,
	Abstract = {This article presents a proposal for Learning Object Design problem-based and well-defined features in the design of the problem, graphical interface design, the pattern of software architecture, the self-assessment process and structure of metadata are geared to the learning style and deployment platforms defined in non-functional requirements.
The stage engineering design software Learning Object (LO), is an extension of the analysis model and requires the support of teachers with extensive knowledge of the issues associated with interactive content. In this design model specifies the system to reach the final program and here we propose the use of techniques in the life cycle for software design, appropriate to the design of Learning Objects to the proposed integrated engineering software Learning Objects based on known problems ISDOA.},
	Acmid = {2261632},
	Address = {New York, NY, USA},
	Author = {C., Carlos A. Castro and M., Edgar Serna and B., Gabriel E. Taborda},
	Booktitle = {Proceedings of the 6th Euro American Conference on Telematics and Information Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:25:35 +0100},
	Doi = {10.1145/2261605.2261632},
	Isbn = {978-1-4503-1012-3},
	Keywords = {learning objects, software design, software engineering, software life cycle},
	Location = {Valencia, Spain},
	Numpages = {7},
	Pages = {181--187},
	Publisher = {ACM},
	Series = {EATIS '12},
	Title = {A Proposed Design of Learning Objects},
	Url = {http://doi.acm.org/10.1145/2261605.2261632},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2261605.2261632},
	Bdsk-Url-2 = {https://doi.org/10.1145/2261605.2261632}}

@article{Singh:2012:PMD:2382756.2382771,
	Abstract = {Data Warehouse (DW) systems maintain sensitive and crucial information, which is integrated from various heterogenous sources of organization. With the ever increasing deployment and usage of networks, these systems are becoming more vulnerable to malicious attacks. With the increased number of attacks, intrusion detection has become vital part of Information Security. In this paper, we have proposed a model for analyzing and detecting anomalous events based on user behavior analysis through usage patterns, user profiles and session management. After monitoring the events in the system, if any intrusion activity occurs, then alerts are issued to system administrators. Since a user profile is not necessarily fixed but rather it evolves with changing time, so a dynamic user behavior modeling is represented as a sequence of events and combination of fact and dimension tables accessed by the users. In this way, DW systems may be protected by the malicious attacks.},
	Acmid = {2382771},
	Address = {New York, NY, USA},
	Author = {Singh, Indu and Kumar, Manoj},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:07:54 +0100},
	Doi = {10.1145/2382756.2382771},
	Issn = {0163-5948},
	Issue_Date = {November 2012},
	Journal = {SIGSOFT Softw. Eng. Notes},
	Keywords = {data warehouse, information security, intrusion detection system, user behaviour},
	Number = {6},
	Numpages = {7},
	Pages = {1--7},
	Publisher = {ACM},
	Title = {A Proposed Model for Data Warehouse User Behaviour Using Intrusion Detection System},
	Url = {http://doi.acm.org/10.1145/2382756.2382771},
	Volume = {37},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2382756.2382771},
	Bdsk-Url-2 = {https://doi.org/10.1145/2382756.2382771}}

@inproceedings{Bonelli:2014:PFA:2658260.2658269,
	Abstract = {Today's rapidly evolving network ecosystem, characterized by increasing traffic volumes, service heterogeneity and mutating cyber-threats, calls for new approaches to packet processing to address key issues such as scalability, flexibility, programmability and fast deployment. To this aim, this paper explores a new direction to packet processing by pushing forward functional programming principles in the definition of a ''software defined networking'' paradigm.
This result is achieved by introducing PFQ-Lang, an extensible functional language which can be used to process, analyze and forward packets captured on modern multi-queue NICs (for example, it allows to quickly develop the early stage of monitoring applications). An implementation of PFQ-Lang, embedded into high level programming languages as an eDSL (embedded Domain Specific Language) is also presented. The proposed approach allows an easy development by leveraging the intuitive functional composition and, at the same time, allows to exploit multi-queue NICs and multi-core architectures to process high-speed network traffic. Experimental results are provided to prove that the presented implementation reaches line rate performance on a 10Gb line card. To demonstrate the effectiveness and expressiveness of PFQ-Lang, the paper also presents a few use-cases ranging from forwarding, firewalling and monitoring of real traffic.},
	Acmid = {2658269},
	Address = {New York, NY, USA},
	Author = {Bonelli, Nicola and Giordano, Stefano and Procissi, Gregorio and Abeni, Luca},
	Booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:26:04 +0100},
	Doi = {10.1145/2658260.2658269},
	Isbn = {978-1-4503-2839-5},
	Keywords = {pfq, software defined networking},
	Location = {Los Angeles, California, USA},
	Numpages = {12},
	Pages = {219--230},
	Publisher = {ACM},
	Series = {ANCS '14},
	Title = {A Purely Functional Approach to Packet Processing},
	Url = {http://doi.acm.org/10.1145/2658260.2658269},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2658260.2658269},
	Bdsk-Url-2 = {https://doi.org/10.1145/2658260.2658269}}

@article{Cheluvaraju:2012:QMP:2237796.2237801,
	Abstract = {Several techniques have been developed to identify and fix defects in software before its deployment. However, the challenge is to quantify how well these techniques prevent defects from occurring in the field from a holistic perspective. Therefore, we propose a novel software quality metric called "The Preventability Metric" that measures the preventability of defects in software. The metric is derived from a composite quantitative evaluation of the efficiency and effectiveness of the individual preventive techniques employed on software before its deployment. It provides a confidence on how well prevention of defects is handled before deployment.},
	Acmid = {2237801},
	Address = {New York, NY, USA},
	Author = {Cheluvaraju, Bharath and Pasala, Anjaneyulu and Padmanabhuni, Srinivas and Chevireddy, Sadhana},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:26:21 +0100},
	Doi = {10.1145/2237796.2237801},
	Issn = {0163-5948},
	Issue_Date = {July 2012},
	Journal = {SIGSOFT Softw. Eng. Notes},
	Keywords = {preventive maintenance software metrics, quantitative techniques, software engineering, software maintenance},
	Number = {4},
	Numpages = {5},
	Pages = {1--5},
	Publisher = {ACM},
	Title = {A Quantitative Measure for Preventive Maintenance in Software},
	Url = {http://doi.acm.org/10.1145/2237796.2237801},
	Volume = {37},
	Year = {2012},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2237796.2237801},
	Bdsk-Url-2 = {https://doi.org/10.1145/2237796.2237801}}

@inproceedings{Kazakos:2016:RIP:2858036.2858585,
	Abstract = {Interactive Voice Response (IVR) platforms have been widely deployed in resource-limited settings. These systems tend to afford asynchronous push interactions, and within the context of health, provide medication reminders, descriptions of symptoms and tips on self-management. Here, we present the development of an IVR system for resource-limited settings that enables real-time, synchronous interaction. Inspired by community radio, and calls for health systems that are truly local, we developed "Sehat ki Vaani". Sehat ki Vaani is a real-time IVR platform that enables hosting and participation in radio chat shows on community-led topics. We deployed Sehat ki Vaani with two communities in North India on topics related to the management of Type 2 diabetes and maternal health. Our deployments highlight the potential for synchronous IVR systems to offer community connection and localised sharing of experience, while also highlighting the complexity of producing, hosting and participating in radio shows in real time through IVR. We discuss the relative strengths and weaknesses of synchronous IVR systems, and highlight lessons learnt for interaction design in this area.},
	Acmid = {2858585},
	Address = {New York, NY, USA},
	Author = {Kazakos, Konstantinos and Asthana, Siddhartha and Balaam, Madeline and Duggal, Mona and Holden, Amey and Jamir, Limalemla and Kannuri, Nanda Kishore and Kumar, Saurabh and Manindla, Amarendar Reddy and Manikam, Subhashini Arcot and Murthy, GVS and Nahar, Papreen and Phillimore, Peter and Sathyanath, Shreyaswi and Singh, Pushpendra and Singh, Meenu and Wright, Pete and Yadav, Deepika and Olivier, Patrick},
	Booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:06:14 +0100},
	Doi = {10.1145/2858036.2858585},
	Isbn = {978-1-4503-3362-7},
	Keywords = {community, design, hci4d, ictd, india, ivr, mhealth, resource-limited settings, user experience},
	Location = {San Jose, California, USA},
	Numpages = {12},
	Pages = {343--354},
	Publisher = {ACM},
	Series = {CHI '16},
	Title = {A Real-Time IVR Platform for Community Radio},
	Url = {http://doi.acm.org/10.1145/2858036.2858585},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2858036.2858585},
	Bdsk-Url-2 = {https://doi.org/10.1145/2858036.2858585}}

@inproceedings{Chauhan:2014:RAP:2578128.2588485,
	Abstract = {Global Software Development (GSD) teams encounter challenges that are associated with distribution of software development activities across multiple geographic regions. The limited support for performing collaborative development and engineering activities and lack of sufficient support for maintaining and resolving dependencies and traceability across heterogeneous tools are major challenges for GSD teams. The lack of insufficient support for cross platform tools integration also makes it hard to address the stated challenges using existing paradigms that are based upon desktop and web-based solutions. The restricted ability of the organizations to have desired alignment of tools with software engineering and development processes results in administrative and managerial overhead that incur increased development cost and poor product quality. Moreover, stakeholders involved in the projects have specific constraints regarding availability and deployments of the tools. The artifacts and data produced or consumed by the tools need to be governed according to the constraints and corresponding quality of service (QoS) parameters. In this paper, we present the research agenda to leverage cloud-computing paradigm for addressing above-mentioned issues by providing a framework to select appropriate tools as well as associated services and reference architecture of the cloud-enabled middleware platform that allows on demand provisioning of software engineering Tools as a Service (TaaS) with focus on integration of tools.},
	Acmid = {2588485},
	Address = {New York, NY, USA},
	Articleno = {16},
	Author = {Chauhan, Muhammad Aufeef},
	Booktitle = {Proceedings of the WICSA 2014 Companion Volume},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:26:42 +0100},
	Doi = {10.1145/2578128.2588485},
	Isbn = {978-1-4503-2523-3},
	Keywords = {cloud computing, global software development (GSD), infrastructure as a service (IaaS), software as a service (SaaS), software engineering (SE), tools as a service (TaaS)},
	Location = {Sydney, Australia},
	Numpages = {6},
	Pages = {16:1--16:6},
	Publisher = {ACM},
	Series = {WICSA '14 Companion},
	Title = {A Reference Architecture for Providing Tools As a Service to Support Global Software Development},
	Url = {http://doi.acm.org/10.1145/2578128.2588485},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2578128.2588485},
	Bdsk-Url-2 = {https://doi.org/10.1145/2578128.2588485}}

@inproceedings{Chen:2015:SML:2774993.2775008,
	Abstract = {Cloud today is evolving towards multi-datacenter deployment, with each datacenter serving customers in different geographical areas. The independence between datacenters, however, prohibits effective inter-datacenter resource sharing and flexible management of the infrastructure. In this paper, we propose WL2, a Software-Defined Networking (SDN) solution to an Internet-scale Layer-2 network across multiple datacenters. In WL2, a logically centralized controller handles control-plane communication and configuration in each datacenter. We achieve scalability in three ways: (1) eliminating Layer-2 broadcast by rerouting control-plane traffic to the controller; (2) introducing a layered addressing scheme for aggregate Layer-2 routing; and (3) creating an overlay abstraction on top of physical topology for fast flow setup. WL2 is fault-tolerant against controller and gateway failures. We deployed and evaluated WL2 in a 2,250-VM testbed across three datacenters. The results indicate high performance and robustness of the system.},
	Acmid = {2775008},
	Address = {New York, NY, USA},
	Articleno = {8},
	Author = {Chen, Chen and Liu, Changbin and Liu, Pingkai and Loo, Boon Thau and Ding, Ling},
	Booktitle = {Proceedings of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:06:34 +0100},
	Doi = {10.1145/2774993.2775008},
	Isbn = {978-1-4503-3451-8},
	Keywords = {fault-tolerance, layer-2 networking, multiple datacenters, scalability, software-defined networking},
	Location = {Santa Clara, California},
	Numpages = {12},
	Pages = {8:1--8:12},
	Publisher = {ACM},
	Series = {SOSR '15},
	Title = {A Scalable Multi-datacenter Layer-2 Network Architecture},
	Url = {http://doi.acm.org/10.1145/2774993.2775008},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2774993.2775008},
	Bdsk-Url-2 = {https://doi.org/10.1145/2774993.2775008}}

@inproceedings{Krcal:2015:SFE:2835185.2835190,
	Abstract = {Current research in climate informatics focuses mainly on the development of novel (machine learning, data mining, or statistical) techniques to analyze climate data (e.g. model, in-situ, or satellite) or to make prediction based on these climate data. One important component missing from this analysis workflow is data management that allows efficient and flexible data retrieval, (ease of) reproducibility, and the (ease of) techniques reuse on user-defined data subsets or other data.
In this paper, we describe our preliminary investigation on the utilization of the distributed array-based database management system, SciDB, to support data-driven climate science research. We focus on modeling and generating indices that allow effective execution of various spatiotemporal queries on satellite data. Moreover, we demonstrate fast and accurate data retrieval based on user-specified trajectories from the SciDB database containing tropical cyclone trajectories and the complete ten-year QuikSCAT ocean surface wind fields satellite data.
Our preliminary work indicates the feasibility of the array-based technology for multiple satellite data storage, query, and analysis. Towards this end, a successful deployment of SciDB-based data storage can facilitate the use of data from multiple satellites for climate and weather research.},
	Acmid = {2835190},
	Address = {New York, NY, USA},
	Author = {Kr\v{c}\'{a}l, Lubo\v{s} and Ho, Shen-Shyang},
	Booktitle = {Proceedings of the 4th International ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:27:07 +0100},
	Doi = {10.1145/2835185.2835190},
	Isbn = {978-1-4503-3974-2},
	Keywords = {QuikSCAT, SciDB, arrays, index, indexing, multidimensional arrays, satellite data, scientific database, spatiotemporal},
	Location = {Bellevue, WA, USA},
	Numpages = {8},
	Pages = {7--14},
	Publisher = {ACM},
	Series = {BigSpatial'15},
	Title = {A SciDB-based Framework for Efficient Satellite Data Storage and Query Based on Dynamic Atmospheric Event Trajectory},
	Url = {http://doi.acm.org/10.1145/2835185.2835190},
	Year = {2015},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2835185.2835190},
	Bdsk-Url-2 = {https://doi.org/10.1145/2835185.2835190}}

@inproceedings{Liu:2010:SAA:1862303.1862307,
	Abstract = {This paper proposes a security architecture for the basic cross indexing systems emerging as foundational structures in current health information systems. In these systems unique identifiers are issued to healthcare providers and consumers. In most cases, such numbering schemes are national in scope and must therefore necessarily be used via an indexing system to identify records contained in pre-existing local, regional or national health information systems. Most large scale electronic health record systems envisage that such correlation between national healthcare identifiers and pre-existing identifiers will be performed by some centrally administered cross referencing, or index system. This paper is concerned with the security architecture for such indexing servers and the manner in which they interface with pre-existing health systems (including both workstations and servers). The paper proposes two required structures to achieve the goal of a national scale, and secure exchange of electronic health information, including: (a) the employment of high trust computer systems to perform an indexing function, and (b) the development and deployment of an appropriate high trust interface module, a Healthcare Interface Processor (HIP), to be integrated into the connected workstations or servers of healthcare service providers. This proposed architecture is specifically oriented toward requirements identified in the Connectivity Architecture for Australia's e-health scheme as outlined by NEHTA and the national e-health strategy released by the Australian Health Ministers.},
	Acmid = {1862307},
	Address = {Darlinghurst, Australia, Australia},
	Author = {Liu, Vicky and Caelli, William and Smith, Jason and May, Lauren and Lee, Min Hui and Ng, Zi Hao and Foo, Jin Hong and Li, Weihao},
	Booktitle = {Proceedings of the Fourth Australasian Workshop on Health Informatics and Knowledge Management - Volume 108},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:06:56 +0100},
	Isbn = {978-1-920682-89-7},
	Keywords = {HL7, architecture of health information systems, health informatics, indexing based system for e-health regime, network security for health systems, security for health information systems, trusted system},
	Location = {Brisbane, Australia},
	Numpages = {10},
	Pages = {7--16},
	Publisher = {Australian Computer Society, Inc.},
	Series = {HIKM '10},
	Title = {A Secure Architecture for Australia's Index Based e-Health Environment},
	Url = {http://dl.acm.org/citation.cfm?id=1862303.1862307},
	Year = {2010},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1862303.1862307}}

@inproceedings{kumar:2009:SUS:1644993.1645006,
	Abstract = {The creation of wireless sensor networks provides a capable solution for a range of ubiquitous information services. Its challenges like, data security and secrecy due to its hostile deployment nature of being flat to physical attacks with restricted source available. In order to competition the security pressure that sensor networks are exposed to, a cryptography algorithm is implemented at sensor nodes for node-to-node encryption, between nodes considering the data redundancy, energy constraint, and security requirement. In this paper, we analyze Dragon stream cipher and propose a new D-MAC secure data scheme which supports node-to-node encryption using Dragon based-privacy [3] for sensor networks. Our procedure regarded the entity verification and message authentication through the performance of authenticated encryption scheme in wireless sensor nodes.},
	Acmid = {1645006},
	Address = {New York, NY, USA},
	Author = {kumar, Pardeep and Lee, Hoon Jae and Chung, Wan-Young},
	Booktitle = {Proceedings of the 2009 International Conference on Hybrid Information Technology},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:27:26 +0100},
	Doi = {10.1145/1644993.1645006},
	Isbn = {978-1-60558-662-5},
	Keywords = {Dragon stream cipher, Telos B sensor mote, authenticated encryption, eSTREAM's project, sensor network security},
	Location = {Daejeon, Korea},
	Numpages = {4},
	Pages = {71--74},
	Publisher = {ACM},
	Series = {ICHIT '09},
	Title = {A Secure Ubiquitous Sensor Network with Dragon},
	Url = {http://doi.acm.org/10.1145/1644993.1645006},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1644993.1645006},
	Bdsk-Url-2 = {https://doi.org/10.1145/1644993.1645006}}

@inproceedings{Rodes:2014:SMB:2593868.2593880,
	Abstract = {Software security metrics that facilitate decision making at the enterprise design and operations levels are a topic of active research and debate. These metrics are desirable to support deployment decisions, upgrade decisions, and so on; however, no single metric or set of metrics is known to provide universally effective and appropriate measurements. Instead, engineers must choose, for each software system, what to measure, how and how much to measure, and must be able to justify the rationale for how these measurements are mapped to stakeholder security goals. An assurance argument for security (i.e., a security argument) provides comprehensive documentation of all evidence and rationales for justifying belief in a security claim about a software system. In this work, we motivate the need for security arguments to facilitate meaningful and comprehensive security metrics, and present a novel framework for assessing security arguments to generate and interpret security metrics.},
	Acmid = {2593880},
	Address = {New York, NY, USA},
	Author = {Rodes, Benjamin D. and Knight, John C. and Wasson, Kimberly S.},
	Booktitle = {Proceedings of the 5th International Workshop on Emerging Trends in Software Metrics},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:07:13 +0100},
	Doi = {10.1145/2593868.2593880},
	Isbn = {978-1-4503-2854-8},
	Keywords = {Assurance Case, Confidence, Security Metrics},
	Location = {Hyderabad, India},
	Numpages = {7},
	Pages = {66--72},
	Publisher = {ACM},
	Series = {WETSoM 2014},
	Title = {A Security Metric Based on Security Arguments},
	Url = {http://doi.acm.org/10.1145/2593868.2593880},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2593868.2593880},
	Bdsk-Url-2 = {https://doi.org/10.1145/2593868.2593880}}

@inproceedings{Bastide:2008:SSC:1387269.1387299,
	Abstract = {The creation of applications able to be executed in ubiquitous environments, involves a better consideration of the execution context in order to ensure service continuity. In component-based software engineering, applications are built by assembling existing components. For deploying such applications in ubiquitous environments, its components must be able to adapt themselves to the current context. To deal with this issue, we propose in this paper an approach aiming at reconfiguring the component structure to allow a flexible deployment of its services according to its use context. This adaptation focusing on the service continuity, consists of determining a structure adapted to the execution context. Then, the current structure is automatically reconfigured and the generated components are redeployed.},
	Acmid = {1387299},
	Address = {New York, NY, USA},
	Author = {Bastide, Gautier and Seriai, Abdelhak and Oussalah, Mourad},
	Booktitle = {Proceedings of the 5th International Conference on Pervasive Services},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:27:51 +0100},
	Doi = {10.1145/1387269.1387299},
	Isbn = {978-1-60558-135-4},
	Keywords = {clustering, context-awareness, deployment, restructuring, self-adaptation, software component, ubiquitous systems},
	Location = {Sorrento, Italy},
	Numpages = {4},
	Pages = {173--176},
	Publisher = {ACM},
	Series = {ICPS '08},
	Title = {A Self-adaptation of Software Component Structures in Ubiquitous Environments},
	Url = {http://doi.acm.org/10.1145/1387269.1387299},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1387269.1387299},
	Bdsk-Url-2 = {https://doi.org/10.1145/1387269.1387299}}

@inproceedings{vanderBurg:2011:SDF:1988008.1988039,
	Abstract = {Deploying components of a service-oriented system in a network of machines is often a complex and labourious process. Usually the environment in which such systems are deployed is dynamic: any machine in the network may crash, network links may temporarily fail, and so on. Such events may render the system partially or completely unusable. If an event occurs, it is difficult and expensive to redeploy the system to the take the new circumstances into account. In this paper we present a self-adaptive deployment framework built on top of Disnix, a model-driven distributed deployment tool for service-oriented systems. This framework dynamically discovers machines in the network and generates a mapping of components to machines based on non-functional properties. Disnix is then invoked to automatically, reliably and efficiently redeploy the system.},
	Acmid = {1988039},
	Address = {New York, NY, USA},
	Author = {van der Burg, Sander and Dolstra, Eelco},
	Booktitle = {Proceedings of the 6th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:29:08 +0100},
	Doi = {10.1145/1988008.1988039},
	Isbn = {978-1-4503-0575-4},
	Keywords = {service-oriented systems, software deployment},
	Location = {Waikiki, Honolulu, HI, USA},
	Numpages = {10},
	Pages = {208--217},
	Publisher = {ACM},
	Series = {SEAMS '11},
	Title = {A Self-adaptive Deployment Framework for Service-oriented Systems},
	Url = {http://doi.acm.org/10.1145/1988008.1988039},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1988008.1988039},
	Bdsk-Url-2 = {https://doi.org/10.1145/1988008.1988039}}

@inproceedings{Lopes:2011:SWA:2064747.2064768,
	Abstract = {Relevant biomedical advances happen daily, and the medical profession relies on this evolution to deliver an improved patient care. In addition, the growing magnitude of data generated by biomedical software and hardware since the initial discovery of the human genome is remarkable in size and variety. Hence, best-of-breed software solutions are at best a couple years behind clinical practice demands.
In this paper we detail an innovative Semantic Web interoperability framework, which provides developers with a complete software stack for semantic application deployment. Interoperability is the defining feature of this framework. On the one hand, new instances are able to integrate several types of distributed and heterogeneous data. On the other hand, collected data are made available through a public SPARQL endpoint.},
	Acmid = {2064768},
	Address = {New York, NY, USA},
	Author = {Lopes, Pedro and Oliveira, Jos{\'e} Lu\'{\i}s},
	Booktitle = {Proceedings of the First International Workshop on Managing Interoperability and Complexity in Health Systems},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:07:36 +0100},
	Doi = {10.1145/2064747.2064768},
	Isbn = {978-1-4503-0954-7},
	Keywords = {bioinformatics, healthcare interoperability, semantic exploration, semantic integration, semantic web},
	Location = {Glasgow, Scotland, UK},
	Numpages = {4},
	Pages = {87--90},
	Publisher = {ACM},
	Series = {MIXHS '11},
	Title = {A Semantic Web Application Framework for Health Systems Interoperability},
	Url = {http://doi.acm.org/10.1145/2064747.2064768},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2064747.2064768},
	Bdsk-Url-2 = {https://doi.org/10.1145/2064747.2064768}}

@inproceedings{Agarwal:2005:SCE:1060745.1060768,
	Abstract = {The demand for quickly delivering new applications is increasingly becoming a business imperative today. Application development is often done in an ad hoc manner, without standard frameworks or libraries, thus resulting in poor reuse of software assets. Web services have received much interest in industry due to their potential in facilitating seamless business-to-business or enterprise application integration. A web services composition tool can help automate the process, from creating business process functionality, to developing executable workflows, to deploying them on an execution environment. However, we find that the main approaches taken thus far to standardize and compose web services are piecemeal and insufficient. The business world has adopted a (distributed) programming approach in which web service instances are described using WSDL, composed into flows with a language like BPEL and invoked with the SOAP protocol. Academia has propounded the AI approach of formally representing web service capabilities in ontologies, and reasoning about their composition using goal-oriented inferencing techniques from planning. We present the first integrated work in composing web services end to end from specification to deployment by synergistically combining the strengths of the above approaches. We describe a prototype service creation environment along with a use-case scenario, and demonstrate how it can significantly speed up the time-to-market for new services.},
	Acmid = {1060768},
	Address = {New York, NY, USA},
	Author = {Agarwal, Vikas and Dasgupta, Koustuv and Karnik, Neeran and Kumar, Arun and Kundu, Ashish and Mittal, Sumit and Srivastava, Biplav},
	Booktitle = {Proceedings of the 14th International Conference on World Wide Web},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:29:29 +0100},
	Doi = {10.1145/1060745.1060768},
	Isbn = {1-59593-046-9},
	Keywords = {Web services composition, planning, semantic Web},
	Location = {Chiba, Japan},
	Numpages = {10},
	Pages = {128--137},
	Publisher = {ACM},
	Series = {WWW '05},
	Title = {A Service Creation Environment Based on End to End Composition of Web Services},
	Url = {http://doi.acm.org/10.1145/1060745.1060768},
	Year = {2005},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1060745.1060768},
	Bdsk-Url-2 = {https://doi.org/10.1145/1060745.1060768}}

@inproceedings{Ermagan:2007:SBC:1270232.1270289,
	Abstract = {The use of commercial off-the-shelf (COTS) software can greatly reduce the development cost and effort for complex software systems. Reusing software can also improve the general quality of a system by leveraging already proven implementations. One of the limiting factors in the adoption of COTS software is the complexity of integrating it with the rest of the system under development. Often, requirements do not entirely match the functionalities available in COTS components, increasing the complexity of the glue software that needs to be written. In this paper, we present the blueprint of a service-oriented architecture that can guide the engineer both in specifying the functionalities of a complex software system and as a deployment architecture to seamlessly integrate COTS components implementing such functionalities. The COTS integration concern, typically a deployment issue, is addressed in the service architecture, and is treated as first-class citizen of the development process.},
	Acmid = {1270289},
	Address = {Washington, DC, USA},
	Author = {Ermagan, Vina and Farcas, Claudiu and Farcas, Emilia and Kruger, Ingolf H. and Menarini, Massimiliano},
	Booktitle = {Proceedings of the Second International Workshop on Incorporating COTS Software into Software Systems: Tools and Techniques},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-24 20:44:24 +0100},
	Doi = {10.1109/IWICSS.2007.2},
	Isbn = {0-7695-2966-6},
	Pages = {6},
	Publisher = {IEEE Computer Society},
	Series = {IWICSS '07},
	Title = {A Service-Oriented Blueprint for COTS Integration: The Hidden Part of the Iceberg},
	Url = {http://dx.doi.org/10.1109/IWICSS.2007.2},
	Year = {2007},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IWICSS.2007.2}}

@inproceedings{Judd:2006:SAP:1160987.1160990,
	Abstract = {Despite their widespread deployment, many aspects of wireless network performance are poorly understood, and there is great room from improvement in wireless network reliability and performance. A key obstacle to understanding and improving wireless networks has been the lack of a realistic yet flexible experimental methodology. Physical layer wireless network emulation promises to achieve much of the flexibility of wireless simulators while maintaining much of the realism of real wireless networks. We have developed a software architecture that tames the complexity of physical layer wireless network emulation, and presents users with a powerful yet ease-to-use interface. We present several case studies showing how this software architecture allows complex wireless experiments to be conducted in an efficient manner while still enabling novice users to quickly run simple experiments.},
	Acmid = {1160990},
	Address = {New York, NY, USA},
	Author = {Judd, Glenn and Steenkiste, Peter},
	Booktitle = {Proceedings of the 1st International Workshop on Wireless Network Testbeds, Experimental Evaluation \& Characterization},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:29:54 +0100},
	Doi = {10.1145/1160987.1160990},
	Isbn = {1-59593-539-8},
	Keywords = {emulation, simulation, software architecture, wireless networks},
	Location = {Los Angeles, CA, USA},
	Numpages = {8},
	Pages = {2--9},
	Publisher = {ACM},
	Series = {WiNTECH '06},
	Title = {A Software Architecture for Physical Layer Wireless Network Emulation},
	Url = {http://doi.acm.org/10.1145/1160987.1160990},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1160987.1160990},
	Bdsk-Url-2 = {https://doi.org/10.1145/1160987.1160990}}

@inproceedings{Bernini:2009:SAD:1582379.1582391,
	Abstract = {Emergency management involves collaboration among different operators (e.g. policemen, firemen, medics) on critical and dangerous situations (e.g. fires, floods). Real-time elaborations of a large amount of information and knowledge are needed to automate control and decision making processes. In this scenario raw data are captured and processed through a chain of activities (filtering, fusion, classification) generating abstract information that is selected, diffused and presented to interested users. These activities can be turned into the interpretation of executable transformation models, abstract representations of input-output transformations which can be coded on a calculator. Executable transformation models might not be static; but they may be dynamically generated by observing and processing incoming data. From the architectural point of view the dynamic model generation implies the existence of mechanisms for the dynamic deployment of the executable transformation models. In this paper we present a software architecture aimed at providing these mechanisms.},
	Acmid = {1582391},
	Address = {New York, NY, USA},
	Author = {Bernini, Diego and Toscani, Daniele and Frigerio, Marco},
	Booktitle = {Proceedings of the 2009 International Conference on Wireless Communications and Mobile Computing: Connecting the World Wirelessly},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:08:16 +0100},
	Doi = {10.1145/1582379.1582391},
	Isbn = {978-1-60558-569-7},
	Keywords = {adaptivity, deployment, software architecture},
	Location = {Leipzig, Germany},
	Numpages = {5},
	Pages = {47--51},
	Publisher = {ACM},
	Series = {IWCMC '09},
	Title = {A Software Architecture for the Deployment of Executable Transformation Models},
	Url = {http://doi.acm.org/10.1145/1582379.1582391},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1582379.1582391},
	Bdsk-Url-2 = {https://doi.org/10.1145/1582379.1582391}}

@inproceedings{Mattmann:2006:SAF:1134285.1134400,
	Abstract = {Modern scientific research is increasingly conducted by virtual communities of scientists distributed around the world. The data volumes created by these communities are extremely large, and growing rapidly. The management of the resulting highly distributed, virtual data systems is a complex task, characterized by a number of formidable technical challenges, many of which are of a software engineering nature. In this paper we describe our experience over the past seven years in constructing and deploying OODT, a software framework that supports large, distributed, virtual scientific communities. We outline the key software engineering challenges that we faced, and addressed, along the way. We argue that a major contributor to the success of OODT was its explicit focus on software architecture. We describe several large-scale, real-world deployments of OODT, and the manner in which OODT helped us to address the domain-specific challenges induced by each deployment.},
	Acmid = {1134400},
	Address = {New York, NY, USA},
	Author = {Mattmann, Chris A. and Crichton, Daniel J. and Medvidovic, Nenad and Hughes, Steve},
	Booktitle = {Proceedings of the 28th International Conference on Software Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:30:14 +0100},
	Doi = {10.1145/1134285.1134400},
	Isbn = {1-59593-375-1},
	Keywords = {OODT, data management, software architecture},
	Location = {Shanghai, China},
	Numpages = {10},
	Pages = {721--730},
	Publisher = {ACM},
	Series = {ICSE '06},
	Title = {A Software Architecture-based Framework for Highly Distributed and Data Intensive Scientific Applications},
	Url = {http://doi.acm.org/10.1145/1134285.1134400},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1134285.1134400},
	Bdsk-Url-2 = {https://doi.org/10.1145/1134285.1134400}}

@inproceedings{Xia:2013:SDA:2486001.2491715,
	Abstract = {The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms.},
	Acmid = {2491715},
	Address = {New York, NY, USA},
	Author = {Xia, Wenfeng and Tsou, Tina and Lopez, Diego R. and Sun, Qiong and Lu, Felix and Xie, Haiyong},
	Booktitle = {Proceedings of the ACM SIGCOMM 2013 Conference on SIGCOMM},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:08:35 +0100},
	Doi = {10.1145/2486001.2491715},
	Isbn = {978-1-4503-2056-6},
	Keywords = {ipv6 transition, software defined network},
	Location = {Hong Kong, China},
	Numpages = {2},
	Pages = {547--548},
	Publisher = {ACM},
	Series = {SIGCOMM '13},
	Title = {A Software Defined Approach to Unified IPv6 Transition},
	Url = {http://doi.acm.org/10.1145/2486001.2491715},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2486001.2491715},
	Bdsk-Url-2 = {https://doi.org/10.1145/2486001.2491715}}

@article{Xia:2013:SDA:2534169.2491715,
	Abstract = {The IPv6 transition has been an ongoing process throughout the world due to the exhaustion of the IPv4 address space. However, this transition leads to costly end-to-end network upgrades and poses new challenges of managing a large number of devices with a variety of transitioning protocols. Recognizing these difficulties, we propose an software defined approach to unifying the deployment of IPv6 in a cost-effective, flexible manner. Our deployment and experiments demonstrate significant benefits of this approach, including low complexity, low cost and high flexibility of adopting different existing transition mechanisms.},
	Acmid = {2491715},
	Address = {New York, NY, USA},
	Author = {Xia, Wenfeng and Tsou, Tina and Lopez, Diego R. and Sun, Qiong and Lu, Felix and Xie, Haiyong},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:30:35 +0100},
	Doi = {10.1145/2534169.2491715},
	Issn = {0146-4833},
	Issue_Date = {October 2013},
	Journal = {SIGCOMM Comput. Commun. Rev.},
	Keywords = {ipv6 transition, software defined network},
	Number = {4},
	Numpages = {2},
	Pages = {547--548},
	Publisher = {ACM},
	Title = {A Software Defined Approach to Unified IPv6 Transition},
	Url = {http://doi.acm.org/10.1145/2534169.2491715},
	Volume = {43},
	Year = {2013},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2534169.2491715},
	Bdsk-Url-2 = {https://doi.org/10.1145/2534169.2491715}}

@inproceedings{Nguyen:2008:SSB:1401890.1402027,
	Abstract = {In this paper, we present an outline of a software system for buzz-based recommendations. This system is based on a large source of queries in an eCommerce application. The buzz events are detected based on query bursts linked to external entities like news and inventory information. A semantic neighborhood of the chosen buzz query is selected and appropriate recommendations are made on products that relate to this neighborhood. The system follows the paradigm of limited quantity merchandizing, in the sense that on a per-day basis the system shows recommendations around a single buzz query with the intent of increasing user curiosity and promoting user activity and stickiness. The system demonstrates the deployment of an interesting application based on KDD principles applied to a high volume industrial context.},
	Acmid = {1402027},
	Address = {New York, NY, USA},
	Author = {Nguyen, Hill and Parikh, Nish and Sundaresan, Neel},
	Booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:08:52 +0100},
	Doi = {10.1145/1401890.1402027},
	Isbn = {978-1-60558-193-4},
	Keywords = {large-scale buzz detection, semantic relatedness, surprise a day},
	Location = {Las Vegas, Nevada, USA},
	Numpages = {4},
	Pages = {1093--1096},
	Publisher = {ACM},
	Series = {KDD '08},
	Title = {A Software System for Buzz-based Recommendations},
	Url = {http://doi.acm.org/10.1145/1401890.1402027},
	Year = {2008},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1401890.1402027},
	Bdsk-Url-2 = {https://doi.org/10.1145/1401890.1402027}}

@inproceedings{DiFabbrizio:2009:SMF:1647314.1647329,
	Abstract = {Amid today's proliferation of Web content and mobile phones with broadband data access, interacting with small-form factor devices is still cumbersome. Spoken interaction could overcome the input limitations of mobile devices, but running an automatic speech recognizer with the limited computational capabilities of a mobile device becomes an impossible challenge when large vocabularies for speech recognition must often be updated with dynamic content. One popular option is to move the speech processing resources into the network by concentrating the heavy computation load onto server farms. Although successful services have exploited this approach, it is unclear how such a model can be generalized to a large range of mobile applications and how to scale it for large deployments. To address these challenges we introduce the AT&T speech mashup architecture, a novel approach to speech services that leverages web services and cloud computing to make it easier to combine web content and speech processing. We show that this new compositional method is suitable for integrating automatic speech recognition and text-to-speech synthesis resources into real multimodal mobile services. The generality of this method allows researchers and speech practitioners to explore a countless variety of mobile multimodal services with a finer grain of control and richer multimedia interfaces. Moreover, we demonstrate that the speech mashup is scalable and particularly optimized to minimize round trips in the mobile network, reducing latency for better user experience.},
	Acmid = {1647329},
	Address = {New York, NY, USA},
	Author = {Di Fabbrizio, Giuseppe and Okken, Thomas and Wilpon, Jay G.},
	Booktitle = {Proceedings of the 2009 International Conference on Multimodal Interfaces},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:16:05 +0100},
	Doi = {10.1145/1647314.1647329},
	Isbn = {978-1-60558-772-1},
	Keywords = {mashups, multimodal, speech mashup, speech services, speech system architecture, web services},
	Location = {Cambridge, Massachusetts, USA},
	Numpages = {8},
	Pages = {71--78},
	Publisher = {ACM},
	Series = {ICMI-MLMI '09},
	Title = {A Speech Mashup Framework for Multimodal Mobile Services},
	Url = {http://doi.acm.org/10.1145/1647314.1647329},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1647314.1647329},
	Bdsk-Url-2 = {https://doi.org/10.1145/1647314.1647329}}

@inproceedings{Emmerich:2014:SNS:2755535.2755553,
	Abstract = {Latency has a high impact on the user satisfaction in real-time multiplayer games. The network between the client and server is, in most cases, the main cause for latency and out of the scope of the game's developer. But the developer should avoid introducing unnecessary delays within his responsibility; i.e. with respect to development and operating costs he should wisely choose a network stack and deployment model for the server in order to reduce latency. In this paper we present latency measurements of the Linux network stack and effects of virtualization. We show that a worst-case scenario can cause a latency in the millisecond range when running a game server. We discuss how latencies can be reduced by using the packet processing framework DPDK to replace the operating system's network stack.},
	Acmid = {2755553},
	Address = {Piscataway, NJ, USA},
	Articleno = {15},
	Author = {Emmerich, Paul and Raumer, Daniel and Wohlfart, Florian and Carle, Georg},
	Booktitle = {Proceedings of the 13th Annual Workshop on Network and Systems Support for Games},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:30:53 +0100},
	Keywords = {intel DPDK, latency, measurement, virtualization},
	Location = {Nagoya, Japan},
	Numpages = {6},
	Pages = {15:1--15:6},
	Publisher = {IEEE Press},
	Series = {NetGames '14},
	Title = {A Study of Network Stack Latency for Game Servers},
	Url = {http://dl.acm.org/citation.cfm?id=2755535.2755553},
	Year = {2014},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2755535.2755553}}

@inproceedings{Abdlhamed:2016:SIP:2896387.2896420,
	Abstract = {Modern critical infrastructures have to process significantly large data sets. Intrusion-detection systems and unified threat management systems have the role of keeping critical infrastructures secure against cyber-attacks. However, in the world of big data, these systems are struggling to cope with overload and often become the bottle neck in the data network. To overcome this, our research investigates the use of deploying intrusion-detection and intrusion-prediction techniques in a cloud environment. Consequently, in this paper, a survey of existing intrusion-detection systems is presented and a discussion on how their deployment can enhance current security techniques in a cloud computing environment is put forward. A novel technique for intrusion prediction system is also put forward in this paper. Predictive statistical methods are used for proving the concepts put forward. The initial results show the necessity for using evolving statistical methods in prediction solutions; and the insufficiency of 'single-technique models' for building general solutions to predict intrusions. Furthermore, as this research shows, the concept of integrating multiple methods, such as game theory concepts and risk assessment methods, facilitates the development of a more efficient prediction model.},
	Acmid = {2896420},
	Address = {New York, NY, USA},
	Articleno = {35},
	Author = {Abdlhamed, Mohamed and Kifayat, Kashif and Shi, Qi and Hurst, William},
	Booktitle = {Proceedings of the International Conference on Internet of Things and Cloud Computing},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:16:24 +0100},
	Doi = {10.1145/2896387.2896420},
	Isbn = {978-1-4503-4063-2},
	Keywords = {Cyber-Attack, Intrusion Prediction, Security, cloud computing},
	Location = {Cambridge, United Kingdom},
	Numpages = {9},
	Pages = {35:1--35:9},
	Publisher = {ACM},
	Series = {ICC '16},
	Title = {A System for Intrusion Prediction in Cloud Computing},
	Url = {http://doi.acm.org/10.1145/2896387.2896420},
	Year = {2016},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2896387.2896420},
	Bdsk-Url-2 = {https://doi.org/10.1145/2896387.2896420}}

@article{Kang:2006:SAM:1132469.1132473,
	Abstract = {In current IT practices, the task of managing post-deployment system changes often falls in no-man's land.},
	Acmid = {1132473},
	Address = {New York, NY, USA},
	Author = {Kang, David and Chiang, Roger},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 00:18:02 +0100},
	Doi = {10.1145/1132469.1132473},
	Issn = {0001-0782},
	Issue_Date = {June 2006},
	Journal = {Commun. ACM},
	Number = {6},
	Numpages = {6},
	Pages = {91--96},
	Publisher = {ACM},
	Title = {A Systematic Approach in Managing Post-deployment System Changes},
	Url = {http://doi.acm.org/10.1145/1132469.1132473},
	Volume = {49},
	Year = {2006},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1132469.1132473},
	Bdsk-Url-2 = {https://doi.org/10.1145/1132469.1132473}}

@inproceedings{Javed:2014:SLR:2601248.2601278,
	Abstract = {The links between the software architecture and the source code of a software system should be based on solid traceability mechanisms in order to effectively perform quality control and maintenance of the software system. There are several primary studies on traceability between software architecture and source code but so far no systematic literature review (SLR) has been undertaken. This study presents an SLR which has been carried out to discover the existing traceability approaches and tools between software architecture and source code, as well as the empirical evidence for these approaches, their benefits and liabilities, their relations to software architecture understanding, and issues, barriers, and challenges of the approaches. In our SLR the ACM Guide to Computing Literature has been electronically searched to accumulate the biggest share of relevant scientific bibliographic citations from the major publishers in computing. The search strategy identified 742 citations, out of which 11 have been included in our study, dated from 1999 to July, 2013, after applying our inclusion and exclusion criteria. Our SLR resulted in the identification of the current state-of-the-art of traceability approaches and tools between software architecture and source code, as well as gaps and pointers for further research. Moreover, the classification scheme developed in this paper can serve as a guide for researchers and practitioners to find a specific approach or set of approaches that is of interest to them.},
	Acmid = {2601278},
	Address = {New York, NY, USA},
	Articleno = {16},
	Author = {Javed, Muhammad Atif and Zdun, Uwe},
	Booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
	Date-Added = {2019-10-30 18:04:30 +0100},
	Date-Modified = {2020-10-22 01:16:47 +0100},
	Doi = {10.1145/2601248.2601278},
	Isbn = {978-1-4503-2476-2},
	Keywords = {software architecture, source code, systematic literature review, traceability},
	Location = {London, England, United Kingdom},
	Numpages = {10},
	Pages = {16:1--16:10},
	Publisher = {ACM},
	Series = {EASE '14},
	Title = {A Systematic Literature Review of Traceability Approaches Between Software Architecture and Source Code},
	Url = {http://doi.acm.org/10.1145/2601248.2601278},
	Year = {2014},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/2601248.2601278},
	Bdsk-Url-2 = {https://doi.org/10.1145/2601248.2601278}}

@inproceedings{7436936,
	Abstract = {In modern environment, delivering innovative idea in a fast and reliable manner is extremely significant for any organizations. In the existing scenario, Insurance industry need to better respond to dynamic market requirements, faster time to market for new initiatives and services, and support innovative ways of customer interaction. In past few years, the transition to cloud platforms has given benefits such as agility, scalability, and lower capital costs but the application lifecycle management practices are slow with this disruptive change. DevOps culture extends the agile methodology to rapidly create applications and deliver them across environment in automated manner to improve performance and quality assurance. Continuous Integration (CI) and Continuous delivery (CD) has emerged as a boon for traditional application development and release management practices to provide the capability to release quality artifacts continuously to customers with continuously integrated feedback. The objective of the paper is to create a proof of concept for designing an effective framework for continuous integration, continuous testing, and continuous delivery to automate the source code compilation, code analysis, test execution, packaging, infrastructure provisioning, deployment, and notifications using build pipeline concept.},
	Author = {M. {Soni}},
	Booktitle = {2015 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:57:39 +0100},
	Doi = {10.1109/CCEM.2015.29},
	Keywords = {cloud computing;insurance data processing;program diagnostics;program testing;software prototyping;source code (software);build pipeline;software deployment;agile methodology;cloud computing;notifications;infrastructure provisioning;packaging;test execution;code analysis;source code compilation;continuous delivery;continuous testing;continuous integration;insurance industry;DevOps;end to end automation;Cloud computing;Pipelines;Automation;Servers;Testing;Configuration management;Insurance;Cloud Computing;DevOps;Continuous Integration;Continuous Testing;Configuration Management;Continuous Delivery;Automation},
	Pages = {85-89},
	Title = {End to End Automation on Cloud with Build Pipeline: The Case for DevOps in Insurance Industry, Continuous Integration, Continuous Testing, and Continuous Delivery},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/CCEM.2015.29}}

@inproceedings{7173368,
	Abstract = {As part of Agile transformation in past few years we have seen IT organizations adopting continuous integration principles in their software delivery lifecycle, which has improved the efficiency of development teams. With the time it has been realized that this optimization as part of continuous integration - alone - is just not helping to make the entire delivery lifecycle efficient or is not driving the organization efficiency. Unless all the pieces of a software delivery lifecycle work like a well oiled machine - efficiency of organization to optimize the delivery lifecycle can not be met. This is the problem which DevOps tries to address. This paper tries to cover all aspects of Devops applicable to various phases of SDLC and specifically talks about business need, ways to move from continuous integration to continuous delivery and its benefits. Continuous delivery transformation in this paper is explained with a real life case study that how infrastructure can be maintained just in form of code (IAAC). Finally this paper touches upon various considerations one must evaluate before adopting DevOps and what kind of benefits one can expect.},
	Author = {M. {Virmani}},
	Booktitle = {Fifth International Conference on the Innovative Computing Technology (INTECH 2015)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 18:00:15 +0100},
	Doi = {10.1109/INTECH.2015.7173368},
	Keywords = {integrated software;software prototyping;DevOps;continuous integration;continuous delivery;agile transformation;software delivery lifecycle;SDLC;IAAC;Software;Organizations;Testing;Automation;Optimization;Production;DevOps;Continuous Integration;Continuous Delivery;Infrastructure as a Code (IAAC)},
	Pages = {78-82},
	Title = {Understanding DevOps bridging the gap from continuous integration to continuous delivery},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/INTECH.2015.7173368}}

@inproceedings{7427129,
	Abstract = {Internet of Things (IoT), one of the hottest research targets, needs the systematic leverage from cloud-centric ICT infrastructure. Among several challenges for fast and economic IoT-Cloud service enablement, the continuous integration and delivery (CI/CD) framework is one of commonly wanted capabilities. In this paper, we introduce early experience with deploying an experimental continuous delivery framework for SmartX-mini IoT-Cloud Playground.},
	Author = {J. {Bae} and J. {Kim}},
	Booktitle = {2016 International Conference on Information Networking (ICOIN)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:56:27 +0100},
	Doi = {10.1109/ICOIN.2016.7427129},
	Keywords = {cloud computing;Internet of Things;experimental continuous delivery framework;SmartX-mini IoT-cloud playground;Internet-of-Things;cloud-centric ICT infrastructure;information and communication technology;continuous integration and delivery framework;CI/CD framework;IoT-Cloud service enablement;Continuous integration and delivery;software-defined infrastructure;SDN/NFV/Cloud integration;IoT-Cloud software framework;Internet of Things},
	Pages = {348-350},
	Title = {An experimental continuous delivery framework for SmartX-mini IoT-cloud playground},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICOIN.2016.7427129}}

@inproceedings{6753788,
	Abstract = {With the growth of Cloud based technologies, so have the cloud based services grown over the past few years. To meet the demands from the Cloud users, service providers have to support such a service environment that can be deployed and cloned automatically and easily. To help tackling such an issue, the Ezilla, which is considered as a private Cloud toolkit, has been developed. The new approach is automatic converting physical machines into virtual machines (P2V) in Cloud environment. It integrates the de facto Cloud middleware, and coordinated cloud infrastructure services such as storage, computing and networking services to form an integrated virtual computer. The merit of the Ezilla is simplifying the deployment complexity of the Cloud system for the users. The proposed Ezilla toolkit leverages genetic virtualization techniques and cluster management toolkits, such as Diskless Remote Boot in Linux - Single System Image (DRBL-SSI), and the clone OS toolkit - Clonezilla, to orchestrate distributed computing resources and convert physical machine to virtual one dynamically.},
	Author = {H. {Chen} and C. {Wu} and Y. {Pan} and H. {Yu} and C. {Chen} and K. {Cheng}},
	Booktitle = {2013 IEEE 5th International Conference on Cloud Computing Technology and Science},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 18:00:08 +0100},
	Doi = {10.1109/CloudCom.2013.25},
	Keywords = {cloud computing;middleware;software tools;virtual machines;distributed computing resources;Clonezilla;clone OS toolkit;DRBL-SSI;Linux-single system image;diskless remote boot;cluster management toolkits;genetic virtualization techniques;integrated virtual computer;coordinated cloud infrastructure services;de facto cloud middleware;cloud environment;automatic converting physical machines;P2V;virtual machines;cloud users;service providers;cloud based services;cloud based technology;Ezilla toolkit;private cloud service;automated fast deployment;Virtual machining;Servers;Cloud computing;Virtualization;Cloning;File systems;P2V;Cloud Middleware;Virtualization Techniques;DRBL;Clonezilla},
	Pages = {136-141},
	Title = {Towards the Automated Fast Deployment and Clone of Private Cloud Service: The Ezilla Toolkit},
	Volume = {1},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/CloudCom.2013.25}}

@inproceedings{6735430,
	Abstract = {To meet the demand from the Cloud users, service providers have to provide such a service environment that can be deployed and provisioned quickly and easily. To help tackle such an issue, the Ezilla which is considered as a private Cloud toolkit, has been developed by the Per Comp Team in the National Center for High-performance Computing. The Ezilla integrates the de facto Cloud middleware, Web-based interface, and coordinated cloud infrastructure services such as storage, computing and networking services to form an integrated virtual computer. With the Ezilla toolkit, a virtual computer, which is configured specifically to meet the needs of an individual Cloud user is just one click away. The merit of the Ezilla is that it can simplify the complexity of the Cloud system for users to deploy more easily. Scientist as well as genera users can deploy the Cloud for their work with no need to deal with the painful process of building the Cloud system. The proposed Ezilla toolkit leverages genetic virtualization techniques, the clone OS toolkit - Clonezilla, and distributed file system, to orchestrate distributed computing resources and convert physical machine to virtual one dynamically.},
	Author = {H. {Yu} and Y. {Pan} and C. {Wu} and H. {Chen} and C. {Chen} and K. {Cheng}},
	Booktitle = {2013 IEEE 5th International Conference on Cloud Computing Technology and Science},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:16 +0100},
	Doi = {10.1109/CloudCom.2013.141},
	Keywords = {cloud computing;distributed processing;middleware;storage management;virtual machines;ondemand automated fast deployment;coordinated cloud services;Ezilla toolkit;private Cloud toolkit;Cloud middleware;Web-based interface;coordinated cloud infrastructure services;integrated virtual computer;genetic virtualization technique;clone OS toolkit;Clonezilla;distributed file system;distributed computing resources;Virtual machining;File systems;Cloud computing;Writing;Computers;Monitoring;Virtualization;Virtualization Techniques;Virtual Cluster;Clonezilla;Distributed File system},
	Pages = {252-255},
	Title = {On-Demand Automated Fast Deployment and Coordinated Cloud Services},
	Volume = {2},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/CloudCom.2013.141}}

@inproceedings{7158514,
	Abstract = {Continuous Delivery (CD) has emerged as an auspicious software development discipline, with the promise of providing organizations the capability to release valuable software continuously to customers. Our organization has been implementing CD for the last two years. Thus far, we have moved 22 software applications to CD. I observed that CD has created a new context for architecting these applications. In this paper, I will try to characterize such a context of CD, explain why we need to architect for CD, describe the implications of architecting for CD, and discuss the challenges this new context creates. This information can provide insights to other practitioners for architecting their software applications, and provide researchers with input for developing their research agendas to further study this increasingly important topic.},
	Author = {L. {Chen}},
	Booktitle = {2015 12th Working IEEE/IFIP Conference on Software Architecture},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 18:00:01 +0100},
	Doi = {10.1109/WICSA.2015.23},
	Keywords = {software architecture;continuous delivery;CD;auspicious software development discipline;software architecture;Software;Context;Computer architecture;Monitoring;Pipelines;Companies;Software reliability;software architecture;continuous delivery;continuous deployment;continuous software engineering;quality attributes;architecturally significant requirements;non-functional requirements;DevOps},
	Pages = {131-134},
	Title = {Towards Architecting for Continuous Delivery},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/WICSA.2015.23}}

@inproceedings{7809441,
	Abstract = {Continuous Delivery (CD) can bring huge benefits, but implementing CD is challenging. For some challenges, one can only see them when he/she travels on the journey far enough. Paddy Power has been implementing CD for more than three years. In this talk, I will present the major obstacles we encountered and how we addressed them. These obstacles cover various areas, including organizational, cultural, process, and technical. I will also discuss the areas where I see researchers can help.},
	Author = {L. {Chen}},
	Booktitle = {2016 IEEE/ACM International Workshop on Continuous Software Evolution and Delivery (CSED)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:56:58 +0100},
	Doi = {10.1109/CSED.2016.023},
	Keywords = {software engineering;adoption obstacles;continuous delivery;CD;Paddy Power;Software;Companies;Conferences;Cultural differences;Software engineering;Software reliability;Continuous Delivery;Continuous Deployment;Continuous Software Engineering;Agile Software Development},
	Pages = {84-84},
	Title = {Continuous Delivery: Overcoming Adoption Obstacles},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/CSED.2016.023}}

@inproceedings{7169441,
	Abstract = {We have been implementing continuous delivery in Paddy Power, a large organization in the bookmaking industry, for more than two years. In this talk, I will reflect on our journey to continuous delivery and discuss the research opportunities I see.},
	Author = {L. {Chen}},
	Booktitle = {2015 IEEE/ACM 3rd International Workshop on Release Engineering},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:50 +0100},
	Doi = {10.1109/RELENG.2015.9},
	Keywords = {organisational aspects;software prototyping;release engineering;continuous software engineering;DevOps;agile software development;bookmaking industry;Paddy Power;bookmaking company;continuous delivery;research opportunities;Companies;Software;Software engineering;Conferences;Industries;Reflection;Continuous delivery;continuous deployment;release engineering;continuous software engineering;DevOps;agile software development},
	Pages = {2-2},
	Title = {Research Opportunities in Continuous Delivery: Reflections from Two Years' Experiences in a Large Bookmaking Company},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/RELENG.2015.9}}

@inproceedings{6612879,
	Abstract = {The popularization of agile development as well as the recent prevalence of virtualization and cloud computing has revolutionized the software delivery process- making it faster and affordable for businesses to release their software continuously. Hence, the need for a reliable and predictable delivery process for software applications. The aim of this paper is to develop a System Dynamics (SD) model to achieve a repetitive, risk-free and effortless Continuous Delivery process to reduce the perils of delayed delivery, delivery cost overrun and poor quality delivered software.},
	Author = {O. {Akerele} and M. {Ramachandran} and M. {Dixon}},
	Booktitle = {2013 Agile Conference},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:59:34 +0100},
	Doi = {10.1109/AGILE.2013.28},
	Keywords = {cloud computing;software prototyping;software quality;virtualisation;agile development popularization;virtualization;cloud computing;software delivery process making;agile continuous delivery process;system dynamic modeling;SD model;repetitive risk-free effortless continuous delivery process;software delivery cost overrun;delayed delivery;Software;Production;Automation;Computational modeling;Pipelines;Educational institutions;Context;Agile software development;Continuous Delivery;Delivery Pipeline;System Dynamics},
	Pages = {60-63},
	Title = {System Dynamics Modeling of Agile Continuous Delivery Process},
	Year = {2013},
	Bdsk-Url-1 = {https://doi.org/10.1109/AGILE.2013.28}}

@inproceedings{7789373,
	Abstract = {Static analysis tools are used for improving software quality and reliability. Since these tools can be time consuming when used for analysis of big codebases, they are normally run during scheduled (e.g. nightly) builds. However, the sooner a defect is found, the easier it is to fix efficiently. In order to detect defects faster, some analysis tools offer an integration with the integrated development environment of the developers at the cost of not always detecting all the issues. To detect defects earlier and still provide a reliable solution, one could think of running an analysis tool at every build of a continuous integration system. In this paper, we share the lessons learned during the integration of the static analysis tool Klocwork (that we are developing) with our continuous integration system. We think that the lessons learned will be beneficial for most companies developing safety-critical software (or less critical systems) that wish to run their analysis tool more often in their build system. We report these lessons learned along with examples of our successes and failures.},
	Author = {C. {Bolduc}},
	Booktitle = {2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:57:53 +0100},
	Doi = {10.1109/ISSREW.2016.48},
	Keywords = {integrated software;program diagnostics;safety-critical software;software quality;static analysis tool;continuous integration system;software quality;software reliability;codebases;integrated development environment;Klocwork;safety-critical software;critical system;Engines;Software;Standards;Software reliability;Companies;Context;static code analysis;continuous integration;software quality;Klocwork},
	Pages = {37-40},
	Title = {Lessons Learned: Using a Static Analysis Tool within a Continuous Integration System},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ISSREW.2016.48}}

@inproceedings{7949582,
	Abstract = {Large-scale complex avionics software system developed by multiple teams may lead to inconsistency issues of different architectures and various standards. The generic open software architecture for avionics system similar to Future Airborne Capability Environment (FACE) is therefore proposed. In this architecture, the portability of components is allowed through standardized interfaces between components and the software computing environment, which consists of an operating system, transport services and I/O services. The conformance test (CT) for the portable components should be automatically executed to support continuous integration. This paper presents a portal component conformance test environment (PCCTE) in which CT tasks are scheduled using event-driven policies based on a continuous integration server (such as Jenkins), and are executed in the cloud environment based on container (suck as Docker & Kubernetes) and micro-services (such as ZooKeeper). As a result, the evaluation shows that PCCTE is workable and effective.},
	Author = {K. {Lv} and Z. {Zhao} and R. {Rao} and P. {Hong} and X. {Zhang}},
	Booktitle = {2016 International Conference on Progress in Informatics and Computing (PIC)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:22 +0100},
	Doi = {10.1109/PIC.2016.7949582},
	Keywords = {aerospace computing;avionics;cloud computing;program testing;software engineering;PCCTE environment;container cloud;avionics software development;future airborne capability environment;FACE;avionics software system;conformance test;software computing environment;portal component conformance test environment;Aerospace electronics;Cloud computing;Containers;Computer architecture;Libraries;Testing;avionics software system;container cloud;conformance test environment},
	Pages = {664-668},
	Title = {PCCTE: A portable component conformance test environment based on container cloud for avionics software development},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/PIC.2016.7949582}}

@inproceedings{7087120,
	Abstract = {The enterprises follow Agile Software Development methodology to because business requirements changes frequently. In Agile Software Development methodology, it is essential to continuously integrate the component into a main trunk of a project to test the new component of the system. Then test all the component of the project; this happens frequently so it needs to stream line processes to orchestrate the tests. So it is difficult to manage the software development life cycle for those changes and maintain the software code quality. To maintain the product quality it is essential to integrate the product component and need to deploy a product on pre-production environment and test the product. Hence the need for Continuous Integration and Continuous Delivery process for software product. The popularization of DevOps, and cloud computing has revolutionized the software delivery process- making it faster and affordable for business to release their software continuously. Hence Enterprises need for reliable and predictable delivery process of software. The objective of the paper is to design an effective framework for automated testing and deployment to help to automate the code analysis, test selection, test scheduling, environment provisioning, test execution, results analysis and deployment pipeline. Test orchestration framework typically very complicated to develop such pipeline to make software reliable, and bug free. For environment provisioning can be provided through virtualization and cloud computing.},
	Author = {N. {Rathod} and A. {Surve}},
	Booktitle = {2015 International Conference on Pervasive Computing (ICPC)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:59:44 +0100},
	Doi = {10.1109/PERVASIVE.2015.7087120},
	Keywords = {program testing;scheduling;software prototyping;software quality;test orchestration;continuous integration process;continuous deployment process;agile software development methodology;component test;software development life cycle;software code quality;product quality;software product;DevOps;cloud computing;code analysis;test selection;test scheduling;environment provisioning;test execution;results analysis;deployment pipeline;virtualization;Software;Automation;Pipelines;Software reliability;Software testing;ContinuousIntegration (CI);Continious Deployment (CD);DevOps;Agile Software Development;Test Oracle;Test Orchestration;Cloud Computing;Delivery Pipeline},
	Pages = {1-5},
	Title = {Test orchestration a framework for Continuous Integration and Continuous deployment},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/PERVASIVE.2015.7087120}}

@inproceedings{5783172,
	Abstract = {Progress in the field of information technology and radio engineering along with customer requirements lead to development of more and more complex multicompanent radio-systems. Our research lab is specialized in creation of heterogeneous hardware-software systems which are made up of components based on various (including embedded) operating systems. Error-free operation of such systems depends on a lot of factors one of which is the software quality. The main conclusion to be drawn from previous experience of hardware-software systems production was to make changes in software development process. In particular, integration practice was set up as independent process with special build-engineer to maintain it. During the stage-by-stage evolution of integration process a number of key factors have been revealed: cross-platform development, component inter dependency, lack of human and time resources. To meet all requirements several integration schemes have been proposed and put into operation. On the basis of previous projects in-depth analysis the decision on introducing into practice continuous integration and automated build processes was made. The set of measures intended to integration process development which was presented in this paper allows us to reduce time cost on software releases from several days to several hours. Furthermore, suggested scheme of integration process is considered to be universal and, therefore, could be applied in any project regardless of operating systems which are used in development process. At the same time it should be noted that for the purpose of most successful implantation of continuous integration it is essential that all process stages should be well documented and understood by all developers before project starts.},
	Author = {I. {Kuzmina}},
	Booktitle = {2010 6th Central and Eastern European Software Engineering Conference (CEE-SECR)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:40 +0100},
	Doi = {10.1109/CEE-SECR.2010.5783172},
	Keywords = {hardware-software codesign;integrated software;operating systems (computers);software quality;software integration process;complex hardware-software system;information technology;radio engineering;customer requirement;multicomponent radio system;operating system;error free operation;software quality;software development process;projects in depth analysis;Electronic mail;Programming;Operating systems;Time measurement;Software measurement;Information technology;hardware-software system;software development process;continuous integration},
	Pages = {179-184},
	Title = {Practical realization of software integration process during the development of complex hardware-software systems},
	Year = {2010},
	Bdsk-Url-1 = {https://doi.org/10.1109/CEE-SECR.2010.5783172}}

@inproceedings{7169448,
	Abstract = {This paper illustrates how Jenkins evolved from being a pure Continuous Integration Platform to a Continuous Delivery one, embracing the new design tendency where not only the build but also the release and the delivery process of the product is automated. In this scenario Jenkins becomes the orchestrator tool for all the teams/roles involved in the software lifecycle, thanks to which Development, Quality&Assurance and Operations teams can work closely together. Goal of this paper is not only to position Jenkins as hub for CD, but also introduce the challenges that still need to be solved in order to strengthen Jenkins' tracking capabilities.},
	Author = {V. {Armenise}},
	Booktitle = {2015 IEEE/ACM 3rd International Workshop on Release Engineering},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:57:13 +0100},
	Doi = {10.1109/RELENG.2015.19},
	Keywords = {public domain software;software quality;Jenkins;continuous integration platform;continuous delivery process;software lifecycle;orchestrator tool;open source CI platform;Pipelines;Software;Automation;Time to market;Fingerprint recognition;Collaboration;Companies},
	Pages = {24-27},
	Title = {Continuous Delivery with Jenkins: Jenkins Solutions to Implement Continuous Delivery},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/RELENG.2015.19}}

@inproceedings{7883316,
	Abstract = {We believe that software engineering should be taught in a hands-on way such as through a project-based capstone course where students apply the learned concepts in a real setting. However, such a teaching format can be challenging and time-consuming for instructors. In this paper we explain how we selected and introduced a set of metrics to improve the manageability of our large multi-project capstone course. We regularly run such a course with over 100 students developing applications in 10-12 parallel projects over the course of one semester. Our approach focuses on measuring the success of three key workflows, namely Merge Management, Continuous Integration and Continuous Delivery. We show how these metrics help the instructors to keep track of the progress of multiple projects running at the same time, enabling them to identify and react to problems early.},
	Author = {L. {Alperowitz} and D. {Dzvonyar} and B. {Bruegge}},
	Booktitle = {2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:05 +0100},
	Keywords = {computer science education;continuous improvement;educational courses;project management;software prototyping;teaching;agile project courses;software engineering;project-based capstone course;teaching format;large multiproject capstone course manageability improvement;application development;merge management;continuous integration;continuous delivery;Measurement;Education;Organizations;Industries;Software;Servers;agile software engineering;capstone course;project manage- ment;metrics;continuous integration;continuous delivery},
	Pages = {323-326},
	Title = {Metrics in Agile Project Courses},
	Year = {2016}}

@inproceedings{6008780,
	Abstract = {Recent years, SaaS (Software as a Service) has become an innovative software delivery model. The traditional centralized software distribution model failed to meet the rapid deployment requirement of distribute software in large scale environment. In this paper, we present Soft-Union: a novel peer-to-peer based software distribution scheme. Soft-Union organizes the nodes sharing the same software into an overlay to reduce the query time. Furthermore, it incorporates a distributed mechanism for meta-information placement mechanism, and a search protocol to balance the query load and shorter the query time of the block meta-information. The experimental results validate that Soft-Union significantly reduces the block query time comparing with flooding and BT-like software distribution solutions.},
	Author = {L. {Zhong} and C. {Hu} and T. {Wo} and J. {Li} and W. {Zeng}},
	Booktitle = {2011 IEEE 4th International Conference on Cloud Computing},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:59 +0100},
	Doi = {10.1109/CLOUD.2011.95},
	Keywords = {cloud computing;peer-to-peer computing;soft union;P2P distribution scheme;software as a service;SaaS;innovative software delivery model;scale environment;peer-to-peer based software distribution;metainformation placement mechanism;Software;Peer to peer computing;Computer architecture;Protocols;Servers;Computational modeling;Cloud computing;SaaS;Software Distribution;Overlay;Peer-to-Peer;Search},
	Pages = {740-741},
	Title = {Soft-Union: An Overlay Based Efficient Software P2P Distribution Scheme},
	Year = {2011},
	Bdsk-Url-1 = {https://doi.org/10.1109/CLOUD.2011.95}}

@inproceedings{7816504,
	Abstract = {Continuous Delivery is an agile software development practice in which developers frequently integrate changes into the main development line and produce releases of their software. An automated Continuous Integration infrastructure builds and tests these changes. Claimed advantages of CD include early discovery of (integration) errors, reduced cycle time, and better adoption of coding standards and guidelines. This paper reports on a study in which we surveyed 152 developers of a large financial organization (ING Nederland), and investigated how they adopt a Continuous Integration and delivery pipeline during their development activities. In our study, we focus on topics related to managing technical debt, as well as test automation practices. The survey results shed light on the adoption of some agile methods in practice, and sometimes confirm, while in other cases, confute common wisdom and results obtained in other studies. For example, we found that refactoring tends to be performed together with other development activities, technical debt is almost always "self-admitted", developers timely document source code, and assure the quality of their product through extensive automated testing, with a third of respondents dedicating more than 50% of their time to do testing activities.},
	Author = {C. {Vassallo} and F. {Zampetti} and D. {Romano} and M. {Beller} and A. {Panichella} and M. D. {Penta} and A. {Zaidman}},
	Booktitle = {2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:57:03 +0100},
	Doi = {10.1109/ICSME.2016.72},
	Keywords = {financial data processing;program testing;quality assurance;software maintenance;software prototyping;software quality;source code (software);continuous delivery;CD;financial organization;ING Nederland;agile software development;continuous integration infrastructure;CI infrastructure;software refactoring;document source code;product quality assurance;automated testing;Pipelines;Testing;Software;Monitoring;Organizations;Measurement;Continuous Delivery;Continuous Integration;DevOps;Agile Development;Technical Debt;Refactoring;Testing;Test-Driven Development},
	Pages = {519-528},
	Title = {Continuous Delivery Practices in a Large Financial Organization},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSME.2016.72}}

@inproceedings{5431740,
	Abstract = {As software systems become larger and more complex, automated software engineering tools play a crucial role for effective software development management, which is a key factor to lead quality software systems. In this work, we present TRICA, an open source-based software development infrastructure. The name of TRICA represents its features such as Traceability, Relationship, Informativeness, Cost-effectiveness, and Automation. Essentially, in TRICA, a continuous integration tool is coupled with a software configuration management tool and an issue tracking tool. We provisioned a mechanism to connect the open source tools in TRICA so that project members use the collaborated information to solve various issues and implementation problems efficiently, and easily share forthcoming issues during the course of the project. We show that TRICA can help to decentralize risks throughout the software development cycle and achieve successful software development.},
	Author = {Y. {Ki} and M. {Song}},
	Booktitle = {2009 IEEE/ACM International Conference on Automated Software Engineering},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:56:37 +0100},
	Doi = {10.1109/ASE.2009.73},
	Keywords = {configuration management;groupware;integrated software;program diagnostics;public domain software;software development management;software quality;software tools;software systems;automated software engineering tools;software development management;software quality;TRICA;continuous integration tool;software configuration management tool;tracking tool;collaborated information;software development cycle;open source tools;Programming;Open source software;Software engineering;Software systems;Software tools;Testing;Software development management;Automation;Collaborative tools;Collaborative software;software engineering tools;continuous integration;SCM;issue tracking;open source},
	Pages = {525-529},
	Title = {An Open Source-Based Approach to Software Development Infrastructures},
	Year = {2009},
	Bdsk-Url-1 = {https://doi.org/10.1109/ASE.2009.73}}

@inproceedings{7453279,
	Abstract = {Continuous Integration (CI) is the most common practice among software developers where they integrate their work into a baseline frequently. The industry is facing huge challenges while developing Software (S/W)s at multiple sites and tested on multiple platforms. The best way to make CI faster and more efficient is to automate the build and testing process. Jenkins is a CI tool that helps in automating the complete process, reducing the work of a developer and check the development at each and every step of S/W evolution. In this paper, we discuss the implementation of Jenkins for software patch integration and release to client. We consider a real-life scenario, how the software development is carried out in corporate ventures and how Jenkins can save developers/integrators crucial work hours by automating the complete process. In this paper, Jenkins is implemented in a master/slave architecture where master node is the Jenkins server and slaves are the Jenkins clients. We discuss the usage of various plug-ins available that allow Jenkins to be used with any environment of software development.},
	Author = {N. {Seth} and R. {Khare}},
	Booktitle = {2015 2nd International Conference on Recent Advances in Engineering Computational Sciences (RAECS)},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:56:17 +0100},
	Doi = {10.1109/RAECS.2015.7453279},
	Keywords = {embedded systems;integrated software;program testing;software engineering;Jenkins clients;Jenkins server;master slave architecture;software patch integration;S/W evolution;CI tool;embedded software development;automated continuous integration;ACI;Testing;Automation;Smart phones;Software;Servers;Computer architecture;Androids;Android;Build;Continuous Integration;Jenkins;Source code Management},
	Pages = {1-6},
	Title = {ACI (automated Continuous Integration) using Jenkins: Key for successful embedded Software development},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/RAECS.2015.7453279}}

@inproceedings{7203074,
	Abstract = {We propose a novel technique for improving the efficiency of cloud-based continuous integration development environments. Our technique identifies repetitive, expensive and time-consuming setup activities that are required to run integration and system tests in the cloud, and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized. We create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud.},
	Author = {A. {Gambi} and Z. {Rostyslav} and S. {Dustdar}},
	Booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
	Date-Added = {2019-10-30 17:55:53 +0100},
	Date-Modified = {2019-10-30 17:58:32 +0100},
	Doi = {10.1109/ICSE.2015.253},
	Keywords = {cloud computing;virtual machines;of cloud-based continuous integration development environments;preconfigured testing virtual machines;test execution cost;testing machines;Virtual machining;Software;Servers;Standards;Load modeling;Software testing;integration test;test-driven development;snapshot;system tests;linear programming;cost flow;flow constraints},
	Pages = {797-798},
	Title = {Poster: Improving Cloud-Based Continuous Integration Environments},
	Volume = {2},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1109/ICSE.2015.253}}

@article{MAKINEN2016175,
	Abstract = {Context: Software companies seek to gain benefit from agile development approaches in order to meet evolving market needs without losing their innovative edge. Agile practices emphasize frequent releases with the help of an automated toolchain from code to delivery. Objective: We investigate, which tools are used in software delivery, what are the reasons omitting certain parts of the toolchain and what implications toolchains have on how rapidly software gets delivered to customers. Method: We present a multiple-case study of the toolchains currently in use in Finnish software-intensive organizations interested in improving their delivery frequency. We conducted qualitative semi-structured interviews in 18 case organizations from various software domains. The interviewees were key representatives of their organization, considering delivery activities. Results: Commodity tools, such as version control and continuous integration, were used in almost every organization. Modestly used tools, such as UI testing and performance testing, were more distinctly missing from some organizations. Uncommon tools, such as artifact repository and acceptance testing, were used only in a minority of the organizations. Tool usage is affected by the state of current workflows, manual work and relevancy of tools. Organizations whose toolchains were more automated and contained fewer manual steps were able to deploy software more rapidly. Conclusions: There is variety in the need for tool support in different development steps as there are domain-specific differences in the goals of the case organizations. Still, a well-founded toolchain supports speedy delivery of new software.},
	Author = {Simo M{\"a}kinen and Marko Lepp{\"a}nen and Terhi Kilamo and Anna-Liisa Mattila and Eero Laukkanen and Max Pagels and Tomi M{\"a}nnist{\"o}},
	Doi = {https://doi.org/10.1016/j.infsof.2016.09.001},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Continuous deployment, Continuous delivery, Software development tools, Deployment pipeline, Agile software development},
	Pages = {175 - 194},
	Title = {Improving the delivery cycle: A multiple-case study of the toolchains in Finnish software intensive enterprises},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584916301434},
	Volume = {80},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584916301434},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2016.09.001}}

@article{MOSSIGE2015169,
	Abstract = {Context
Testing complex industrial robots (CIRs) requires testing several interacting control systems. This is challenging, especially for robots performing process-intensive tasks such as painting or gluing, since their dedicated process control systems can be loosely coupled with the robot's motion control.
Objective
Current practices for validating CIRs involve manual test case design and execution. To reduce testing costs and improve quality assurance, a trend is to automate the generation of test cases. Our work aims to define a cost-effective automated testing technique to validate CIR control systems in an industrial context.
Method
This paper reports on a methodology, developed at ABB Robotics in collaboration with SIMULA, for the fully automated testing of CIRs control systems. Our approach draws on continuous integration principles and well-established constraint-based testing techniques. It is based on a novel constraint-based model for automatically generating test sequences where test sequences are both generated and executed as part of a continuous integration process.
Results
By performing a detailed analysis of experimental results over a simplified version of our constraint model, we determine the most appropriate parameterization of the operational version of the constraint model. This version is now being deployed at ABB Robotics's CIR testing facilities and used on a permanent basis. This paper presents the empirical results obtained when automatically generating test sequences for CIRs at ABB Robotics. In a real industrial setting, the results show that our methodology is not only able to detect reintroduced known faults, but also to spot completely new faults.
Conclusion
Our empirical evaluation shows that constraint-based testing is appropriate for automatically generating test sequences for CIRs and can be faithfully deployed in an industrial context.},
	Author = {Morten Mossige and Arnaud Gotlieb and Hein Meling},
	Doi = {https://doi.org/10.1016/j.infsof.2014.09.009},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Constraint programming, Continuous integration, Robotized painting, Software testing, Distributed real time systems, Agile development},
	Pages = {169 - 185},
	Title = {Testing robot controllers using constraint programming and continuous integration},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584914002080},
	Volume = {57},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584914002080},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2014.09.009}}

@article{FLUDER20151134,
	Abstract = {The software development for the control system of the cryogenics in the LHC is partially automatized. However, every single modification requires a sequence of consecutive and interdependent tasks to be executed manually by software developers. A large number of control system consolidations and the evolution of the used IT technologies lead to reviewing the software production methodology. As a result, an open-source continuous integration server has been employed integrating all development tasks, tools and technologies. This paper describes the main improvements that have been made to fully automate the process of software production and the achieved results.},
	Author = {C. Fluder and T. Wolak and A. Drozd and M. Dudek and F. Frassinelli and M. Pezzetti and A. Tovar-Gonzalez and M. Zapolski},
	Doi = {https://doi.org/10.1016/j.phpro.2015.06.176},
	Issn = {1875-3892},
	Journal = {Physics Procedia},
	Keywords = {CERN, LHC, cryogenics, industrial, control, software, development, continuous integration},
	Note = {Proceedings of the 25th International Cryogenic Engineering Conference and International Cryogenic Materials Conference 2014},
	Pages = {1134 - 1140},
	Title = {Improved Software Production for the LHC Tunnel Cryogenics Control System},
	Url = {http://www.sciencedirect.com/science/article/pii/S187538921500557X},
	Volume = {67},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S187538921500557X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.phpro.2015.06.176}}

@article{DODT201379,
	Abstract = {At the JET experiment data from routine diagnostics is analysed automatically by a suite of codes within minutes after operation. The maintenance of these interdependent codes and the provision of a consistent state of the physics database over many experimental campaigns against a backdrop of continuous hardware and software updates, requires well defined maintenance and validation procedures. In this paper, the development of a new generation of maintenance tools using distributed version control and a work-flow following the principle of continuous integration [1] is described.},
	Author = {D. Dodt and N. Cook and D. McDonald and D. Harting and S. Pamela},
	Doi = {https://doi.org/10.1016/j.fusengdes.2012.11.015},
	Issn = {0920-3796},
	Journal = {Fusion Engineering and Design},
	Keywords = {Intershot analysis of plasma diagnostics, Continuous integration, Distributed version control, Data traceability},
	Number = {2},
	Pages = {79 - 84},
	Title = {Improved framework for the maintenance of the JET intershot analysis chain},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920379612004784},
	Volume = {88},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0920379612004784},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.fusengdes.2012.11.015}}

@article{ROBATI2016229,
	Abstract = {Integrated Modular Avionics (IMA) architectures complemented with Time-Triggered Ethernet (TTEthernet) provides a strong platform to support the design and deployment of distributed avionic software systems. The complexity of the design and continuous integration of such systems can be managed using a model-based methodology. In this paper, we build on top of our extension of the AADL modeling language to model TTEthernet-based distributed systems and leverage model transformations to enable undertaking the verification of the system models produced with this methodology. In particular, we propose to transform the system models to a model suitable for a simulation with DEVS. We illustrate the proposed approach using an example of a navigation and guidance system and we use this example to show the verification of the contention-freedom property of TTEthernet schedule.},
	Author = {Tiyam Robati and Abdelouahed Gherbi and John Mullins},
	Doi = {https://doi.org/10.1016/j.procs.2016.04.120},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Integrated Modular Avionics, TTEthernet, Model transformation, Verification, DEVS, Simulation},
	Note = {The 7th International Conference on Ambient Systems, Networks and Technologies (ANT 2016) / The 6th International Conference on Sustainable Energy Information Technology (SEIT-2016) / Affiliated Workshops},
	Pages = {229 - 236},
	Title = {A Modeling and Verification Approach to the Design of Distributed IMA Architectures Using TTEthernet},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050916301430},
	Volume = {83},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050916301430},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2016.04.120}}

@article{FREDIAN2014649,
	Abstract = {Support of the MDSplus data handling system has been enhanced by the addition of an automated build system which does nightly builds of MDSplus for many computer platforms producing software packages which can now be downloaded using a web browser or via package repositories suitable for automatic updating. The build system was implemented using an extensible continuous integration server product called Hudson which schedules software builds on a collection of VMware based virtual machines. New releases are created based on updates via the MDSplus cvs code repository and versioning are managed using cvs tags and branches. Currently stable, beta and alpha releases of MDSplus are maintained for eleven different platforms including Windows, MacOSX, RedHat Enterprise Linux, Fedora, Ubuntu and Solaris. For some of these platforms, MDSplus packaging has been broken into functional modules so users can pick and choose which MDSplus features they want to install. An added feature to the latest Linux based platforms is the use of package dependencies. When installing MDSplus from the package repositories, any additional required packages used by MDSplus will be installed automatically greatly simplifying the installation of MDSplus. This paper will describe the MDSplus package automated build and distribution system.},
	Author = {T. Fredian and J. Stillerman and G. Manduchi},
	Doi = {https://doi.org/10.1016/j.fusengdes.2013.11.012},
	Issn = {0920-3796},
	Journal = {Fusion Engineering and Design},
	Keywords = {Data acquisition systems, Data management, Data formats, MDSplus},
	Note = {Proceedings of the 9th IAEA Technical Meeting on Control, Data Acquisition, and Remote Participation for Fusion Research},
	Number = {5},
	Pages = {649 - 651},
	Title = {MDSplus automated build and distribution system},
	Url = {http://www.sciencedirect.com/science/article/pii/S0920379613007126},
	Volume = {89},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0920379613007126},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.fusengdes.2013.11.012}}

@article{PREUVENEERS2016162,
	Abstract = {Recent software engineering paradigms such as software product lines, supporting development techniques like feature modeling, and cloud provisioning models such as platform and infrastructure as a service, allow for great flexibility during both software design and deployment, resulting in potentially large cost savings. However, all this flexibility comes with a catch: as the combinatorial complexity of optional design features and deployment variability increases, the difficulty of assessing system qualities such as scalability and quality of service increases too. And if the software itself is not scalable (for instance, because of a specific set of selected features), deploying additional service instances is a futile endeavor. Clearly there is a need to systematically measure the impact of feature selection on scalability, as the potential cost savings can be completely mitigated by the risk of having a system that is unable to meet service demand. In this work, we document our results on systematic load testing for automated quality of service and scalability analysis. The major contribution of our work is tool support and a methodology to analyze the scalability of these distributed, feature oriented multi-tenant software systems in a continuous integration process. We discuss our approach to select features for load testing such that a representative set of feature combinations is used to elicit valuable information on the performance impact and feature interactions. Additionally, we highlight how our methodology and framework for performance and scalability prediction differs from state-of-practice solutions. We take the viewpoint of both the tenant of the service and the service provider, and report on our experiences applying the approach to an industrial use case in the domain of electronic payments. We conclude that the integration of systematic scalability tests in a continuous integration process offers strong advantages to software developers and service providers, such as the ability to quantify the impact of new features in existing service compositions, and the early detection of hidden feature interactions that may negatively affect the overall performance of multi-tenant services.},
	Author = {Davy Preuveneers and Thomas Heyman and Yolande Berbers and Wouter Joosen},
	Doi = {https://doi.org/10.1016/j.jss.2015.12.024},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Distributed systems, Scalability, Tool support},
	Pages = {162 - 176},
	Title = {Systematic scalability assessment for feature oriented multi-tenant services},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121215002897},
	Volume = {116},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121215002897},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2015.12.024}}

@article{AGRAWAL2016291,
	Abstract = {The Agile methods favours more communication, continuous integration, rapid delivery of software modules, iterative and incremental approach, but at the same time Agile software development has limitations like lack of upfront planning, lack of sufficient documentation, lack of predictability, etc.. Sometimes these limitations and so many methods make Agile software development more stressful. This work is about finding the current limitations and advantages of Agile software development. For finding the actual limitations beyond the literature, an online survey was conducted with the specified sample size of Agile experienced professionals, then the ANOVA test is applied to satisfy the hypothesis.},
	Author = {Ashish Agrawal and Mohd. Aurangzeb Atiq and L.S. Maurya},
	Doi = {https://doi.org/10.1016/j.procs.2016.02.056},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Agile, Scrum, Survey, SDLC, Software Development},
	Note = {1st International Conference on Information Security & Privacy 2015},
	Pages = {291 - 297},
	Title = {A Current Study on the Limitations of Agile Methods in Industry Using Secure Google Forms},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050916000582},
	Volume = {78},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050916000582},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procs.2016.02.056}}

@article{KIDAMBI2013526,
	Abstract = {An explosion of digital multimedia technologies that permit quick and easy uploading of any multimedia file to the web, coupled with the rapid advancement of hardware and software have resulted in petabytes of multimedia data being available on the internet. In this paper, we provide a framework that rapidly integrates these data for personalized storyboarding utilizing Content Based Multimedia Retrieval (CBMR) techniques. A storyboard is a narrative of audio, text, video and images of a particular topic that are linked systematically. People have different abilities to understand a story/topic according to their learning styles coupled with experience, age, knowledge, gender etc. Personalized storyboard multimedia learning help the user dictate the learning process in a creative way through multiple means of representation, expression and engagement. This process involves active learning, both behavioral as well as cognitive. Learning is constructive, and information learned is remembered at a deeper level. The use of multimedia promotes meaningful learning that can be transferred or generalized to other situations. In this paper, various learning styles are discussed. The goal of the research was to design and implement a novel approach that integrates human reasoning with computerized algorithms for multimedia storyboarding for learning. By systematically coupling human reasoning and computerized process, we attempt to minimize the barrier between the human's cognitive model of what they are trying to storyboard and the computers understanding of the user's task. We present our model which utilizes a fusion of content based image retrieval, content based video retrieval, content based audio retrieval, and text based retrieval techniques with human computer interaction based relevance feedback to enhance the learning process through personalized multimedia storyboarding. To illustrate the advantages of this model in a greater detail, we showcased the concepts of this model utilizing the game of cricket. We summarize the paper by discussing the future in this area.},
	Author = {Phani Kidambi and S. Narayanan},
	Doi = {https://doi.org/10.3182/20130811-5-US-2037.00098},
	Issn = {1474-6670},
	Journal = {IFAC Proceedings Volumes},
	Keywords = {Human Computer Interaction, Content Based Multimedia Retrieval, Learning, Storyboarding},
	Note = {12th IFAC Symposium on Analysis, Design, and Evaluation of Human-Machine Systems},
	Number = {15},
	Pages = {526 - 532},
	Title = {Personalized Interactive Storyboarding utilizing Content Based Multimedia Retrieval},
	Url = {http://www.sciencedirect.com/science/article/pii/S1474667016331135},
	Volume = {46},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1474667016331135},
	Bdsk-Url-2 = {https://doi.org/10.3182/20130811-5-US-2037.00098}}

@article{STAHL201448,
	Abstract = {Continuous integration is a software practice where developers integrate frequently, at least daily. While this is an ostensibly simple concept, it does leave ample room for interpretation: what is it the developers integrate with, what happens when they do, and what happens before they do? These are all open questions with regards to the details of how one implements the practice of continuous integration, and it is conceivable that not all such implementations in the industry are alike. In this paper we show through a literature review that there are differences in how the practice of continuous integration is interpreted and implemented from case to case. Based on these findings we propose a descriptive model for documenting and thereby better understanding implementations of the continuous integration practice and their differences. The application of the model to an industry software development project is then described in an illustrative case study.},
	Author = {Daniel St{\aa}hl and Jan Bosch},
	Doi = {https://doi.org/10.1016/j.jss.2013.08.032},
	Issn = {0164-1212},
	Journal = {Journal of Systems and Software},
	Keywords = {Continuous integration, Agile software development},
	Pages = {48 - 59},
	Title = {Modeling continuous integration practice differences in industry software development},
	Url = {http://www.sciencedirect.com/science/article/pii/S0164121213002276},
	Volume = {87},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0164121213002276},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jss.2013.08.032}}

@article{HAMDAN20152019,
	Abstract = {The research in this paper combines two main areas, the first one is software quality and the second is the agile practices of continuous integration. Software quality has been an important topic since the beginning of the software development and production. Many researches have been conducted to discuss how the quality of software is a critical factor to its success [1--5]. Because software became an important part of almost every task in our daily life, having high quality software that meets the users' expectations is important [6]. Software integration is a stage in every software development lifecycle, it is defined as the process to assemble the software components and produce a single product. It has been shown that software integration and integration testing can make more than 40% of the overall project cost, so it is important that they are done efficiently and easily to be able to manage the involved risks [7]. A software engineering practice called continuous integration (CI) was introduced by Kent Beck and Ron Jeffries to mitigate the risks of software integration, enhance its process and improve its quality [8]. In this research, the principles of CI are identified and applied to a case study in order to analyze their impact on the software development process quality factors.},
	Author = {Saba Hamdan and Suad Alramouni},
	Doi = {https://doi.org/10.1016/j.promfg.2015.07.249},
	Issn = {2351-9789},
	Journal = {Procedia Manufacturing},
	Keywords = {Continuous integration, Software quality framework, ISO, Agile, Extreme programming, Software development},
	Note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
	Pages = {2019 - 2025},
	Title = {A Quality Framework for Software Continuous Integration},
	Url = {http://www.sciencedirect.com/science/article/pii/S2351978915002504},
	Volume = {3},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2351978915002504},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.promfg.2015.07.249}}

@article{CHEN2008123,
	Abstract = {Modelling of a complex carving surface is the most important process for digitization of art carving such as Chinese classical furniture carving, and it is difficult to be fulfilled. However, a complex 2D curve flower pattern can be easily acquired or drawn by handcraft or a drawing software. This paper presents a quick integrative 3D modeling method of complex carving surface based on a 2D curve flower pattern. The proposed method uses a scanning analysis algorithm, a normal distribution function and a distance function to model and create carving tracks. In this paper, the delamination, combination and interpolation of modelling process are described as well. The provided research method will make the modelling of complex carving surface more intelligent, agile, and will meet the requirement of integrative 3D modelling of digital art carving. Experimental results show that this method is of quick modelling and multi-model effective characteristics with realizable interactive designing and excellent practicability.},
	Author = {Yutuo Chen and Xuli Han and Minoru Okada and Y. Chen},
	Doi = {https://doi.org/10.1016/j.cad.2007.06.013},
	Issn = {0010-4485},
	Journal = {Computer-Aided Design},
	Keywords = {3D modelling, Complex carving surface, Multilayer model, Combinating model, Model interpolation},
	Note = {Constrained Design of Curves and Surfaces},
	Number = {1},
	Pages = {123 - 132},
	Title = {Integrative 3D modelling of complex carving surface},
	Url = {http://www.sciencedirect.com/science/article/pii/S0010448507001558},
	Volume = {40},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0010448507001558},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.cad.2007.06.013}}

@article{NOGUCHI20131077,
	Abstract = {While viewing works of art in galleries, we evaluate them by integrating at least two types of information: their visual properties (e.g., colors, symmetry, and proportion) and contextual information accompanying them (e.g., titles and names of artists). How rapidly the brain integrates visual and contextual information of artworks remains to be investigated. Using electroencephalography (EEG), we investigated neural activity when subjects with no professional experience in art viewed images of sculptures (masterpieces from the Classical and Renaissance periods, characterized by a canonical proportion of the golden ratio) and performed a five-scale rating of how appealing they were. At the beginning of each trial, we manipulated the expectations of the subjects for an upcoming sculpture by presenting information about its authenticity (either ``genuine'' or ``fake''), although all images were actually taken from genuine artworks. The image of the sculpture was then presented, either in its original proportion or after being deformed by a photo-editing software. This 2  2 factorial design enabled us to identify whether each component of the EEG response was sensitive to contextual information (genuine or fake), visual information (original or deformed), or both. Results revealed that amplitudes of a positive EEG component emerging at 200--300ms after the presentation of the artworks (mainly distributed over the parietal cortex) were significantly modulated by both visual and contextual factors, indicating a rapid integration of these two types of information in the brain.},
	Author = {Yasuki Noguchi and Miharu Murota},
	Doi = {https://doi.org/10.1016/j.neuropsychologia.2013.03.003},
	Issn = {0028-3932},
	Journal = {Neuropsychologia},
	Keywords = {Neuroesthetics, Electroencephalography, Contextual information, Parietal cortex},
	Number = {6},
	Pages = {1077 - 1084},
	Title = {Temporal dynamics of neural activity in an integration of visual and contextual information in an esthetic preference task},
	Url = {http://www.sciencedirect.com/science/article/pii/S0028393213000754},
	Volume = {51},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0028393213000754},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.neuropsychologia.2013.03.003}}

@article{THOMPSON200777,
	Abstract = {Currently, there is a plethora of low-cost commercial off-the-shelf (COTS) hardware available for implementing control systems. These range from devices with fairly low intelligence, e.g. smart sensors and actuators, to dedicated controllers such as PowerPC, programmable logic controllers (PLCs) and PC-based boards to dedicated systems-on-a-chip (SoC) ASICS and FPGAs. When considering the construction of complex distributed systems, e.g. for a ship, aircraft, car, train, process plant, the ability to rapidly integrate a variety of devices from different manufacturers is essential. A problem, however, is that manufacturers prefer to supply proprietary tools for programming their products. As a consequence of this lack of `openness', rapid prototyping and development of distributed systems is extremely difficult and costly for a systems integrator. Great opportunities thus exist to produce high-performance, dependable distributed systems. However, the key element that is missing is software tool support for systems integration. The objective of the Flexible Control Systems Development and Integration Environment for Control Systems (FLEXICON) project IST-2001-37269 is to solve these problems for industry and reduce development and implementation costs for distributed control systems by providing an integrated suite of tools to support all the development life-cycle of the system. Work within the Rolls-Royce supported University Technology Centre (UTC) is investigating rapid prototyping of controllers for aero-engines, unmanned aerial vehicles and ships. This paper describes the use of the developed co-simulation environment for a high-speed merchant vessel propulsion system application.},
	Author = {H.A. Thompson and D.N. Ramos-Hernandez and J. Fu and L. Jiang and I. Choi and K. Cartledge and J. Fortune and A. Brown},
	Doi = {https://doi.org/10.1016/j.conengprac.2006.04.005},
	Issn = {0967-0661},
	Journal = {Control Engineering Practice},
	Keywords = {CORBA, Co-simulation, Distributed systems, Tool support, Systems integration, Obsolescence management},
	Number = {1},
	Pages = {77 - 94},
	Title = {A flexible environment for rapid prototyping and analysis of distributed real-time safety-critical systems},
	Url = {http://www.sciencedirect.com/science/article/pii/S0967066106000839},
	Volume = {15},
	Year = {2007},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0967066106000839},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.conengprac.2006.04.005}}

@article{WANG2009558,
	Abstract = {In this paper, a novel ambient intelligence (AmI) platform is proposed to facilitate fast integration of different control algorithms, device networks and user interfaces. This platform defines the overall hardware/software architecture and communication standards. It consists of four layers, namely the ubiquitous environment, middleware, multi-agent system and application layer. The multi-agent system is implemented using Java Agent DEvelopment (JADE) framework and allows users to incorporate multiple control algorithms as agents for managing different tasks. The Universal Plug and Play (UPnP) device discovery protocol is used as a middleware, which isolates the multi-agent system and physical ubiquitous environment while providing a standard communication channel between the two. An XML content language has been designed to provide standard communication between various user interfaces and the multi-agent system. A mobile ubiquitous setup box is designed to allow fast construction of ubiquitous environments in any physical space. The real time performance analysis shows the potential of the proposed AmI platform to be used in real-life AmI applications. A case study has also been carried out to demonstrate the possibility of integrating multiple control algorithms in the multi-agent system and achieving a significant improvement on the overall offline learning performance.},
	Author = {Kevin I.-K. Wang and Waleed H. Abdulla and Zoran Salcic},
	Doi = {https://doi.org/10.1016/j.pmcj.2009.06.003},
	Issn = {1574-1192},
	Journal = {Pervasive and Mobile Computing},
	Keywords = {Ambient intelligence, Multi-agent system, Ubiquitous environment},
	Number = {5},
	Pages = {558 - 573},
	Title = {Ambient intelligence platform using multi-agent system and mobile ubiquitous hardware},
	Url = {http://www.sciencedirect.com/science/article/pii/S1574119209000522},
	Volume = {5},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1574119209000522},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.pmcj.2009.06.003}}

@article{CASTILLORODRIGUEZ2010629,
	Abstract = {The Environmental Units Map (EUM) is a strategic document that provides valuable information about landscape attributes for studies focused on environmental research, urban and land planning and environmental management. Traditionally, the systematic mapping of landforms has been used to integrate landforms and environmental data. Widespread availability of remote sensing data along with thematic cartography and implementation of Geographic Information Systems (GIS), allows a fast integration of landscape environmental attributes, effectively reducing time and costs. In this study we propose an approach to delineate an environmental units map using a geomorphologic map and a multivariate analysis processed in a GIS on a regional cartographic scale (1:75,000). Our study area is La Malinche volcano (located in central M{\'e}xico) where there are highly contrasting biophysical conditions and land use over relatively short distances. By means of a Hierarchical Cluster Analysis (HCA) a total of 29 environmental units were obtained for La Malinche. The environmental units range from alpine environments to semi arid lowlands (over a wide volcanic piedmont) where crops and urban development predominate. Our results suggest that integrating environmental units using a multivariate statistical approach not only produces results in agreement with what we observe empirically, but it also allows us to identify those factors which control the grouping of environmental attributes. The method proposed here can be used to integrate environmental data in a single map, and this could prove useful for environmental management in the future.},
	Author = {M. Castillo-Rodr{\'\i}guez and J. L{\'o}pez-Blanco and E. Mu{\~n}oz-Salinas},
	Doi = {https://doi.org/10.1016/j.apgeog.2010.01.003},
	Issn = {0143-6228},
	Journal = {Applied Geography},
	Keywords = {Environmental units map, Land system mapping, Hierarchical cluster analysis, GIS, La Malinche volcano},
	Note = {Climate Change and Applied Geography -- Place, Policy, and Practice},
	Number = {4},
	Pages = {629 - 638},
	Title = {A geomorphologic GIS-multivariate analysis approach to delineate environmental units, a case study of La Malinche volcano (central M{\'e}xico)},
	Url = {http://www.sciencedirect.com/science/article/pii/S0143622810000044},
	Volume = {30},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0143622810000044},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.apgeog.2010.01.003}}

@article{JAMES20159389,
	Abstract = {Easily accessible patent databases and advances in technology have enabled the exploration of organizational innovation through the analysis of patent records. However, the textual content of patents presents obstacles to gleaning useful information. In this study, we develop an expert system framework that utilizes text and data mining procedures for analyzing innovation through textual patent data. Specifically, we use patent titles representing the innovation activity at one company (SAP) and perform a bibliometric analysis using our proposed framework. Enterprise software, of which SAP is a pioneering developer, must serve a wide assortment of functions for companies in many different industries. In addition, SAP's sole focus is on enterprise software and it is a market leader in the category with substantial patent activity over the last decade. Using our framework to analyze SAP's patent activity provides a demonstration of how our bibliometric analysis can summarize and identify trends in innovation in a large software company. Our results illustrate that SAP has a breadth of innovative activity spread over the three-tier software engineering architecture and a lack of topical repetition indicative of limited depth. SAP's innovation is also seen to emphasize data management and quickly integrate emerging technologies. Results of an analysis on any company following our framework could be used for a variety of purposes, including: to examine the scope and scale of innovation of an organization, to examine the influence of technological trends on businesses, or to gain insight into corporate strategy that could be used to aid planning, investment, and purchasing decisions.},
	Author = {Tabitha L. James and Deborah F. Cook and Sumali Conlon and Kellie B. Keeling and Stephane Collignon and Trevor White},
	Doi = {https://doi.org/10.1016/j.eswa.2015.08.007},
	Issn = {0957-4174},
	Journal = {Expert Systems with Applications},
	Keywords = {Software, Innovation, Enterprise data management, Business intelligence and social media/data analytics, Text analysis, Cluster analysis},
	Number = {24},
	Pages = {9389 - 9401},
	Title = {A framework to explore innovation at SAP through bibliometric analysis of patent applications},
	Url = {http://www.sciencedirect.com/science/article/pii/S0957417415005461},
	Volume = {42},
	Year = {2015},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0957417415005461},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.eswa.2015.08.007}}

@article{HOFFMANN2016496,
	Abstract = {On the way to the fourth industrial revolution, one major requirement lies in reaching interoperability between hardware and software systems. Especially real-time propagation of shop floor information in top-level production planning and control systems as well as the consolidation of distributed information into a consistent data basis for comprehensive data analysis are still missing in most production environments. Existing approaches to serve interoperability through standardized interfaces are limited by proprietary data exchange protocols and information models. Within industrial manufacturing and automation, standardization attempts between these systems are primarily focused on industrial interfaces like OPC/OPC-UA. However, the aggregation of data created by devices like sensors or machinery control units into useful information has not been satisfactorily solved yet as their underlying models are carried out using different modeling paradigms and programming languages, thus intercommunication is difficult to implement and to maintain. In this work, an integration chain for data from field level to top-level information systems is presented. As Manufacturing Execution Systems or Enterprise Resource Planning tools are implemented in higher programming languages, the modeling of field level information has to be adapted in terms of a semantic interpretation. The approach provides integration capabilities for OPC-conform data generated on the field level. The information is extracted from low level information systems, transformed according to object-oriented programming paradigms and object-relational standards and finally integrated into databases that allow full semantic annotation and interpretation compatible to a common information model. Hence, users on management levels of the enterprise are able to perform holistic data treatment and data exploration along with personalized information views based on this central data storage by means of a reliable and comfortable data acquisition. This increases the quality of data and of the decision support itself, as more time remains for the actual task of data evaluation.},
	Author = {Max Hoffmann and Christian B{\"u}scher and Tobias Meisen and Sabina Jeschke},
	Doi = {https://doi.org/10.1016/j.procir.2015.12.059},
	Issn = {2212-8271},
	Journal = {Procedia CIRP},
	Keywords = {Interoperability, Information integration, Ontology, Semantic data, OPC, OPC UA},
	Note = {Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems},
	Pages = {496 - 501},
	Title = {Continuous Integration of Field Level Production Data into Top-level Information Systems Using the OPC Interface Standard},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212827115011385},
	Volume = {41},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212827115011385},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procir.2015.12.059}}

@article{SCHUH2016620,
	Abstract = {Short development times that are characterized by a high return on engineering (RoE) become increasingly important as a critical success factor for the realization of radical innovation. However, the high degree of complexity of modern products results in a long and costly development time when traditional sequential development processes are employed. The paper at hand describes the restraints of typical gate-oriented product development processes and builds up on recent studies recommending highly iterative innovation processes for the fast realization of physical product ideas. The suggested methodology represents an approach based on the Scrum process model from the software industry that includes the continuous integration of costumers and production engineers based on the execution of feasibility studies by the early and stepwise development of prototypes. In this context, modern ramp up and demonstration factories possessing a product lifecycle management (PLM) system, an integrated ICT infrastructure, interdisciplinary engineering teams and scalable manufacturing technologies are suggested as key enablers. The authors illustrate that these facilities, together with a sensor-based product expedition are particularly suitable for implementing an adapted Scrum process for the development of physical product ideas. A critical reflection on the basis of the development of an electric car aims to underline the suitability of the presented methodology in enabling radical innovation},
	Author = {G{\"u}nther Schuh and Thomas Gartzen and Felix Basse and Elisabeth Schrey},
	Doi = {https://doi.org/10.1016/j.procir.2016.01.014},
	Issn = {2212-8271},
	Journal = {Procedia CIRP},
	Keywords = {Radical innovation, Highly iterative product development, Scrum, Complexity management, Ramp up},
	Note = {Research and Innovation in Manufacturing: Key Enabling Technologies for the Factories of the Future - Proceedings of the 48th CIRP Conference on Manufacturing Systems},
	Pages = {620 - 625},
	Title = {Enabling Radical Innovation through Highly Iterative Product Expedition in Ramp up and Demonstration Factories},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212827116000251},
	Volume = {41},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212827116000251},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.procir.2016.01.014}}

@article{EMRULI201433,
	Abstract = {The rapid integration of physical systems with cyberspace infrastructure, the so-called Internet of Things, is likely to have a significant effect on how people interact with the physical environment and design information and communication systems. Internet-connected systems are expected to vastly outnumber people on the planet in the near future, leading to grand challenges in software engineering and automation in application domains involving complex and evolving systems. Several decades of artificial intelligence research suggests that conventional approaches to making such systems automatically interoperable using handcrafted ``semantic'' descriptions of services and information are difficult to apply. In this paper we outline a bioinspired learning approach to creating interoperable systems, which does not require handcrafted semantic descriptions and rules. Instead, the idea is that a functioning system (of systems) can emerge from an initial pseudorandom state through learning from examples, provided that each component conforms to a set of information coding rules. We combine a binary vector symbolic architecture (VSA) with an associative memory known as sparse distributed memory (SDM) to model context-dependent prediction by learning from examples. We present simulation results demonstrating that the proposed architecture can enable system interoperability by learning, for example by human demonstration.},
	Author = {Blerim Emruli and Fredrik Sandin and Jerker Delsing},
	Doi = {https://doi.org/10.1016/j.bica.2014.06.002},
	Issn = {2212-683X},
	Journal = {Biologically Inspired Cognitive Architectures},
	Keywords = {Communications, Interoperability, Learning, Vector symbolic architecture, Sparse distributed memory, System of systems},
	Note = {Neural-Symbolic Networks for Cognitive Capacities},
	Pages = {33 - 45},
	Title = {Vector space architecture for emergent interoperability of systems by learning from demonstration},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212683X14000474},
	Volume = {9},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212683X14000474},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.bica.2014.06.002}}

@article{JOHNSON2016181,
	Abstract = {Context
Recent years have seen an increasing interest in general theories of software engineering. As in other academic fields, these theories aim to explain and predict the key phenomena of the discipline.
Objective
The present article proposes a general theory of software engineering that we have labeled the Tarpit theory, in reference to the 1982 epigram by Alan Perlis.
Method
An integrative theory development approach was employed to develop the Tarpit theory from four underlying theoretical fields: (i) languages and automata, (ii) cognitive architecture, (iii) problem solving, and (iv) organization structure. Its applicability was explored in three test cases.
Results
The theory demonstrates an explanatory and predictive potential for a diverse set of software engineering phenomena. It demonstrates a capability of explaining Brooks's law, of making predictions about domain-specific languages, and of evaluating the pros and cons of the practice of continuous integration.
Conclusion
The presented theory appears capable of explaining and predicting a wide range of software engineering phenomena. Further refinement and application of the theory remains as future work.},
	Author = {Pontus Johnson and Mathias Ekstedt},
	Doi = {https://doi.org/10.1016/j.infsof.2015.06.001},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software engineering, Theory, Languages and automata, Cognitive architecture, Problem solving, Organization structure},
	Pages = {181 - 203},
	Title = {The Tarpit -- A general theory of software engineering},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584915001056},
	Volume = {70},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584915001056},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2015.06.001}}

@article{GROCEVS2016457,
	Abstract = {In the emerging world of information technologies, a growing number of students is choosing this specialization for their education. Therefore, the number of homework and laboratory research assignments that should be tested is also growing. The majority of these tasks is based on the necessity to implement some algorithm as a small program. This article discusses the possible solutions to the problem of automated testing of programming laboratory research assignments. The course ``Algorithmization and Programming of Solutions'' is offered to all the first-year students of The Faculty of Computer Science and Information Technology (500 students) in Riga Technical University and it provides the students the basics of the algorithmization of computing processes and the technology of program design using Java programming language (the given course and the University will be considered as an example of the implementation of the automated testing). During the course eight laboratory research assignments are planned, where the student has to develop an algorithm, create a program and submit it to the education portal of the University. The VBA test program was designed as one of the solutions, the requirements for each laboratory assignment were determined and the special tests have been created. At some point, however, the VBA offered options were no longer able to meet the requirements, therefore the activities on identifying the requirements for the automation of the whole cycle of programming work reception, testing and evaluation have begun.},
	Author = {Aleksejs Grocevs and Nat{\=a}lija Prokofjeva},
	Doi = {https://doi.org/10.1016/j.sbspro.2016.07.070},
	Issn = {1877-0428},
	Journal = {Procedia - Social and Behavioral Sciences},
	Keywords = {Automation, Assignment, Testing, Continuous Integration ;},
	Note = {2nd International Conference on Higher Education Advances,HEAd'16, 21-23 June 2016, Val{\`e}ncia, Spain},
	Pages = {457 - 461},
	Title = {The Capabilities of Automated Functional Testing of Programming Assignments},
	Url = {http://www.sciencedirect.com/science/article/pii/S187704281630996X},
	Volume = {228},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S187704281630996X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.sbspro.2016.07.070}}

@article{GHANAM2012968,
	Abstract = {Context
While there are many success stories of achieving high reuse and improved quality using software platforms, there is a need to investigate the issues and challenges organizations face when transitioning to a software platform strategy.
Objective
This case study provides a comprehensive taxonomy of the challenges faced when a medium-scale organization decided to adopt software platforms. The study also reveals how new trends in software engineering (i.e. agile methods, distributed development, and flat management structures) interplayed with the chosen platform strategy.
Method
We used an ethnographic approach to collect data by spending time at a medium-scale company in Scandinavia. We conducted 16in-depth interviews with representatives of eight different teams, three of which were working on three separate platforms. The collected data was analyzed using Grounded Theory.
Results
The findings identify four classes of challenges, namely: business challenges, organizational challenges, technical challenges, and people challenges. The article explains how these findings can be used to help researchers and practitioners identify practical solutions and required tool support.
Conclusion
The organization's decision to adopt a software platform strategy introduced a number of challenges. These challenges need to be understood and addressed in order to reap the benefits of reuse. Researchers need to further investigate issues such as supportive organizational structures for platform development, the role of agile methods in software platforms, tool support for testing and continuous integration in the platform context, and reuse recommendation systems.},
	Author = {Yaser Ghanam and Frank Maurer and Pekka Abrahamsson},
	Doi = {https://doi.org/10.1016/j.infsof.2012.03.005},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software platform, Software reuse, Platform challenges, Ethnographic study, Grounded Theory},
	Number = {9},
	Pages = {968 - 984},
	Title = {Making the leap to a software platform strategy: Issues and challenges},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912000547},
	Volume = {54},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584912000547},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2012.03.005}}

@article{GONZALEZ201264,
	Abstract = {The vast number of microbial sequences resulting from sequencing efforts using new technologies require us to re-assess currently available analysis methodologies and tools. Here we describe trends in the development and distribution of software for analyzing microbial sequence data. We then focus on one widely used set of methods, dimensionality reduction techniques, which allow users to summarize and compare these vast datasets. We conclude by emphasizing the utility of formal software engineering methods for the development of computational biology tools, and the need for new algorithms for comparing microbial communities. Such large-scale comparisons will allow us to fulfill the dream of rapid integration and comparison of microbial sequence data sets, in a replicable analytical environment, in order to describe the microbial world we inhabit.},
	Author = {Antonio Gonzalez and Rob Knight},
	Doi = {https://doi.org/10.1016/j.copbio.2011.11.028},
	Issn = {0958-1669},
	Journal = {Current Opinion in Biotechnology},
	Note = {Analytical biotechnology},
	Number = {1},
	Pages = {64 - 71},
	Title = {Advancing analytical algorithms and pipelines for billions of microbial sequences},
	Url = {http://www.sciencedirect.com/science/article/pii/S0958166911007397},
	Volume = {23},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0958166911007397},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.copbio.2011.11.028}}

@article{KOSTOV2011770,
	Abstract = {The modeling of bioreactors with immobilized cells is a complex process, incorporating in itself different complex operations. Depending on the design of the bioreactor and the biotechnological process are possible different combinations of mathematical models. Knowledge of the kinetics of biological processes and mass transfer are necessary to understand the basic principle of operation of the bioreactor. An important step in developing a complete model of the bioreactor is to explore and describe the flows' structure in the working volume of the apparatus. For reactors with different flow and mixing characteristics are needed different methods for design, modeling and scaling-up. In this work an analysis of the experimental curves to determine the liquid residence time distribution (RTD) of liquid in the bioreactor was made. Based on this analysis, two different combined models of the flow structures were built. The study was done using specialized software RTD 3.14. The estimation of the models parameter was made using least square method. The algorithm permits parallel, serial and other kinds of connections of building blocks, including recycles. Directions of flows between blocks and flow fractions, when necessary, are set by user. The models are characterized with simple structure and with good ability to reproduce experimental results with high accuracy. They were built by plug flow and ideal mixing units, which is a prerequisite for easy and quick integration in the synthesis model of the fermentation process in the bioreactor. The developed combined models for RTD description will be used to establish a system for control of the ethanol fermentation with immobilized cells. Thus, the impact of flow hydrodynamics will be render in account on the fermentation process.},
	Author = {Georgi Kostov and Mihail Angelov and Ivan Mihailov and Donka Stoeva},
	Doi = {https://doi.org/10.1016/j.profoo.2011.09.116},
	Issn = {2211-601X},
	Journal = {Procedia Food Science},
	Keywords = {immobilized cells, fluidized bed, residence time distribution, combined models, RTD 3.14 ;},
	Note = {11th International Congress on Engineering and Food (ICEF11)},
	Pages = {770 - 775},
	Title = {Development of combined models to describe the residence time distribution in fluidized-bed bioreactor with light beads},
	Url = {http://www.sciencedirect.com/science/article/pii/S2211601X11001179},
	Volume = {1},
	Year = {2011},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2211601X11001179},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.profoo.2011.09.116}}

@article{PULLEN200597,
	Abstract = {The distributed information technologies collectively known as Web services recently have demonstrated powerful capabilities for scalable interoperation of heterogeneous software across a wide variety of networked platforms. This approach supports a rapid integration cycle and shows promise for ultimately supporting automatic composability of services using discovery via registries. This paper presents a rationale for extending Web services to distributed simulation environments, together with a description and examples of the integration methodology used to develop significant prototype implementations, and argues for combining the power of Grid computing with Web services to further expand this demanding computation and database access environment.},
	Author = {J. Mark Pullen and Ryan Brunton and Don Brutzman and David Drake and Michael Hieb and Katherine L. Morse and Andreas Tolk},
	Doi = {https://doi.org/10.1016/j.future.2004.09.031},
	Issn = {0167-739X},
	Journal = {Future Generation Computer Systems},
	Keywords = {Web services, Grid computing, Distributed simulation, Composability},
	Number = {1},
	Pages = {97 - 106},
	Title = {Using Web services to integrate heterogeneous simulations in a grid environment},
	Url = {http://www.sciencedirect.com/science/article/pii/S0167739X0400144X},
	Volume = {21},
	Year = {2005},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0167739X0400144X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.future.2004.09.031}}

@article{CHILINGARYAN2009717,
	Abstract = {Huge magnetic clouds of plasma emitted by the Sun dominate intense geomagnetic storm occurrences and simultaneously they are correlated with variations of spectra of particles and nuclei in the interplanetary space, ranging from subtermal solar wind ions till GeV energy galactic cosmic rays. For a reliable and fast forecast of Space Weather world-wide networks of particle detectors are operated at different latitudes, longitudes, and altitudes. Based on a new type of hybrid particle detector developed in the context of the International Heliophysical Year (IHY 2007) at Aragats Space Environmental Center (ASEC) we start to prepare hardware and software for the first sites of Space Environmental Viewing and Analysis Network (SEVAN). In the paper the architecture of the newly developed data acquisition system for SEVAN is presented. We plan to run the SEVAN network under one-and-the-same data acquisition system, enabling fast integration of data for on-line analysis of Solar Flare Events. An Advanced Data Acquisition System (ADAS) is designed as a distributed network of uniform components connected by Web Services. Its main component is Unified Readout and Control Server (URCS) which controls the underlying electronics by means of detector specific drivers and makes a preliminary analysis of the on-line data. The lower level components of URCS are implemented in C and a fast binary representation is used for the data exchange with electronics. However, after preprocessing, the data are converted to a self-describing hybrid XML/Binary format. To achieve better reliability all URCS are running on embedded computers without disk and fans to avoid the limited lifetime of moving mechanical parts. The data storage is carried out by means of high performance servers working in parallel to provide data security. These servers are periodically inquiring the data from all URCS and storing it in a MySQL database. The implementation of the control interface is based on high level web standards and, therefore, all properties of the system can be remotely managed and monitored by the operators using web browsers. The advanced data acquisition system at ASEC in Armenia was started in November, 2006. The reliability of the multi-client service was proven by continuously monitoring neutral and charged cosmic ray particles. Seven particle monitors are located at 2000 and 3200m above sea level at a distance of 40 and 60km from the main data server.},
	Author = {Suren Chilingaryan and Ashot Chilingarian and Varuzhan Danielyan and Wolfgang Eppler},
	Doi = {https://doi.org/10.1016/j.asr.2008.10.008},
	Issn = {0273-1177},
	Journal = {Advances in Space Research},
	Keywords = {Data acquisition, Particle monitor, Detector networks, IHY},
	Note = {Solar Extreme Events: Fundamental Science and Applied Aspects},
	Number = {4},
	Pages = {717 - 720},
	Title = {Advanced data acquisition system for SEVAN},
	Url = {http://www.sciencedirect.com/science/article/pii/S0273117708005395},
	Volume = {43},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0273117708005395},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.asr.2008.10.008}}

@article{WOOD2013660,
	Abstract = {Context
Developing a theory of agile technology, in combination with empirical work, must include assessing its performance effects, and whether all or some of its key ingredients account for any performance advantage over traditional methods. Given the focus on teamwork, is the agile technology what really matters, or do general team factors, such as cohesion, primarily account for a team's success? Perhaps the more specific software engineering team factors, for example the agile development method's collective ownership and code management, are decisive.
Objective
To assess the contribution of agile methodology, agile-specific team methods, and general team factors in the performance of software teams.
Method
We studied 40 small-scale software development teams which used Extreme Programming (XP). We measured (1) the teams' adherence to XP methods, (2) their use of XP-specific team practices, and (3) standard team attributes, as well as the quality of the project's outcomes. We used Williams et al.'s (2004a) [33] Shodan measures of XP methods, and regression analysis.
Results
All three types of variables are associated with the project's performance. Teamworking is important but it is the XP-specific team factor (continuous integration, coding standards, and collective code ownership) that is significant. Only customer planning (release planning/planning game, customer access, short releases, and stand-up meeting) is positively related to performance. A negative relationship between foundations (automated unit tests, customer acceptance tests, test-first design, pair programming, and refactoring) is found and is moderated by craftsmanship (sustainable pace, simple design, and metaphor/system of names). Of the general team factors only cooperation is related to performance. Cooperation mediates the relationship between the XP-specific team factor and performance.
Conclusion
Client and team foci of the XP method are its critical active ingredients.},
	Author = {Stephen Wood and George Michaelides and Chris Thomson},
	Doi = {https://doi.org/10.1016/j.infsof.2012.10.002},
	Issn = {0950-5849},
	Journal = {Information and Software Technology},
	Keywords = {Software development, Extreme programming, Agile methods, Teamwork, Cooperation, Performance},
	Number = {4},
	Pages = {660 - 672},
	Title = {Successful extreme programming: Fidelity to the methodology or good teamworking?},
	Url = {http://www.sciencedirect.com/science/article/pii/S0950584912002091},
	Volume = {55},
	Year = {2013},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0950584912002091},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.infsof.2012.10.002}}

@article{GUI20092149,
	Abstract = {Robots have played a very important role in the growing popular flexible manufacturing environments. However, state-of-art industrial robots with high accuracy are rather costly and static. Our works aims at providing a low-cost fast integrating platform with advanced middleware support to seamlessly integrate off-the-shelf or future robot sensors, robots, and actuators as well as industrial IT system. To support such approach, a component-based reconfigurable middleware system is designed. A system runtime service is employed to manage the dependence and whole lifecycle of realtime components by reasoning from component's contract-based service description. A continues deployment mechanism is also designed The software architecture was implemented by so called -- Hybrid component model. The evaluation shows that the ARFLEX system achieve the goal of enhance in accuracy, flexibility while provide good real-time characteristics.},
	Author = {Ning Gui and Vincenzo De Florio and Gabriella Caporaletti and Chris Blondia},
	Doi = {https://doi.org/10.3182/20090603-3-RU-2001.0352},
	Issn = {1474-6670},
	Journal = {IFAC Proceedings Volumes},
	Note = {13th IFAC Symposium on Information Control Problems in Manufacturing},
	Number = {4},
	Pages = {2149 - 2154},
	Title = {Adaptive Robot Design and Applications in Flexible Manufacturing Environments},
	Url = {http://www.sciencedirect.com/science/article/pii/S1474667016341210},
	Volume = {42},
	Year = {2009},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1474667016341210},
	Bdsk-Url-2 = {https://doi.org/10.3182/20090603-3-RU-2001.0352}}

@article{SUGENG2008446,
	Abstract = {Objectives
Our study goals were to evaluate the 3-dimensional matrix array transesophageal echocardiographic (3D-MTEE) probe by assessing the image quality of native valves and other intracardiac structures.
Background
Because 3-dimensional transesophageal echocardiography with gated rotational acquisition is not used routinely as the result of artifacts, lengthy acquisition, and processing, a 3D-MTEE probe was developed (Philips Medical Systems, Andover, Massachusetts).
Methods
In 211 patients, 3D-MTEE zoom images of the mitral valve (MV), aortic valve, tricuspid valve, interatrial septum, and left atrial appendage were obtained, followed by a left ventricular wide-angled acquisition. Images were reviewed and graded off-line (Xcelera with QLAB software, Philips Medical Systems).
Results
Excellent visualization of the MV (85% to 91% for all scallops of both MV leaflets), interatrial septum (84%), left atrial appendage (86%), and left ventricle (77%) was observed. Native aortic and tricuspid valves were optimally visualized only in 18% and 11% of patients, respectively.
Conclusions
The use of 3D-MTEE imaging, which is feasible in most patients, provides superb imaging of native MVs, which makes this modality an excellent choice for MV surgical planning and guidance of percutaneous interventions. Optimal aortic and tricuspid valve imaging will depend on further technological developments. Fast acquisition and immediate online display will facilitate wider acceptance and routine use in clinical practice.},
	Author = {Lissa Sugeng and Stanton K. Shernan and Ivan S. Salgo and Lynn Weinert and Doug Shook and Jai Raman and Valluvan Jeevanandam and Frank DuPont and Scott Settlemier and Bernard Savord and John Fox and Victor Mor-Avi and Roberto M. Lang},
	Doi = {https://doi.org/10.1016/j.jacc.2008.04.038},
	Issn = {0735-1097},
	Journal = {Journal of the American College of Cardiology},
	Keywords = {transesophageal echocardiography, 3-dimensional echocardiography, intraoperative echocardiography, mitral valve, valvular disease},
	Number = {6},
	Pages = {446 - 449},
	Title = {Live 3-Dimensional Transesophageal Echocardiography: Initial Experience Using the Fully-Sampled Matrix Array Probe},
	Url = {http://www.sciencedirect.com/science/article/pii/S0735109708017968},
	Volume = {52},
	Year = {2008},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0735109708017968},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.jacc.2008.04.038}}

@article{VERWEIJ20102167,
	Abstract = {Policy makers have a growing interest in integrated assessments of policies. The Integrated Assessment Modelling (IAM) community is reacting to this interest by extending the application of model development from pure scientific analysis towards application in decision making or policy context by giving tools a higher capability for analysis targeted at non-experts, but intelligent users. Many parties are involved in the construction of such tools including modellers, domain experts and tool users, resulting in as many views on the proposed tool. During tool development research continues which leads to advanced understanding of the system and may alter early specifications. Accumulation of changes to the initial design obscures the design, usually vastly increasing the number of defects in the software. The software engineering community uses concepts, methods and practices to deal with ambiguous specifications, changing requirements and incompletely conceived visions, and to design and develop maintainable/extensible quality software. The aim of this paper is to introduce modellers to software engineering concepts and methods which have the potential to improve model and tool development using experiences from the development of the Sustainability Impact Assessment Tool. These range from choosing a software development methodology for planning activities and coordinating people, technical design principles impacting maintainability, quality and reusability of the software to prototyping and user involvement. It is argued that adaptive development methods seem to best fit research projects, that typically have unclear upfront and changing requirements. The break-down of a system into elements that overlap as little as possible in features and behaviour helps to divide the work across teams and to achieve a modular and flexible system. However, this must be accompanied by proper automated testing methods and automated continuous integration of the elements. Prototypes, screen sketches and mock-ups are useful to align the different views, build a shared vision of required functionality and to match expectations.},
	Author = {P.J.F.M. Verweij and M.J.R. Knapen and W.P. de Winter and J.J.F. Wien and J.A. te Roller and S. Sieber and J.M.L. Jansen},
	Doi = {https://doi.org/10.1016/j.ecolmodel.2010.01.006},
	Issn = {0304-3800},
	Journal = {Ecological Modelling},
	Keywords = {Software development process, Software architecture, Modelling, Integrated assessment, Assessment tool},
	Note = {Model-based Systems to Support Impact Assessment - Methods, Tools and Applications},
	Number = {18},
	Pages = {2167 - 2176},
	Title = {An IT perspective on integrated environmental modelling: The SIAT case},
	Url = {http://www.sciencedirect.com/science/article/pii/S0304380010000451},
	Volume = {221},
	Year = {2010},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0304380010000451},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ecolmodel.2010.01.006}}

@article{GUOWEI201218,
	Abstract = {According to the situation that the IT students can not meet the software industry demand for qualified personnel, a ``triple-driven'' three-dimensional software development practical teaching system was proposed, aiming to improve the software development capabilities and innovation sense of students. This system can effectively improve students the interest of software development and the practical skills and sense of innovation, laying a solid foundation for student after graduation to rapidly integrate into the software development process, meeting the needs of software industry.},
	Author = {Tang Guowei and Guo Lingling and Fu Yu and Li Jinghui and Zhao Wanping},
	Doi = {https://doi.org/10.1016/j.ieri.2012.06.045},
	Issn = {2212-6678},
	Journal = {IERI Procedia},
	Keywords = {triple-driven, practical teaching, software development},
	Note = {International Conference on Future Computer Supported Education, August 22- 23, 2012,Fraser Place Central - Seoul},
	Pages = {18 - 23},
	Title = {Research and Practice on ``Triple-driven'' Based Software Development Practical Teaching System},
	Url = {http://www.sciencedirect.com/science/article/pii/S2212667812000536},
	Volume = {2},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S2212667812000536},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ieri.2012.06.045}}
